{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run this cell to ensure you have all the packages you need for the following program. Put the zip file full of txt in the same folder as this program.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "print('Run this cell to ensure you have all the packages you need for the following program. Put the zip file full of txt in the same folder as this program.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_and_last_name_list(first_name_csv_file, last_name_csv_file):\n",
    "    first_name_array = []\n",
    "    last_name_array = []\n",
    "    with open(first_name_csv_file) as f:\n",
    "        lines = f.readlines()\n",
    "    lines = np.array(lines)\n",
    "    for rows in lines:\n",
    "        first_name_array.append(rows)\n",
    "    first_name_array = np.array(first_name_array)\n",
    "    with open(last_name_csv_file) as f:\n",
    "        lines = f.readlines()\n",
    "    lines = np.array(lines)\n",
    "    for rows in lines:\n",
    "        last_name_array.append(rows)\n",
    "    last_name_array = np.array(last_name_array)\n",
    "    return first_name_array, last_name_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_initialization_chat_and_transcript(csv_file):\n",
    "    name_onyen_dictionary = {}\n",
    "    with open(csv_file) as f:\n",
    "        next(f)\n",
    "        next(f)\n",
    "        next(f)\n",
    "        lines = f.readlines()\n",
    "    lines = np.array(lines)\n",
    "    for rows in lines:\n",
    "        if (rows.split(\",\")[2] == \"\"): # No last name\n",
    "            name_onyen_dictionary.update({rows.split(\",\")[3]:rows.split(\",\")[1]})\n",
    "        if (rows.split(\",\")[3] == \"\"): # No first name\n",
    "            name_onyen_dictionary.update({rows.split(\",\")[2]:rows.split(\",\")[1]})\n",
    "        name_onyen_dictionary.update({rows.split(\",\")[3] + \" \" + rows.split(\",\")[2]:rows.split(\",\")[1]}) # Both first and last name\n",
    "    # print(name_onyen_dictionary)\n",
    "    return name_onyen_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_hash_chat(txt_file, csv_file, first_name_csv_file, last_name_csv_file, general_first_name_hashmap_json_file, general_last_name_hashmap_json_file, general_onyen_hashmap_json_file):\n",
    "    # Get updated first and last name list\n",
    "    first_name_array, last_name_array = first_and_last_name_list(first_name_csv_file, last_name_csv_file)\n",
    "    # Get updated general anti_hashmap\n",
    "    with open(general_first_name_hashmap_json_file) as f:\n",
    "        general_first_name_hashmap = json.load(f)\n",
    "    with open(general_last_name_hashmap_json_file) as f:\n",
    "        general_last_name_hashmap = json.load(f)\n",
    "    with open(general_onyen_hashmap_json_file) as f:\n",
    "        general_onyen_hashmap = json.load(f)\n",
    "    # Establish the name-onyen dictionary\n",
    "    name_onyen_dictionary = dictionary_initialization_chat_and_transcript(csv_file)\n",
    "    hashed_first_names = ()\n",
    "    hashed_last_names = ()\n",
    "    hashed_onyen = ()\n",
    "    fake_first_name_hashed_value_dictionary = {}\n",
    "    fake_last_name_hashed_value_dictionary = {}\n",
    "    fake_onyen_hashed_value_dictionary = {}\n",
    "    with open(txt_file,encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    lines = np.array(lines)\n",
    "    answer = []\n",
    "    fake_name_answer = []\n",
    "    hash_map = {}\n",
    "    anti_hash_map = {}\n",
    "    for rows in lines:\n",
    "        # print(\"Original message: \" + rows)\n",
    "        if \"From\" not in rows:\n",
    "            line = rows.split('\\t')\n",
    "            if len(line)<2:\n",
    "                continue\n",
    "            name = line[1]\n",
    "            if (len(name.split(\" \")) < 2): # No first or last name\n",
    "                first_name = name.split(\" \")[0]\n",
    "                last_name = \"\"\n",
    "            else: # 这里存在一个问题，如果一个人的名字有不止一个空格（如middle name），将会导致first name与last name的选取出错\n",
    "                first_name = name.split(\" \")[0]\n",
    "                last_name = name.split(\" \")[1]\n",
    "\n",
    "            # Using the dictionary to convert the names to corresponding onyens\n",
    "            if name not in name_onyen_dictionary.keys():\n",
    "                onyen = '*'+name+'*'\n",
    "            else:\n",
    "                onyen = name_onyen_dictionary[name]\n",
    "\n",
    "            # sha-256 encryption\n",
    "            if (first_name != \"\"):\n",
    "                encoded_first = first_name.encode()\n",
    "                result_first = hashlib.sha256(encoded_first)\n",
    "                if (result_first.hexdigest() not in hashed_first_names) and (result_first.hexdigest() not in general_first_name_hashmap): # First time that encounters a first name\n",
    "                    hashed_first_names = hashed_first_names + (result_first.hexdigest(),)\n",
    "                    fake_first_name = first_name_array[0].replace(\"\\n\", \"\") # Always pick the first element\n",
    "                    first_name_array = np.delete(first_name_array,0) # Eliminate the name once it is used to prevent using one fake name for two different real names\n",
    "                    fake_first_name_hashed_value_dictionary.update({result_first.hexdigest():fake_first_name})\n",
    "                    general_first_name_hashmap.update({result_first.hexdigest():fake_first_name})\n",
    "                elif (result_first.hexdigest() not in hashed_first_names) and (result_first.hexdigest() in general_first_name_hashmap): # NOT first time that encounters a first name\n",
    "                    fake_first_name = general_first_name_hashmap[result_first.hexdigest()]\n",
    "                    fake_first_name_hashed_value_dictionary.update({result_first.hexdigest():fake_first_name})\n",
    "                elif (result_first.hexdigest() in hashed_first_names) and (result_first.hexdigest() not in general_first_name_hashmap): # NOT first time that encounters a first name\n",
    "                    fake_first_name = fake_first_name_hashed_value_dictionary[result_first.hexdigest()]\n",
    "                    general_first_name_hashmap.update({result_first.hexdigest():fake_first_name})\n",
    "                else:\n",
    "                    fake_first_name = fake_first_name_hashed_value_dictionary[result_first.hexdigest()]\n",
    "            else: # First name is \"\"\n",
    "                fake_first_name = \"\"\n",
    "\n",
    "            if (last_name != \"\"):\n",
    "                encoded_last = last_name.encode()\n",
    "                result_last = hashlib.sha256(encoded_last)\n",
    "                if (result_last.hexdigest() not in hashed_last_names) and (result_last.hexdigest() not in general_last_name_hashmap): # First time that encounters a last name\n",
    "                    hashed_last_names = hashed_last_names + (result_last.hexdigest(),)\n",
    "                    fake_last_name = last_name_array[0].replace(\"\\n\", \"\") # Always pick the first element\n",
    "                    last_name_array = np.delete(last_name_array, 0) # Eliminate the name once it is used to prevent using one fake name for two different real names\n",
    "                    fake_last_name_hashed_value_dictionary.update({result_last.hexdigest():fake_last_name})\n",
    "                    general_last_name_hashmap.update({result_last.hexdigest():fake_last_name})\n",
    "                elif (result_last.hexdigest() not in hashed_last_names) and (result_last.hexdigest() in general_last_name_hashmap): # NOT first time that encounters a last name\n",
    "                    fake_last_name = general_last_name_hashmap[result_last.hexdigest()]\n",
    "                    fake_last_name_hashed_value_dictionary.update({result_last.hexdigest():fake_last_name})\n",
    "                elif (result_last.hexdigest() in hashed_last_names) and (result_last.hexdigest() not in general_last_name_hashmap): # NOT first time that encounters a last name\n",
    "                    fake_last_name = fake_last_name_hashed_value_dictionary[result_last.hexdigest()]\n",
    "                    general_last_name_hashmap.update({result_last.hexdigest():fake_last_name})\n",
    "                else:\n",
    "                    fake_last_name = fake_last_name_hashed_value_dictionary[result_last.hexdigest()]\n",
    "            else: # Last name is \"\"\n",
    "                fake_last_name = \"\"\n",
    "\n",
    "            if (onyen != \"\"):\n",
    "                encoded_onyen = onyen.encode()\n",
    "                result_onyen = hashlib.sha256(encoded_onyen)\n",
    "                if (result_onyen.hexdigest() not in hashed_onyen) and (result_onyen.hexdigest() not in general_onyen_hashmap): # First time that encounters an onyen\n",
    "                    hashed_onyen = hashed_onyen + (result_onyen.hexdigest(),)\n",
    "                    if (fake_first_name != \"\") and (fake_last_name != \"\"):\n",
    "                        fake_onyen = fake_first_name + fake_last_name # 这里暂时不处理同名同姓的不同人的问题，因为如果两人名字相同但onyen不同，在Zoom由name map到onyen时就会报错\n",
    "                    elif (fake_first_name == \"\") and (fake_last_name != \"\"):\n",
    "                        fake_onyen = fake_last_name + fake_last_name\n",
    "                    else:\n",
    "                        fake_onyen = fake_first_name + fake_first_name\n",
    "                    fake_onyen_hashed_value_dictionary.update({result_onyen.hexdigest():fake_onyen})\n",
    "                    general_onyen_hashmap.update({result_onyen.hexdigest():fake_onyen})\n",
    "                elif (result_onyen.hexdigest() not in hashed_onyen) and (result_onyen.hexdigest() in general_onyen_hashmap): # NOT first time that encounters an onyen\n",
    "                    fake_onyen = general_onyen_hashmap[result_onyen.hexdigest()]\n",
    "                    fake_onyen_hashed_value_dictionary.update({result_onyen.hexdigest():fake_onyen})\n",
    "                elif (result_onyen.hexdigest() in hashed_onyen) and (result_onyen.hexdigest() not in general_onyen_hashmap): # NOT first time that encounters an onyen\n",
    "                    fake_onyen = fake_onyen_hashed_value_dictionary[result_onyen.hexdigest()]\n",
    "                    general_onyen_hashmap.update({result_onyen.hexdigest():fake_onyen})\n",
    "                else:\n",
    "                    fake_onyen = fake_onyen_hashed_value_dictionary[result_onyen.hexdigest()]\n",
    "            else: # Onyen is \"\"\n",
    "                fake_onyen = \"\"\n",
    "\n",
    "            #hashed_str = rows[:idx1 + len(sub1) + 1]\n",
    "            #anti_hashed_str = rows[:idx1 + len(sub1) + 1]\n",
    "            name_onyen_substring = \"\"\n",
    "            anti_name_onyen_substring = \"\"\n",
    "\n",
    "            if (first_name != \"\"):\n",
    "                name_onyen_substring = name_onyen_substring + str(result_first.hexdigest())\n",
    "                anti_name_onyen_substring = anti_name_onyen_substring + fake_first_name\n",
    "            if ((first_name != \"\") and (last_name != \"\")):\n",
    "                name_onyen_substring = name_onyen_substring + \" \"\n",
    "                anti_name_onyen_substring = anti_name_onyen_substring + \" \"\n",
    "            if (last_name != \"\"):\n",
    "                name_onyen_substring = name_onyen_substring + str(result_last.hexdigest())\n",
    "                anti_name_onyen_substring = anti_name_onyen_substring + fake_last_name\n",
    "            if (onyen != \"\"):\n",
    "                name_onyen_substring = name_onyen_substring + \" \" + \"(\" + str(result_onyen.hexdigest()) + \")\"\n",
    "                anti_name_onyen_substring = anti_name_onyen_substring + \" \" + \"(\" + fake_onyen + \")\"\n",
    "            else:\n",
    "                name_onyen_substring = name_onyen_substring + \" \" + \"(\" + \")\"\n",
    "                anti_name_onyen_substring = anti_name_onyen_substring + \" \" + \"(\" + \")\"\n",
    "\n",
    "            hashed_str = line[0] +'\\t' + name_onyen_substring  +':\\t' +line[2]\n",
    "            anti_hashed_str = line[0] + '\\t' + anti_name_onyen_substring + ':\\t' + line[2]\n",
    "\n",
    "            # print(\"Hashed message: \" + hashed_str)\n",
    "            answer.append(hashed_str)\n",
    "            fake_name_answer.append(anti_hashed_str)\n",
    "            hash_map.update({name + \" (\" + onyen + \")\":name_onyen_substring})\n",
    "            if (first_name != \"\") and (last_name != \"\") and (onyen != \"\"):\n",
    "                anti_hash_map.update({anti_name_onyen_substring:result_first.hexdigest() + \" \" + result_last.hexdigest() + \" (\" + result_onyen.hexdigest() + \")\"})\n",
    "            elif (first_name == \"\") and (onyen == \"\"):\n",
    "                anti_hash_map.update({anti_name_onyen_substring:result_last.hexdigest() + \" ()\"})\n",
    "            elif (last_name != \"\") and (onyen != \"\"):\n",
    "                anti_hash_map.update({anti_name_onyen_substring:result_first.hexdigest() + \" ()\"})\n",
    "            elif (first_name == \"\"):\n",
    "                anti_hash_map.update({anti_name_onyen_substring:result_last.hexdigest() + \" (\" + result_onyen.hexdigest() + \")\"})\n",
    "            elif (last_name == \"\"):\n",
    "                anti_hash_map.update({anti_name_onyen_substring:result_first.hexdigest() + \" (\" + result_onyen.hexdigest() + \")\"})\n",
    "            elif (onyen == \"\"):\n",
    "                anti_hash_map.update({anti_name_onyen_substring:result_first.hexdigest() + \" \" + result_last.hexdigest() + \" ()\"})\n",
    "        elif (\"From\" in rows) and (\":\" in rows):\n",
    "            if \"(Direct Message)\" not in rows:\n",
    "                # initializing substrings\n",
    "                sub1 = \"From\"\n",
    "                sub2 = \" :\"\n",
    "                \n",
    "                # getting index of substrings\n",
    "                idx1 = rows.index(sub1)\n",
    "                idx2 = rows.index(sub2)\n",
    "\n",
    "                # length of substring 1 is added to\n",
    "                # get string from next character\n",
    "                name = rows[idx1 + len(sub1) + 1: idx2].lstrip().rstrip() # remove spaces\n",
    "                if (len(name.split(\" \")) < 2): # No first or last name\n",
    "                    first_name = name.split(\" \")[0]\n",
    "                    last_name = \"\"\n",
    "                else: # 这里存在一个问题，如果一个人的名字有不止一个空格（如middle name），将会导致first name与last name的选取出错\n",
    "                    first_name = name.split(\" \")[0]\n",
    "                    last_name = name.split(\" \")[1]\n",
    "                \n",
    "                # Using the dictionary to convert the names to corresponding onyens\n",
    "                if name not in name_onyen_dictionary.keys():\n",
    "                    onyen = '*'+name+'*'\n",
    "                else:\n",
    "                    onyen = name_onyen_dictionary[name]\n",
    "\n",
    "                # sha-256 encryption\n",
    "                if (first_name != \"\"):\n",
    "                    encoded_first = first_name.encode()\n",
    "                    result_first = hashlib.sha256(encoded_first)\n",
    "                    if (result_first.hexdigest() not in hashed_first_names) and (result_first.hexdigest() not in general_first_name_hashmap): # First time that encounters a first name\n",
    "                        hashed_first_names = hashed_first_names + (result_first.hexdigest(),)\n",
    "                        fake_first_name = first_name_array[0].replace(\"\\n\", \"\") # Always pick the first element\n",
    "                        first_name_array = np.delete(first_name_array,0) # Eliminate the name once it is used to prevent using one fake name for two different real names\n",
    "                        fake_first_name_hashed_value_dictionary.update({result_first.hexdigest():fake_first_name})\n",
    "                        general_first_name_hashmap.update({result_first.hexdigest():fake_first_name})\n",
    "                    elif (result_first.hexdigest() not in hashed_first_names) and (result_first.hexdigest() in general_first_name_hashmap): # NOT first time that encounters a first name\n",
    "                        fake_first_name = general_first_name_hashmap[result_first.hexdigest()]\n",
    "                        fake_first_name_hashed_value_dictionary.update({result_first.hexdigest():fake_first_name})\n",
    "                    elif (result_first.hexdigest() in hashed_first_names) and (result_first.hexdigest() not in general_first_name_hashmap): # NOT first time that encounters a first name\n",
    "                        fake_first_name = fake_first_name_hashed_value_dictionary[result_first.hexdigest()]\n",
    "                        general_first_name_hashmap.update({result_first.hexdigest():fake_first_name})\n",
    "                    else:\n",
    "                        fake_first_name = fake_first_name_hashed_value_dictionary[result_first.hexdigest()]\n",
    "                else: # First name is \"\"\n",
    "                    fake_first_name = \"\"\n",
    "                        \n",
    "                if (last_name != \"\"):\n",
    "                    encoded_last = last_name.encode()\n",
    "                    result_last = hashlib.sha256(encoded_last)\n",
    "                    if (result_last.hexdigest() not in hashed_last_names) and (result_last.hexdigest() not in general_last_name_hashmap): # First time that encounters a last name\n",
    "                        hashed_last_names = hashed_last_names + (result_last.hexdigest(),)\n",
    "                        fake_last_name = last_name_array[0].replace(\"\\n\", \"\") # Always pick the first element\n",
    "                        last_name_array = np.delete(last_name_array, 0) # Eliminate the name once it is used to prevent using one fake name for two different real names\n",
    "                        fake_last_name_hashed_value_dictionary.update({result_last.hexdigest():fake_last_name})\n",
    "                        general_last_name_hashmap.update({result_last.hexdigest():fake_last_name})\n",
    "                    elif (result_last.hexdigest() not in hashed_last_names) and (result_last.hexdigest() in general_last_name_hashmap): # NOT first time that encounters a last name\n",
    "                        fake_last_name = general_last_name_hashmap[result_last.hexdigest()]\n",
    "                        fake_last_name_hashed_value_dictionary.update({result_last.hexdigest():fake_last_name})\n",
    "                    elif (result_last.hexdigest() in hashed_last_names) and (result_last.hexdigest() not in general_last_name_hashmap): # NOT first time that encounters a last name\n",
    "                        fake_last_name = fake_last_name_hashed_value_dictionary[result_last.hexdigest()]\n",
    "                        general_last_name_hashmap.update({result_last.hexdigest():fake_last_name})\n",
    "                    else:\n",
    "                        fake_last_name = fake_last_name_hashed_value_dictionary[result_last.hexdigest()]\n",
    "                else: # Last name is \"\"\n",
    "                    fake_last_name = \"\"\n",
    "                \n",
    "                if (onyen != \"\"):\n",
    "                    encoded_onyen = onyen.encode()\n",
    "                    result_onyen = hashlib.sha256(encoded_onyen)\n",
    "                    if (result_onyen.hexdigest() not in hashed_onyen) and (result_onyen.hexdigest() not in general_onyen_hashmap): # First time that encounters an onyen\n",
    "                        hashed_onyen = hashed_onyen + (result_onyen.hexdigest(),)\n",
    "                        if (fake_first_name != \"\") and (fake_last_name != \"\"):\n",
    "                            fake_onyen = fake_first_name + fake_last_name # 这里暂时不处理同名同姓的不同人的问题，因为如果两人名字相同但onyen不同，在Zoom由name map到onyen时就会报错\n",
    "                        elif (fake_first_name == \"\") and (fake_last_name != \"\"):\n",
    "                            fake_onyen = fake_last_name + fake_last_name\n",
    "                        else:\n",
    "                            fake_onyen = fake_first_name + fake_first_name\n",
    "                        fake_onyen_hashed_value_dictionary.update({result_onyen.hexdigest():fake_onyen})\n",
    "                        general_onyen_hashmap.update({result_onyen.hexdigest():fake_onyen})\n",
    "                    elif (result_onyen.hexdigest() not in hashed_onyen) and (result_onyen.hexdigest() in general_onyen_hashmap): # NOT first time that encounters an onyen\n",
    "                        fake_onyen = general_onyen_hashmap[result_onyen.hexdigest()]\n",
    "                        fake_onyen_hashed_value_dictionary.update({result_onyen.hexdigest():fake_onyen})\n",
    "                    elif (result_onyen.hexdigest() in hashed_onyen) and (result_onyen.hexdigest() not in general_onyen_hashmap): # NOT first time that encounters an onyen\n",
    "                        fake_onyen = fake_onyen_hashed_value_dictionary[result_onyen.hexdigest()]\n",
    "                        general_onyen_hashmap.update({result_onyen.hexdigest():fake_onyen})\n",
    "                    else:\n",
    "                        fake_onyen = fake_onyen_hashed_value_dictionary[result_onyen.hexdigest()]\n",
    "                else: # Onyen is \"\"\n",
    "                    fake_onyen = \"\"\n",
    "                    \n",
    "                hashed_str = rows[:idx1 + len(sub1) + 1]\n",
    "                anti_hashed_str = rows[:idx1 + len(sub1) + 1]\n",
    "                name_onyen_substring = \"\"\n",
    "                anti_name_onyen_substring = \"\"\n",
    "                \n",
    "                if (first_name != \"\"):\n",
    "                    name_onyen_substring = name_onyen_substring + str(result_first.hexdigest())\n",
    "                    anti_name_onyen_substring = anti_name_onyen_substring + fake_first_name\n",
    "                if ((first_name != \"\") and (last_name != \"\")):\n",
    "                    name_onyen_substring = name_onyen_substring + \" \"\n",
    "                    anti_name_onyen_substring = anti_name_onyen_substring + \" \"\n",
    "                if (last_name != \"\"):\n",
    "                    name_onyen_substring = name_onyen_substring + str(result_last.hexdigest())\n",
    "                    anti_name_onyen_substring = anti_name_onyen_substring + fake_last_name\n",
    "                if (onyen != \"\"):\n",
    "                    name_onyen_substring = name_onyen_substring + \" \" + \"(\" + str(result_onyen.hexdigest()) + \")\"\n",
    "                    anti_name_onyen_substring = anti_name_onyen_substring + \" \" + \"(\" + fake_onyen + \")\"\n",
    "                else:\n",
    "                    name_onyen_substring = name_onyen_substring + \" \" + \"(\" + \")\"\n",
    "                    anti_name_onyen_substring = anti_name_onyen_substring + \" \" + \"(\" + \")\"\n",
    "                    \n",
    "                hashed_str = hashed_str + name_onyen_substring + rows[idx2:]\n",
    "                anti_hashed_str = anti_hashed_str + anti_name_onyen_substring + rows[idx2:]\n",
    "\n",
    "                # print(\"Hashed message: \" + hashed_str)\n",
    "                answer.append(hashed_str)\n",
    "                fake_name_answer.append(anti_hashed_str)\n",
    "                hash_map.update({name + \" (\" + onyen + \")\":name_onyen_substring})\n",
    "                if (first_name != \"\") and (last_name != \"\") and (onyen != \"\"):\n",
    "                    anti_hash_map.update({anti_name_onyen_substring:result_first.hexdigest() + \" \" + result_last.hexdigest() + \" (\" + result_onyen.hexdigest() + \")\"})\n",
    "                elif (first_name == \"\") and (onyen == \"\"):\n",
    "                    anti_hash_map.update({anti_name_onyen_substring:result_last.hexdigest() + \" ()\"})\n",
    "                elif (last_name != \"\") and (onyen != \"\"):\n",
    "                    anti_hash_map.update({anti_name_onyen_substring:result_first.hexdigest() + \" ()\"})\n",
    "                elif (first_name == \"\"):\n",
    "                    anti_hash_map.update({anti_name_onyen_substring:result_last.hexdigest() + \" (\" + result_onyen.hexdigest() + \")\"})\n",
    "                elif (last_name == \"\"):\n",
    "                    anti_hash_map.update({anti_name_onyen_substring:result_first.hexdigest() + \" (\" + result_onyen.hexdigest() + \")\"})\n",
    "                elif (onyen == \"\"):\n",
    "                    anti_hash_map.update({anti_name_onyen_substring:result_first.hexdigest() + \" \" + result_last.hexdigest() + \" ()\"})\n",
    "                    \n",
    "            if \"(Direct Message)\" in rows:\n",
    "                # initializing substrings\n",
    "                sub1 = \"From\"\n",
    "                sub2 = \"to\"\n",
    "                sub3 = \"to\"\n",
    "                sub4 = \"(Direct Message) :\"\n",
    "\n",
    "                # getting index of substrings\n",
    "                idx1 = rows.index(sub1)\n",
    "                idx2 = rows.index(sub2)\n",
    "                idx3 = rows.index(sub3)\n",
    "                idx4 = rows.index(sub4)\n",
    "\n",
    "                # length of substring 1 is added to\n",
    "                # get string from next character\n",
    "                name1 = rows[idx1 + len(sub1) + 1: idx2].lstrip().rstrip() # remove spaces\n",
    "                if (len(name1.split(\" \")) < 2): # No first or last name\n",
    "                    first_name1 = name1.split(\" \")[0]\n",
    "                    last_name1 = \"\"\n",
    "                else: # 这里存在一个问题，如果一个人的名字有不止一个空格（如middle name），将会导致first name与last name的选取出错\n",
    "                    first_name1 = name1.split(\" \")[0]\n",
    "                    last_name1 = name1.split(\" \")[1]\n",
    "                name2 = rows[idx3 + len(sub3) + 1: idx4].lstrip().rstrip() # remove spaces\n",
    "                if (len(name2.split(\" \")) < 2): # No first or last name\n",
    "                    first_name2 = name2.split(\" \")[0]\n",
    "                    last_name2 = \"\"\n",
    "                else: # 这里存在一个问题，如果一个人的名字有不止一个空格（如middle name），将会导致first name与last name的选取出错\n",
    "                    first_name2 = name2.split(\" \")[0]\n",
    "                    last_name2 = name2.split(\" \")[1]\n",
    "                \n",
    "                # Using the dictionary to convert the names to corresponding onyens\n",
    "                if name1 not in name_onyen_dictionary.keys():\n",
    "                    onyen1 = '*'+name1+'*'\n",
    "                else: \n",
    "                    onyen1 = name_onyen_dictionary[name1]\n",
    "                if name2 not in name_onyen_dictionary.keys():\n",
    "                    onyen2 = '*'+name2+'*'\n",
    "                else:\n",
    "                    onyen2 = name_onyen_dictionary[name2]\n",
    "\n",
    "                # sha-256 encryption\n",
    "                if (first_name1 != \"\"):\n",
    "                    encoded_first1 = first_name1.encode()\n",
    "                    result_first1 = hashlib.sha256(encoded_first1)\n",
    "                    if (result_first1.hexdigest() not in hashed_first_names) and (result_first1.hexdigest() not in general_first_name_hashmap): # First time that encounters a first name\n",
    "                        hashed_first_names = hashed_first_names + (result_first1.hexdigest(),)\n",
    "                        fake_first_name1 = first_name_array[0].replace(\"\\n\", \"\") # Always pick the first element\n",
    "                        first_name_array = np.delete(first_name_array, 0) # Eliminate the name once it is used to prevent using one fake name for two different real names\n",
    "                        fake_first_name_hashed_value_dictionary.update({result_first1.hexdigest():fake_first_name1})\n",
    "                        general_first_name_hashmap.update({result_first1.hexdigest():fake_first_name1})\n",
    "                    elif (result_first1.hexdigest() not in hashed_first_names) and (result_first1.hexdigest() in general_first_name_hashmap): # NOT first time that encounters a first name\n",
    "                        fake_first_name1 = general_first_name_hashmap[result_first1.hexdigest()]\n",
    "                        fake_first_name_hashed_value_dictionary.update({result_first1.hexdigest():fake_first_name1})\n",
    "                    elif (result_first1.hexdigest() in hashed_first_names) and (result_first1.hexdigest() not in general_first_name_hashmap): \n",
    "                        fake_first_name1 = fake_first_name_hashed_value_dictionary[result_first1.hexdigest()]\n",
    "                        general_first_name_hashmap.update({result_first1.hexdigest():fake_first_name1})\n",
    "                    else:\n",
    "                        fake_first_name1 = fake_first_name_hashed_value_dictionary[result_first1.hexdigest()]\n",
    "                else: # First name is \"\"\n",
    "                    fake_first_name1 = \"\"\n",
    "                        \n",
    "                if (last_name1 != \"\"):\n",
    "                    encoded_last1 = last_name1.encode()\n",
    "                    result_last1 = hashlib.sha256(encoded_last1)\n",
    "                    if (result_last1.hexdigest() not in hashed_last_names) and (result_last1.hexdigest() not in general_last_name_hashmap): # First time that encounters a last name\n",
    "                        hashed_last_names = hashed_last_names + (result_last1.hexdigest(),)\n",
    "                        fake_last_name1 = last_name_array[0].replace(\"\\n\", \"\") # Always pick the first element\n",
    "                        last_name_array = np.delete(last_name_array, 0) # Eliminate the name once it is used to prevent using one fake name for two different real names\n",
    "                        fake_last_name_hashed_value_dictionary.update({result_last1.hexdigest():fake_last_name1})\n",
    "                        general_last_name_hashmap.update({result_last1.hexdigest():fake_last_name1})\n",
    "                    elif (result_last1.hexdigest() not in hashed_last_names) and (result_last1.hexdigest() in general_last_name_hashmap): # NOT first time that encounters a last name\n",
    "                        fake_last_name1 = general_last_name_hashmap[result_last1.hexdigest()]\n",
    "                        fake_last_name_hashed_value_dictionary.update({result_last1.hexdigest():fake_last_name1})\n",
    "                    elif (result_last1.hexdigest() in hashed_last_names) and (result_last1.hexdigest() not in general_last_name_hashmap):\n",
    "                        fake_last_name1 = fake_last_name_hashed_value_dictionary[result_last1.hexdigest()]\n",
    "                        general_last_name_hashmap.update({result_last1.hexdigest():fake_last_name1})\n",
    "                    else:\n",
    "                        fake_last_name1 = fake_last_name_hashed_value_dictionary[result_last1.hexdigest()]\n",
    "                else: # Last name is \"\"\n",
    "                    fake_last_name1 = \"\"\n",
    "                \n",
    "                if (onyen1 != \"\"):\n",
    "                    encoded_onyen1 = onyen1.encode()\n",
    "                    result_onyen1 = hashlib.sha256(encoded_onyen1)\n",
    "                    if (result_onyen1.hexdigest() not in hashed_onyen) and (result_onyen1.hexdigest() not in general_onyen_hashmap): # First time that encounters an onyen\n",
    "                        hashed_onyen = hashed_onyen + (result_onyen1.hexdigest(),)\n",
    "                        if (fake_first_name1 != \"\") and (fake_last_name1 != \"\"):\n",
    "                            fake_onyen1 = fake_first_name1 + fake_last_name1 # 这里暂时不处理同名同姓的不同人的问题，因为如果两人名字相同但onyen不同，在Zoom由name map到onyen时就会报错\n",
    "                        elif (fake_first_name1 == \"\") and (fake_last_name1 != \"\"):\n",
    "                            fake_onyen1 = fake_last_name1 + fake_last_name1\n",
    "                        else:\n",
    "                            fake_onyen1 = fake_first_name1 + fake_first_name1\n",
    "                        fake_onyen_hashed_value_dictionary.update({result_onyen1.hexdigest():fake_onyen1})\n",
    "                        general_onyen_hashmap.update({result_onyen1.hexdigest():fake_onyen1})\n",
    "                    elif (result_onyen1.hexdigest() not in hashed_onyen) and (result_onyen1.hexdigest() in general_onyen_hashmap): # NOT first time that encounters an onyen\n",
    "                        fake_onyen1 = general_onyen_hashmap[result_onyen1.hexdigest()]\n",
    "                        fake_onyen_hashed_value_dictionary.update({result_onyen1.hexdigest():fake_onyen1})\n",
    "                    elif (result_onyen1.hexdigest() in hashed_onyen) and (result_onyen1.hexdigest() not in general_onyen_hashmap):\n",
    "                        fake_onyen1 = fake_onyen_hashed_value_dictionary[result_onyen1.hexdigest()]\n",
    "                        general_onyen_hashmap.update({result_onyen1.hexdigest():fake_onyen1})\n",
    "                    else:\n",
    "                        fake_onyen1 = fake_onyen_hashed_value_dictionary[result_onyen1.hexdigest()]\n",
    "                else: # Onyen is \"\"\n",
    "                    fake_onyen1 = \"\"\n",
    "                    \n",
    "                if (first_name2 != \"\"):\n",
    "                    encoded_first2 = first_name2.encode()\n",
    "                    result_first2 = hashlib.sha256(encoded_first2)\n",
    "                    if (result_first2.hexdigest() not in hashed_first_names) and (result_first2.hexdigest() not in general_first_name_hashmap): # First time that encounters a first name\n",
    "                        hashed_first_names = hashed_first_names + (result_first2.hexdigest(),)\n",
    "                        fake_first_name2 = first_name_array[0].replace(\"\\n\", \"\") # Always pick the first element\n",
    "                        first_name_array = np.delete(first_name_array, 0) # Eliminate the name once it is used to prevent using one fake name for two different real names\n",
    "                        fake_first_name_hashed_value_dictionary.update({result_first2.hexdigest():fake_first_name2})\n",
    "                        general_first_name_hashmap.update({result_first2.hexdigest():fake_first_name2})\n",
    "                    elif (result_first2.hexdigest() not in hashed_first_names) and (result_first2.hexdigest() in general_first_name_hashmap): # NOT first time that encounters a first name\n",
    "                        fake_first_name2 = general_first_name_hashmap[result_first2.hexdigest()]\n",
    "                        fake_first_name_hashed_value_dictionary.update({result_first2.hexdigest():fake_first_name2})\n",
    "                    elif (result_first2.hexdigest() in hashed_first_names) and (result_first2.hexdigest() not in general_first_name_hashmap): \n",
    "                        fake_first_name2 = fake_first_name_hashed_value_dictionary[result_first2.hexdigest()]\n",
    "                        general_first_name_hashmap.update({result_first2.hexdigest():fake_first_name2})\n",
    "                    else:\n",
    "                        fake_first_name2 = fake_first_name_hashed_value_dictionary[result_first2.hexdigest()]\n",
    "                else: # First name is \"\"\n",
    "                    fake_first_name2 = \"\"\n",
    "                        \n",
    "                if (last_name2 != \"\"):\n",
    "                    encoded_last2 = last_name2.encode()\n",
    "                    result_last2 = hashlib.sha256(encoded_last2)\n",
    "                    if (result_last2.hexdigest() not in hashed_last_names) and (result_last2.hexdigest() not in general_last_name_hashmap): # First time that encounters a last name\n",
    "                        hashed_last_names = hashed_last_names + (result_last2.hexdigest(),)\n",
    "                        fake_last_name2 = last_name_array[0].replace(\"\\n\", \"\") # Always pick the first element\n",
    "                        last_name_array = np.delete(last_name_array, 0) # Eliminate the name once it is used to prevent using one fake name for two different real names\n",
    "                        fake_last_name_hashed_value_dictionary.update({result_last2.hexdigest():fake_last_name2})\n",
    "                        general_last_name_hashmap.update({result_last2.hexdigest():fake_last_name2})\n",
    "                    elif (result_last2.hexdigest() not in hashed_last_names) and (result_last2.hexdigest() in general_last_name_hashmap): # NOT first time that encounters a last name\n",
    "                        fake_last_name2 = general_last_name_hashmap[result_last2.hexdigest()]\n",
    "                        fake_last_name_hashed_value_dictionary.update({result_last2.hexdigest():fake_last_name2})\n",
    "                    elif (result_last2.hexdigest() in hashed_last_names) and (result_last2.hexdigest() not in general_last_name_hashmap):\n",
    "                        fake_last_name2 = fake_last_name_hashed_value_dictionary[result_last2.hexdigest()]\n",
    "                        general_last_name_hashmap.update({result_last2.hexdigest():fake_last_name2})\n",
    "                    else:\n",
    "                        fake_last_name2 = fake_last_name_hashed_value_dictionary[result_last2.hexdigest()]\n",
    "                else: # Last name is \"\"\n",
    "                    fake_last_name2 = \"\"\n",
    "                \n",
    "                if (onyen2 != \"\"):\n",
    "                    encoded_onyen2 = onyen2.encode()\n",
    "                    result_onyen2 = hashlib.sha256(encoded_onyen2)\n",
    "                    if (result_onyen2.hexdigest() not in hashed_onyen) and (result_onyen2.hexdigest() not in general_onyen_hashmap): # First time that encounters an onyen\n",
    "                        hashed_onyen = hashed_onyen + (result_onyen2.hexdigest(),)\n",
    "                        if (fake_first_name2 != \"\") and (fake_last_name2 != \"\"):\n",
    "                            fake_onyen2 = fake_first_name2 + fake_last_name2 # 这里暂时不处理同名同姓的不同人的问题，因为如果两人名字相同但onyen不同，在Zoom由name map到onyen时就会报错\n",
    "                        elif (fake_first_name2 == \"\") and (fake_last_name2 != \"\"):\n",
    "                            fake_onyen2 = fake_last_name2 + fake_last_name2\n",
    "                        else:\n",
    "                            fake_onyen2 = fake_first_name2 + fake_first_name2\n",
    "                        fake_onyen_hashed_value_dictionary.update({result_onyen2.hexdigest():fake_onyen2})\n",
    "                        general_onyen_hashmap.update({result_onyen2.hexdigest():fake_onyen2})\n",
    "                    elif (result_onyen2.hexdigest() not in hashed_onyen) and (result_onyen2.hexdigest() in general_onyen_hashmap): # NOT first time that encounters an onyen\n",
    "                        fake_onyen2 = general_onyen_hashmap[result_onyen2.hexdigest()]\n",
    "                        fake_onyen_hashed_value_dictionary.update({result_onyen2.hexdigest():fake_onyen2})\n",
    "                    elif (result_onyen2.hexdigest() in hashed_onyen) and (result_onyen2.hexdigest() not in general_onyen_hashmap):\n",
    "                        fake_onyen2 = fake_onyen_hashed_value_dictionary[result_onyen2.hexdigest()]\n",
    "                        general_onyen_hashmap.update({result_onyen2.hexdigest():fake_onyen2})\n",
    "                    else:\n",
    "                        fake_onyen2 = fake_onyen_hashed_value_dictionary[result_onyen2.hexdigest()]\n",
    "                else: # Onyen is \"\"\n",
    "                    fake_onyen2 = \"\"\n",
    "                    \n",
    "                hashed_str = rows[:idx1 + len(sub1) + 1]\n",
    "                anti_hashed_str = rows[:idx1 + len(sub1) + 1]\n",
    "                name_onyen_substring1 = \"\"\n",
    "                anti_name_onyen_substring1 = \"\"\n",
    "                name_onyen_substring2 = \"\"\n",
    "                anti_name_onyen_substring2 = \"\"\n",
    "                \n",
    "                if (first_name1 != \"\"):\n",
    "                    name_onyen_substring1 = name_onyen_substring1 + str(result_first1.hexdigest())\n",
    "                    anti_name_onyen_substring1  = anti_name_onyen_substring1 + fake_first_name1\n",
    "                if ((first_name1 != \"\") and (last_name1 != \"\")):\n",
    "                    name_onyen_substring1 = name_onyen_substring1 + \" \"\n",
    "                    anti_name_onyen_substring1 = anti_name_onyen_substring1 + \" \"\n",
    "                if (last_name1 != \"\"):\n",
    "                    name_onyen_substring1 = name_onyen_substring1 + str(result_last1.hexdigest())\n",
    "                    anti_name_onyen_substring1 = anti_name_onyen_substring1 + fake_last_name1\n",
    "                if (onyen1 != \"\"):\n",
    "                    name_onyen_substring1 = name_onyen_substring1 + \" \" + \"(\" + str(result_onyen1.hexdigest()) + \")\"\n",
    "                    anti_name_onyen_substring1 = anti_name_onyen_substring1 + \" \" + \"(\" + fake_onyen1 + \")\"\n",
    "                else:\n",
    "                    name_onyen_substring1 = name_onyen_substring1 + \" \" + \"(\" + \")\"\n",
    "                    anti_name_onyen_substring1 = anti_name_onyen_substring1 + \" \" + \"(\" + \")\"\n",
    "                    \n",
    "                hashed_str = hashed_str + name_onyen_substring1 + \" \" + rows[idx2:idx3 + len(sub3)] + \" \"\n",
    "                anti_hashed_str = anti_hashed_str + anti_name_onyen_substring1 + \" \" + rows[idx2:idx3 + len(sub3)] + \" \"\n",
    "                    \n",
    "                if (first_name2 != \"\"):\n",
    "                    name_onyen_substring2 = name_onyen_substring2 + str(result_first2.hexdigest())\n",
    "                    anti_name_onyen_substring2 = anti_name_onyen_substring2 + fake_first_name2\n",
    "                if ((first_name2 != \"\") and (last_name2 != \"\")):\n",
    "                    name_onyen_substring2 = name_onyen_substring2 + \" \"\n",
    "                    anti_name_onyen_substring2 = anti_name_onyen_substring2 + \" \"\n",
    "                if (last_name2 != \"\"):\n",
    "                    name_onyen_substring2 = name_onyen_substring2 + str(result_last2.hexdigest())\n",
    "                    anti_name_onyen_substring2 = anti_name_onyen_substring2 + fake_last_name2\n",
    "                if (onyen2 != \"\"):\n",
    "                    name_onyen_substring2 = name_onyen_substring2 + \" \" + \"(\" + str(result_onyen2.hexdigest()) + \")\"\n",
    "                    anti_name_onyen_substring2 = anti_name_onyen_substring2 + \" \" + \"(\" + fake_onyen2 + \")\"\n",
    "                else:\n",
    "                    name_onyen_substring2 = name_onyen_substring2 + \" \" + \"(\" + \")\"\n",
    "                    anti_name_onyen_substring2 = anti_name_onyen_substring2 + \" \" + \"(\" + \")\"\n",
    "\n",
    "                hashed_str = hashed_str + name_onyen_substring2 + rows[idx4:]\n",
    "                anti_hashed_str = anti_hashed_str + anti_name_onyen_substring2 + rows[idx4:]\n",
    "                \n",
    "                # print(\"Hashed message: \" + hashed_str)\n",
    "                answer.append(hashed_str)\n",
    "                fake_name_answer.append(anti_hashed_str)\n",
    "                hash_map.update({name1 + \" (\" + onyen1 + \")\":name_onyen_substring1, name2 + \" (\" + onyen2 + \")\":name_onyen_substring2})\n",
    "                if (first_name1 != \"\") and (last_name1 != \"\") and (onyen1 != \"\"):\n",
    "                    anti_hash_map.update({anti_name_onyen_substring1:result_first1.hexdigest() + \" \" + result_last1.hexdigest() + \" (\" + result_onyen1.hexdigest() + \")\"})\n",
    "                elif (first_name1 == \"\") and (onyen1 == \"\"):\n",
    "                    anti_hash_map.update({anti_name_onyen_substring1:result_last1.hexdigest() + \" ()\"})\n",
    "                elif (last_name1 != \"\") and (onyen1 != \"\"):\n",
    "                    anti_hash_map.update({anti_name_onyen_substring1:result_first1.hexdigest() + \" ()\"})\n",
    "                elif (first_name1 == \"\"):\n",
    "                    anti_hash_map.update({anti_name_onyen_substring1:result_last1.hexdigest() + \" (\" + result_onyen1.hexdigest() + \")\"})\n",
    "                elif (last_name1 == \"\"):\n",
    "                    anti_hash_map.update({anti_name_onyen_substring1:result_first1.hexdigest() + \" (\" + result_onyen1.hexdigest() + \")\"})\n",
    "                elif (onyen1 == \"\"):\n",
    "                    anti_hash_map.update({anti_name_onyen_substring1:result_first1.hexdigest() + \" \" + result_last1.hexdigest() + \" ()\"})\n",
    "                    \n",
    "                if (first_name2 != \"\") and (last_name2 != \"\") and (onyen2 != \"\"):\n",
    "                    anti_hash_map.update({anti_name_onyen_substring2:result_first2.hexdigest() + \" \" + result_last2.hexdigest() + \" (\" + result_onyen2.hexdigest() + \")\"})\n",
    "                elif (first_name2 == \"\") and (onyen2 == \"\"):\n",
    "                    anti_hash_map.update({anti_name_onyen_substring2:result_last2.hexdigest() + \" ()\"})\n",
    "                elif (last_name2 != \"\") and (onyen2 != \"\"):\n",
    "                    anti_hash_map.update({anti_name_onyen_substring2:result_first2.hexdigest() + \" ()\"})\n",
    "                elif (first_name2 == \"\"):\n",
    "                    anti_hash_map.update({anti_name_onyen_substring2:result_last2.hexdigest() + \" (\" + result_onyen2.hexdigest() + \")\"})\n",
    "                elif (last_name2 == \"\"):\n",
    "                    anti_hash_map.update({anti_name_onyen_substring2:result_first2.hexdigest() + \" (\" + result_onyen2.hexdigest() + \")\"})\n",
    "                elif (onyen2 == \"\"):\n",
    "                    anti_hash_map.update({anti_name_onyen_substring2:result_first2.hexdigest() + \" \" + result_last2.hexdigest() + \" ()\"})\n",
    "        else:\n",
    "            answer.append(rows)\n",
    "            fake_name_answer.append(rows)\n",
    "    \n",
    "    # Save the hashed txt file\n",
    "    with open(txt_file.split('.')[0] + \"(hashed_chat).txt\", 'w') as f:\n",
    "            for line in answer:\n",
    "                f.write(line)\n",
    "    # Save the fake_named txt file\n",
    "    with open(txt_file.split('.')[0] + \"(fake_name_chat).txt\", 'w') as f:\n",
    "            for line in fake_name_answer:\n",
    "                f.write(line)\n",
    "    # Save the hashmap in json file\n",
    "    with open(txt_file.split('.')[0] +'(HashTable_chat).json', 'w') as fp:\n",
    "        json.dump(hash_map, fp)\n",
    "    # Save the anti_hashmap in json file\n",
    "    with open(txt_file.split('.')[0] +'(Anti_HashTable_chat).json', 'w') as fp:\n",
    "        json.dump(anti_hash_map, fp)\n",
    "    # Update fake first names.csv\n",
    "    delete_file = open(first_name_csv_file, \"w\")\n",
    "    delete_file.truncate()\n",
    "    with open(first_name_csv_file, 'w') as f:\n",
    "            for line in first_name_array:\n",
    "                f.write(line)\n",
    "    # Update fake last names.csv\n",
    "    delete_file = open(last_name_csv_file, \"w\")\n",
    "    delete_file.truncate()\n",
    "    with open(last_name_csv_file, 'w') as f:\n",
    "            for line in last_name_array:\n",
    "                f.write(line)\n",
    "    # Update the general_anti_hashmap in json file\n",
    "    delete_file = open(general_first_name_hashmap_json_file, \"w\")\n",
    "    delete_file.truncate()\n",
    "    with open(general_first_name_hashmap_json_file, 'w') as fp:\n",
    "        json.dump(general_first_name_hashmap, fp)\n",
    "    delete_file = open(general_last_name_hashmap_json_file, \"w\")\n",
    "    delete_file.truncate()\n",
    "    with open(general_last_name_hashmap_json_file, 'w') as fp:\n",
    "        json.dump(general_last_name_hashmap, fp)\n",
    "    delete_file = open(general_onyen_hashmap_json_file, \"w\")\n",
    "    delete_file.truncate()\n",
    "    with open(general_onyen_hashmap_json_file, 'w') as fp:\n",
    "        json.dump(general_onyen_hashmap, fp)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_hash_audio_transcript(vtt_file, csv_file, first_name_csv_file, last_name_csv_file, general_first_name_hashmap_json_file, general_last_name_hashmap_json_file, general_onyen_hashmap_json_file):\n",
    "    # Get updated first and last name list\n",
    "    first_name_array, last_name_array = first_and_last_name_list(first_name_csv_file, last_name_csv_file)\n",
    "    # Get updated general anti_hashmap\n",
    "    with open(general_first_name_hashmap_json_file) as f:\n",
    "        general_first_name_hashmap = json.load(f)\n",
    "    with open(general_last_name_hashmap_json_file) as f:\n",
    "        general_last_name_hashmap = json.load(f)\n",
    "    with open(general_onyen_hashmap_json_file) as f:\n",
    "        general_onyen_hashmap = json.load(f)\n",
    "    # Establish the name-onyen dictionary\n",
    "    name_onyen_dictionary = dictionary_initialization_chat_and_transcript(csv_file)\n",
    "    hashed_first_names = ()\n",
    "    hashed_last_names = ()\n",
    "    hashed_onyen = ()\n",
    "    fake_first_name_hashed_value_dictionary = {}\n",
    "    fake_last_name_hashed_value_dictionary = {}\n",
    "    fake_onyen_hashed_value_dictionary = {}\n",
    "    with open(vtt_file) as f:\n",
    "        lines = f.readlines()\n",
    "    lines = np.array(lines)\n",
    "    answer = []\n",
    "    fake_name_answer = []\n",
    "    hash_map = {}\n",
    "    anti_hash_map = {}\n",
    "    for rows in lines:\n",
    "        # print(\"Original message: \" + rows)\n",
    "        if \":\" in rows and rows.lstrip().rstrip() != \"WEBVTT\" and rows.lstrip().rstrip() != \"\" and rows[0] not in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'] and rows[2] != \":\":\n",
    "            # initializing substrings\n",
    "            sub = \": \"\n",
    "\n",
    "            # getting index of substrings\n",
    "            idx = rows.index(sub)\n",
    "\n",
    "            # length of substring 1 is added to\n",
    "            # get string from next character\n",
    "            name = rows[0: idx].lstrip().rstrip() # remove spaces\n",
    "            if (len(name.split(\" \")) < 2): # No first or last name\n",
    "                first_name = name.split(\" \")[0]\n",
    "                last_name = \"\"\n",
    "            else: # 这里存在一个问题，如果一个人的名字有不止一个空格（如middle name），将会导致first name与last name的选取出错\n",
    "                first_name = name.split(\" \")[0]\n",
    "                last_name = name.split(\" \")[1]\n",
    "\n",
    "            # Using the dictionary to convert the names to corresponding onyens\n",
    "            if name not in name_onyen_dictionary.keys():\n",
    "                onyen = '*'+name+'*'\n",
    "            else:\n",
    "                onyen = name_onyen_dictionary[name]\n",
    "\n",
    "            # sha-256 encryption\n",
    "            if (first_name != \"\"):\n",
    "                encoded_first = first_name.encode()\n",
    "                result_first = hashlib.sha256(encoded_first)\n",
    "                if (result_first.hexdigest() not in hashed_first_names) and (result_first.hexdigest() not in general_first_name_hashmap): # First time that encounters a first name\n",
    "                    hashed_first_names = hashed_first_names + (result_first.hexdigest(),)\n",
    "                    fake_first_name = first_name_array[0].replace(\"\\n\", \"\") # Always pick the first element\n",
    "                    first_name_array = np.delete(first_name_array, 0) # Eliminate the name once it is used to prevent using one fake name for two different real names\n",
    "                    fake_first_name_hashed_value_dictionary.update({result_first.hexdigest():fake_first_name})\n",
    "                    general_first_name_hashmap.update({result_first.hexdigest():fake_first_name})\n",
    "                elif (result_first.hexdigest() not in hashed_first_names) and (result_first.hexdigest() in general_first_name_hashmap): # NOT first time that encounters a first name\n",
    "                    fake_first_name = general_first_name_hashmap[result_first.hexdigest()]\n",
    "                    fake_first_name_hashed_value_dictionary.update({result_first.hexdigest():fake_first_name})\n",
    "                elif (result_first.hexdigest() in hashed_first_names) and (result_first.hexdigest() not in general_first_name_hashmap): # NOT first time that encounters a first name\n",
    "                    fake_first_name = fake_first_name_hashed_value_dictionary[result_first.hexdigest()]\n",
    "                    general_first_name_hashmap.update({result_first.hexdigest():fake_first_name})\n",
    "                else:\n",
    "                    fake_first_name = fake_first_name_hashed_value_dictionary[result_first.hexdigest()]\n",
    "            else: # First name is \"\"\n",
    "                fake_first_name = \"\"\n",
    "\n",
    "            if (last_name != \"\"):\n",
    "                encoded_last = last_name.encode()\n",
    "                result_last = hashlib.sha256(encoded_last)\n",
    "                if (result_last.hexdigest() not in hashed_last_names) and (result_last.hexdigest() not in general_last_name_hashmap): # First time that encounters a last name\n",
    "                    hashed_last_names = hashed_last_names + (result_last.hexdigest(),)\n",
    "                    fake_last_name = last_name_array[0].replace(\"\\n\", \"\") # Always pick the first element\n",
    "                    last_name_array = np.delete(last_name_array, 0) # Eliminate the name once it is used to prevent using one fake name for two different real names\n",
    "                    fake_last_name_hashed_value_dictionary.update({result_last.hexdigest():fake_last_name})\n",
    "                    general_last_name_hashmap.update({result_last.hexdigest():fake_last_name})\n",
    "                elif (result_last.hexdigest() not in hashed_last_names) and (result_last.hexdigest() in general_last_name_hashmap): # NOT first time that encounters a last name\n",
    "                    fake_last_name = general_last_name_hashmap[result_last.hexdigest()]\n",
    "                    fake_last_name_hashed_value_dictionary.update({result_last.hexdigest():fake_last_name})\n",
    "                elif (result_last.hexdigest() in hashed_last_names) and (result_last.hexdigest() not in general_last_name_hashmap): # NOT first time that encounters a last name\n",
    "                    fake_last_name = fake_last_name_hashed_value_dictionary[result_last.hexdigest()]\n",
    "                    general_last_name_hashmap.update({result_last.hexdigest():fake_last_name})\n",
    "                else:\n",
    "                    fake_last_name = fake_last_name_hashed_value_dictionary[result_last.hexdigest()]\n",
    "            else: # Last name is \"\"\n",
    "                fake_last_name = \"\"\n",
    "\n",
    "            if (onyen != \"\"):\n",
    "                encoded_onyen = onyen.encode()\n",
    "                result_onyen = hashlib.sha256(encoded_onyen)\n",
    "                if (result_onyen.hexdigest() not in hashed_onyen) and (result_onyen.hexdigest() not in general_onyen_hashmap): # First time that encounters an onyen\n",
    "                    hashed_onyen = hashed_onyen + (result_onyen.hexdigest(),)\n",
    "                    if (fake_first_name != \"\") and (fake_last_name != \"\"):\n",
    "                        fake_onyen = fake_first_name + fake_last_name # 这里暂时不处理同名同姓的不同人的问题，因为如果两人名字相同但onyen不同，在Zoom由name map到onyen时就会报错\n",
    "                    elif (fake_first_name == \"\") and (fake_last_name != \"\"):\n",
    "                        fake_onyen = fake_last_name + fake_last_name\n",
    "                    else:\n",
    "                        fake_onyen = fake_first_name + fake_first_name\n",
    "                    fake_onyen_hashed_value_dictionary.update({result_onyen.hexdigest():fake_onyen})\n",
    "                    general_onyen_hashmap.update({result_onyen.hexdigest():fake_onyen})\n",
    "                elif (result_onyen.hexdigest() not in hashed_onyen) and (result_onyen.hexdigest() in general_onyen_hashmap): # NOT first time that encounters an onyen\n",
    "                    fake_onyen = general_onyen_hashmap[result_onyen.hexdigest()]\n",
    "                    fake_onyen_hashed_value_dictionary.update({result_onyen.hexdigest():fake_onyen})\n",
    "                elif (result_onyen.hexdigest() in hashed_onyen) and (result_onyen.hexdigest() not in general_onyen_hashmap): # NOT first time that encounters an onyen\n",
    "                    fake_onyen = fake_onyen_hashed_value_dictionary[result_onyen.hexdigest()]\n",
    "                    general_onyen_hashmap.update({result_onyen.hexdigest():fake_onyen})\n",
    "                else:\n",
    "                    fake_onyen = fake_onyen_hashed_value_dictionary[result_onyen.hexdigest()]\n",
    "            else: # Onyen is \"\"\n",
    "                fake_onyen = \"\"\n",
    "\n",
    "            hashed_str = \"\"\n",
    "            anti_hashed_str = \"\"\n",
    "            name_onyen_substring = \"\"\n",
    "            anti_name_onyen_substring = \"\"\n",
    "\n",
    "            if (first_name != \"\"):\n",
    "                name_onyen_substring = name_onyen_substring + str(result_first.hexdigest())\n",
    "                anti_name_onyen_substring = anti_name_onyen_substring + fake_first_name\n",
    "            if ((first_name != \"\") and (last_name != \"\")):\n",
    "                name_onyen_substring = name_onyen_substring + \" \"\n",
    "                anti_name_onyen_substring = anti_name_onyen_substring + \" \"\n",
    "            if (last_name != \"\"):\n",
    "                name_onyen_substring = name_onyen_substring + str(result_last.hexdigest())\n",
    "                anti_name_onyen_substring = anti_name_onyen_substring + fake_last_name\n",
    "            if (onyen != \"\"):\n",
    "                name_onyen_substring = name_onyen_substring + \" \" + \"(\" + str(result_onyen.hexdigest()) + \")\"\n",
    "                anti_name_onyen_substring = anti_name_onyen_substring + \" \" + \"(\" + fake_onyen + \")\"\n",
    "            else:\n",
    "                name_onyen_substring = name_onyen_substring + \" \" + \"(\" + \")\"\n",
    "                anti_name_onyen_substring = anti_name_onyen_substring + \" \" + \"(\" + \")\"\n",
    "\n",
    "            hashed_str = hashed_str + name_onyen_substring + rows[idx:]\n",
    "            anti_hashed_str = anti_hashed_str + anti_name_onyen_substring + rows[idx:]\n",
    "\n",
    "            # print(\"Hashed message: \" + hashed_str)\n",
    "            answer.append(hashed_str)\n",
    "            fake_name_answer.append(anti_hashed_str)\n",
    "            hash_map.update({name + \" (\" + onyen + \")\":name_onyen_substring})\n",
    "            if (first_name != \"\") and (last_name != \"\") and (onyen != \"\"):\n",
    "                anti_hash_map.update({anti_name_onyen_substring:result_first.hexdigest() + \" \" + result_last.hexdigest() + \" (\" + result_onyen.hexdigest() + \")\"})\n",
    "            elif (first_name == \"\") and (onyen == \"\"):\n",
    "                anti_hash_map.update({anti_name_onyen_substring:result_last.hexdigest() + \" ()\"})\n",
    "            elif (last_name != \"\") and (onyen != \"\"):\n",
    "                anti_hash_map.update({anti_name_onyen_substring:result_first.hexdigest() + \" ()\"})\n",
    "            elif (first_name == \"\"):\n",
    "                anti_hash_map.update({anti_name_onyen_substring:result_last.hexdigest() + \" (\" + result_onyen.hexdigest() + \")\"})\n",
    "            elif (last_name == \"\"):\n",
    "                anti_hash_map.update({anti_name_onyen_substring:result_first.hexdigest() + \" (\" + result_onyen.hexdigest() + \")\"})\n",
    "            elif (onyen == \"\"):\n",
    "                anti_hash_map.update({anti_name_onyen_substring:result_first.hexdigest() + \" \" + result_last.hexdigest() + \" ()\"})\n",
    "        else:\n",
    "            answer.append(rows)\n",
    "            fake_name_answer.append(rows)\n",
    "    # Save the hashed txt file\n",
    "    with open(vtt_file.split('.')[0] + \"(hashed_audio_transcript).txt\", 'w') as f:\n",
    "            for line in answer:\n",
    "                f.write(line)\n",
    "    # Save the fake_named txt file\n",
    "    with open(vtt_file.split('.')[0] + \"(fake_name_transcript).txt\", 'w') as f:\n",
    "            for line in fake_name_answer:\n",
    "                f.write(line)\n",
    "    # Save the hashmap in json file\n",
    "    with open(vtt_file.split('.')[0] +'(HashTable_audio_transcript).json', 'w') as fp:\n",
    "        json.dump(hash_map, fp)\n",
    "    # Save the anti_hashmap in json file\n",
    "    with open(vtt_file.split('.')[0] +'(Anti_HashTable_transcript).json', 'w') as fp:\n",
    "        json.dump(anti_hash_map, fp)\n",
    "    # Update fake first names.csv\n",
    "    delete_file = open(first_name_csv_file, \"w\")\n",
    "    delete_file.truncate()\n",
    "    with open(first_name_csv_file, 'w') as f:\n",
    "            for line in first_name_array:\n",
    "                f.write(line)\n",
    "    # Update fake last names.csv\n",
    "    delete_file = open(last_name_csv_file, \"w\")\n",
    "    delete_file.truncate()\n",
    "    with open(last_name_csv_file, 'w') as f:\n",
    "            for line in last_name_array:\n",
    "                f.write(line)\n",
    "    # Update the general_anti_hashmap in json file\n",
    "    delete_file = open(general_first_name_hashmap_json_file, \"w\")\n",
    "    delete_file.truncate()\n",
    "    with open(general_first_name_hashmap_json_file, 'w') as fp:\n",
    "        json.dump(general_first_name_hashmap, fp)\n",
    "    delete_file = open(general_last_name_hashmap_json_file, \"w\")\n",
    "    delete_file.truncate()\n",
    "    with open(general_last_name_hashmap_json_file, 'w') as fp:\n",
    "        json.dump(general_last_name_hashmap, fp)\n",
    "    delete_file = open(general_onyen_hashmap_json_file, \"w\")\n",
    "    delete_file.truncate()\n",
    "    with open(general_onyen_hashmap_json_file, 'w') as fp:\n",
    "        json.dump(general_onyen_hashmap, fp)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip everything in a given path while keeping the original structure\n",
    "def unzipdir(path):  \n",
    "    # unzip everything in this directory first\n",
    "    for file in os.listdir(path):\n",
    "        #print(file)\n",
    "        if (file[-4:]=='.zip'):\n",
    "            filename=file[:-4] #remove .zip\n",
    "            with zipfile.ZipFile(path+'\\\\'+file, 'r') as zip_ref:\n",
    "                zip_ref.extractall(path+'\\\\'+filename)\n",
    "            # remove empty folder or folder with only one subfolder\n",
    "            if(len(os.listdir(path+'\\\\'+filename))==1):\n",
    "                for i in os.listdir(path+'\\\\'+filename):\n",
    "                    for j in os.listdir(path+'\\\\'+filename+'\\\\'+i):\n",
    "                        shutil.move(path+'\\\\'+filename+'\\\\'+i+'\\\\'+j,path+'\\\\'+filename)\n",
    "                    if not os.listdir(path+'\\\\'+filename+'\\\\'+i):\n",
    "                        os.removedirs(path+'\\\\'+filename+'\\\\'+i)\n",
    "            os.remove(path+'\\\\'+file) # remove all zip files\n",
    "    for file2 in os.listdir(path):\n",
    "        if (os.path.isdir(path+'\\\\'+file2)):\n",
    "            unzipdir(path+'\\\\'+file2)\n",
    "#unzipdir('C:\\\\Users\\\\PC-002\\\\OneDrive\\\\UNC\\\\Research\\\\Dewan\\\\CAT\\\\try1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymyzer(path,second_path,third_path,csv_file, first_name_csv_file, last_name_csv_file, general_first_name_hashmap_json_file, general_last_name_hashmap_json_file, general_onyen_hashmap_json_file):\n",
    "    file_list = os.listdir(path)\n",
    "    for i in file_list:\n",
    "        if (('hashed' not in i) and ('HashTable' not in i) and ('fake_name' not in i) and ('.txt' in i)): #chat txt files\n",
    "            extract_and_hash_chat(path+'\\\\'+i, csv_file, first_name_csv_file, last_name_csv_file, general_first_name_hashmap_json_file, general_last_name_hashmap_json_file, general_onyen_hashmap_json_file)\n",
    "            os.remove(path+'\\\\'+i)  #remove original file\n",
    "        if (('hashed' not in i) and ('HashTable' not in i) and ('fake_name' not in i) and ('.vtt' in i)): # audio transcript vtt files\n",
    "            extract_and_hash_audio_transcript(path+'\\\\'+i, csv_file, first_name_csv_file, last_name_csv_file, general_first_name_hashmap_json_file, general_last_name_hashmap_json_file, general_onyen_hashmap_json_file)\n",
    "            os.remove(path+'\\\\'+i)  #remove original file\n",
    "    # move files in separate files\n",
    "    file_list = os.listdir(path)\n",
    "    try:\n",
    "        os.makedirs(path+'\\\\'+'hashedChats') # create destination directory, if needed (similar to mkdir -p)\n",
    "        os.makedirs(path+'\\\\'+'hashedTranscript') # create destination directory, if needed (similar to mkdir -p)\n",
    "    except OSError:\n",
    "        # The directory already existed, nothing to do\n",
    "        pass\n",
    "    try:\n",
    "        os.makedirs(second_path+'\\\\'+'hashTables(Chat)') # create destination directory, if needed (similar to mkdir -p)\n",
    "        os.makedirs(second_path+'\\\\'+'hashTables(Audio Transcript)') # create destination directory, if needed (similar to mkdir -p)\n",
    "    except OSError:\n",
    "        # The directory already existed, nothing to do\n",
    "        pass\n",
    "    try:\n",
    "        os.makedirs(third_path+'\\\\'+'FakeNameChats') # create destination directory, if needed (similar to mkdir -p)\n",
    "        os.makedirs(third_path+'\\\\'+'FakeNameTranscript') # create destination directory, if needed (similar to mkdir -p)\n",
    "        os.makedirs(third_path+'\\\\'+'Anti_hashTables(Chat)') # create destination directory, if needed (similar to mkdir -p)\n",
    "        os.makedirs(third_path+'\\\\'+'Anti_hashTables(Audio Transcript)') # create destination directory, if needed (similar to mkdir -p)\n",
    "    except OSError:\n",
    "        # The directory already existed, nothing to do\n",
    "        pass\n",
    "\n",
    "    for j in file_list:\n",
    "        if ('(hashed_chat).txt' in j):\n",
    "            shutil.move(path+'\\\\'+j,path+'\\\\'+'hashedChats')\n",
    "        if ('(hashed_audio_transcript).txt' in j):\n",
    "            shutil.move(path+'\\\\'+j,path+'\\\\'+'hashedTranscript')\n",
    "        if ('(HashTable_chat).json' in j):\n",
    "            shutil.move(path+'\\\\'+j,second_path+'\\\\'+'hashTables(Chat)')\n",
    "        if ('(HashTable_audio_transcript).json' in j):\n",
    "            shutil.move(path+'\\\\'+j,second_path+'\\\\'+'hashTables(Audio Transcript)')\n",
    "        if ('(fake_name_chat).txt' in j):\n",
    "            shutil.move(path+'\\\\'+j,third_path+'\\\\'+'FakeNameChats')\n",
    "        if ('(fake_name_transcript).txt' in j):\n",
    "            shutil.move(path+'\\\\'+j,third_path+'\\\\'+'FakeNameTranscript')\n",
    "        if ('(Anti_HashTable_chat).json' in j):\n",
    "            shutil.move(path+'\\\\'+j,third_path+'\\\\'+'Anti_hashTables(Chat)')\n",
    "        if ('(Anti_HashTable_transcript).json' in j):\n",
    "            shutil.move(path+'\\\\'+j,third_path+'\\\\'+'Anti_hashTables(Audio Transcript)')\n",
    "    # recursively go through every subfolder\n",
    "    for file2 in os.listdir(path):\n",
    "        if (os.path.isdir(path+'\\\\'+file2) and ('hashedChats' not in file2) and ('hashedTranscript' not in file2) and ('hashTables(Chat)' not in file2) and ('hashTables(Audio Transcript)' not in file2) and ('FakeNameChats' not in file2) and ('FakeNameTranscript' not in file2) and ('Anti_hashTables(Chat)' not in file2) and ('Anti_hashTables(Audio Transcript)' not in file2)):\n",
    "            anonymyzer(path+'\\\\'+file2 ,second_path+'\\\\'+file2,third_path+'\\\\'+file2, csv_file, first_name_csv_file, last_name_csv_file, general_first_name_hashmap_json_file, general_last_name_hashmap_json_file, general_onyen_hashmap_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty folders in a given path\n",
    "def removeempty(path):\n",
    "    file_list = os.listdir(path)\n",
    "    for i in file_list:\n",
    "        if (os.path.isdir(path+'\\\\'+i)):\n",
    "            if len(os.listdir(path+'\\\\'+i))==0:\n",
    "                shutil.rmtree(path+'\\\\'+i)\n",
    "            else:\n",
    "                removeempty(path+'\\\\'+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymyzer_driver(filename, csv_file, first_name_csv_file, last_name_csv_file, general_first_name_hashmap_json_file, general_last_name_hashmap_json_file, general_onyen_hashmap_json_file):\n",
    "    cwd = os.getcwd()\n",
    "    path = cwd\n",
    "    try:\n",
    "        os.makedirs(cwd+'\\\\'+'hashTables') \n",
    "    except OSError:\n",
    "            # The directory already existed, nothing to do\n",
    "        pass\n",
    "    # general unzip tha root file\n",
    "    with zipfile.ZipFile(cwd+'\\\\'+filename+'.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(cwd+'\\\\'+filename)\n",
    "    if(len(os.listdir(path+'\\\\'+filename))==1):\n",
    "        for i in os.listdir(path+'\\\\'+filename):\n",
    "            for j in os.listdir(path+'\\\\'+filename+'\\\\'+i):\n",
    "                shutil.move(path+'\\\\'+filename+'\\\\'+i+'\\\\'+j,path+'\\\\'+filename)\n",
    "            if not os.listdir(path+'\\\\'+filename+'\\\\'+i):\n",
    "                os.removedirs(path+'\\\\'+filename+'\\\\'+i)\n",
    "    # unzip all files\n",
    "    unzipdir(cwd+'\\\\'+filename)\n",
    "    # Create 3 enpty general anti_hashmap files\n",
    "    with open(general_first_name_hashmap_json_file, 'w') as fp:\n",
    "        json.dump({}, fp)\n",
    "    with open(general_last_name_hashmap_json_file, 'w') as fp:\n",
    "        json.dump({}, fp)\n",
    "    with open(general_onyen_hashmap_json_file, 'w') as fp:\n",
    "        json.dump({}, fp)\n",
    "    # anonymize all files\n",
    "    first_path = cwd+'\\\\'+filename\n",
    "    second_path = cwd+'\\\\'+filename+'_hashTables'\n",
    "    third_path = cwd+'\\\\'+filename+'_FakeName + Anti_hashTables'\n",
    "    anonymyzer(first_path,second_path,third_path, csv_file, first_name_csv_file, last_name_csv_file, general_first_name_hashmap_json_file, general_last_name_hashmap_json_file, general_onyen_hashmap_json_file)\n",
    "    # remove all empty folders\n",
    "    removeempty(path+'\\\\'+filename)\n",
    "    removeempty(path+'\\\\'+filename+'_hashTables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to get the result\n",
    "filename = 'try3'  # the name of zip file without .zip\n",
    "csv_file = \"grades.csv\" #change the zip file name here WITH extension\n",
    "first_name_csv_file = \"fake first names.csv\"\n",
    "last_name_csv_file = \"fake last names.csv\"\n",
    "general_first_name_hashmap_json_file = \"General Anti_HashMap(first name).json\"\n",
    "general_last_name_hashmap_json_file = \"General Anti_HashMap(last name).json\"\n",
    "general_onyen_hashmap_json_file = \"General Anti_HashMap(onyen).json\"\n",
    "anonymyzer_driver(filename, csv_file, first_name_csv_file, last_name_csv_file, general_first_name_hashmap_json_file, general_last_name_hashmap_json_file, general_onyen_hashmap_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
