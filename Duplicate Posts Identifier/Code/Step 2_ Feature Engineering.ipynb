{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab7dadcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LTX_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LTX_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, ngrams\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "color = sns.color_palette()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53cae39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(399787, 14)\n",
      "(4290, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: DtypeWarning: Columns (5,6,7,9,11,12) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# Supported data_clean_type (DO NOT forget to put on \"()\", also, if data_clean_type is NOT empty string, please put a \" \" before (cleaned)):\n",
    "# empty string, no character\n",
    "# (cleaned)\n",
    "# (cleaned)(hyper_cleaned)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)(words_shortened)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)(alternative_stopwords_used)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)(alternative_stopwords_used)(words_shortened)\n",
    "data_clean_type = \" (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)(alternative_stopwords_used)(words_shortened)\"\n",
    "train_df = pd.read_csv(\"train\" + data_clean_type + \".csv\")\n",
    "test_df = pd.read_csv(\"test\" + data_clean_type + \".csv\")\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32555d21",
   "metadata": {},
   "source": [
    "# Add word match share feature (feature 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "131754e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qs = pd.Series(train_df['question1'].tolist() + train_df['question2'].tolist()).astype(str)\n",
    "\n",
    "# If a word appears only once, we ignore it completely (likely a typo)\n",
    "# Epsilon defines a smoothing constant, which makes the effect of extremely rare words smaller\n",
    "def get_weight(count, eps=10000, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "\n",
    "eps = 5000 \n",
    "words = (\" \".join(train_qs)).lower().split()\n",
    "counts = Counter(words)\n",
    "weights = {word: get_weight(count) for word, count in counts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd171f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_match_share(row):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row[3]).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row[4]).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        \n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def word_match_share_test(row):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row[1]).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row[2]).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        \n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "train_df['word_match_share'] = train_df.apply(word_match_share, axis=1, raw=True)\n",
    "test_df['word_match_share'] = test_df.apply(word_match_share_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864333de",
   "metadata": {},
   "source": [
    "# Add TF-IDF word match share feature (feature 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d887317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LTX_\\AppData\\Local\\Temp/ipykernel_22996/581765615.py:15: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  R = np.sum(shared_weights) / np.sum(total_weights)\n"
     ]
    }
   ],
   "source": [
    "def tfidf_word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row[3]).lower().split():\n",
    "        q1words[word] = 1\n",
    "    for word in str(row[4]).lower().split():\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "def tfidf_word_match_share_test(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row[1]).lower().split():\n",
    "        q1words[word] = 1\n",
    "    for word in str(row[2]).lower().split():\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "train_df['tfidf_word_match_share'] = train_df.apply(tfidf_word_match_share, axis=1, raw=True)\n",
    "test_df['tfidf_word_match_share'] = test_df.apply(tfidf_word_match_share_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675932cc",
   "metadata": {},
   "source": [
    "# Add TF-IDF word match share for stop words feature (feature 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "265e9f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LTX_\\AppData\\Local\\Temp/ipykernel_22996/1295776133.py:18: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  R = np.sum(shared_weights) / np.sum(total_weights)\n"
     ]
    }
   ],
   "source": [
    "def tfidf_word_match_share_stop(row):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row[3]).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row[4]).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "def tfidf_word_match_share_stop_test(row):\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row[1]).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row[2]).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "train_df['tfidf_word_match'] = train_df.apply(tfidf_word_match_share_stop, axis=1, raw=True)\n",
    "test_df['tfidf_word_match'] = test_df.apply(tfidf_word_match_share_stop_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095a72c0",
   "metadata": {},
   "source": [
    "# Add \"unigrams\" feature (feature 4 and 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1899ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigrams(que):\n",
    "    return [word for word in word_tokenize(que.lower()) if word not in eng_stopwords]\n",
    "\n",
    "def get_common_unigrams(row):\n",
    "    return len( set(row[\"unigrams_ques1\"]).intersection(set(row[\"unigrams_ques2\"])) )\n",
    "\n",
    "def get_common_unigram_ratio(row):\n",
    "    return float(row[\"unigrams_common_count\"]) / max(len( set(row[\"unigrams_ques1\"]).union(set(row[\"unigrams_ques2\"])) ),1)\n",
    "\n",
    "train_df[\"unigrams_ques1\"] = train_df['question1'].apply(lambda x: get_unigrams(str(x)))\n",
    "train_df[\"unigrams_ques2\"] = train_df['question2'].apply(lambda x: get_unigrams(str(x)))\n",
    "train_df[\"unigrams_common_count\"] = train_df.apply(lambda row: get_common_unigrams(row),axis=1)\n",
    "train_df[\"unigrams_common_ratio\"] = train_df.apply(lambda row: get_common_unigram_ratio(row), axis=1)\n",
    "\n",
    "test_df[\"unigrams_ques1\"] = test_df['question1'].apply(lambda x: get_unigrams(str(x)))\n",
    "test_df[\"unigrams_ques2\"] = test_df['question2'].apply(lambda x: get_unigrams(str(x)))\n",
    "test_df[\"unigrams_common_count\"] = test_df.apply(lambda row: get_common_unigrams(row),axis=1)\n",
    "test_df[\"unigrams_common_ratio\"] = test_df.apply(lambda row: get_common_unigram_ratio(row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2926a7",
   "metadata": {},
   "source": [
    "# Add frequency + intersection features(feature 6, 7, and 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be47cfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = pd.concat([train_df[['question1', 'question2']], \\\n",
    "        test_df[['question1', 'question2']]], axis=0).reset_index(drop='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e87cf62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "q_dict = defaultdict(set)\n",
    "for i in range(ques.shape[0]):\n",
    "        q_dict[ques.question1[i]].add(ques.question2[i])\n",
    "        q_dict[ques.question2[i]].add(ques.question1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80f2d877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_freq(row):\n",
    "    return(len(q_dict[row[3]]))\n",
    "    \n",
    "def q2_freq(row):\n",
    "    return(len(q_dict[row[4]]))\n",
    "    \n",
    "def q1_q2_intersect(row):\n",
    "    return(len(set(q_dict[row[3]]).intersection(set(q_dict[row[4]]))))\n",
    "\n",
    "train_df['q1_q2_intersect'] = train_df.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "train_df['q1_freq'] = train_df.apply(q1_freq, axis=1, raw=True)\n",
    "train_df['q2_freq'] = train_df.apply(q2_freq, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d82ca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_freq_test(row):\n",
    "    return(len(q_dict[row[1]]))\n",
    "    \n",
    "def q2_freq_test(row):\n",
    "    return(len(q_dict[row[2]]))\n",
    "    \n",
    "def q1_q2_intersect_test(row):\n",
    "    return(len(set(q_dict[row[1]]).intersection(set(q_dict[row[2]]))))\n",
    "\n",
    "test_df['q1_q2_intersect'] = test_df.apply(q1_q2_intersect_test, axis=1, raw=True)\n",
    "test_df['q1_freq'] = test_df.apply(q1_freq_test, axis=1, raw=True)\n",
    "test_df['q2_freq'] = test_df.apply(q2_freq_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6239d30",
   "metadata": {},
   "source": [
    "# Add jaccard feature (feature 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08cde5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(row):\n",
    "    wic = set(str(row[3]).lower().split()).intersection(set(str(row[4]).lower().split()))\n",
    "    uw = set(str(row[3]).lower().split()).union(set(str(row[4]).lower().split()))\n",
    "    if len(uw) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return (len(wic) / len(uw))\n",
    "\n",
    "def jaccard_test(row):\n",
    "    wic = set(str(row[1]).lower().split()).intersection(set(str(row[2]).lower().split()))\n",
    "    uw = set(str(row[1]).lower().split()).union(set(str(row[2]).lower().split()))\n",
    "    if len(uw) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return (len(wic) / len(uw))\n",
    "\n",
    "train_df['jaccard'] = train_df.apply(jaccard, axis=1, raw=True)\n",
    "test_df['jaccard'] = test_df.apply(jaccard_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e831f441",
   "metadata": {},
   "source": [
    "# Add common words feature (feature 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec9ea346",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_words(row):\n",
    "    return len(set(str(row[3]).lower().split()).intersection(set(str(row[4]).lower().split())))\n",
    "\n",
    "def common_words_test(row):\n",
    "    return len(set(str(row[1]).lower().split()).intersection(set(str(row[2]).lower().split())))\n",
    "\n",
    "train_df['common_words'] = train_df.apply(common_words, axis=1, raw=True)\n",
    "test_df['common_words'] = test_df.apply(common_words_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388481e9",
   "metadata": {},
   "source": [
    "# Add common words for non-stop words feature (feature 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ae02c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_words_stop(row):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    return len([x for x in set(str(row[3]).lower().split()).intersection(set(str(row[4]).lower().split())) if x not in stops])\n",
    "\n",
    "def common_words_stop_test(row):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    return len([x for x in set(str(row[1]).lower().split()).intersection(set(str(row[2]).lower().split())) if x not in stops])\n",
    "\n",
    "train_df['common_words_stop'] = train_df.apply(common_words_stop, axis=1, raw=True)\n",
    "test_df['common_words_stop'] = test_df.apply(common_words_stop_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607ea259",
   "metadata": {},
   "source": [
    "# Add total unique words feature (feature 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9ee4597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_unique_words(row):\n",
    "    return len(set(str(row[3]).lower().split()).union(set(str(row[4]).lower().split())))\n",
    "\n",
    "def total_unique_words_test(row):\n",
    "    return len(set(str(row[1]).lower().split()).union(set(str(row[2]).lower().split())))\n",
    "\n",
    "train_df['total_unique_words'] = train_df.apply(total_unique_words, axis=1, raw=True)\n",
    "test_df['total_unique_words'] = test_df.apply(total_unique_words_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621312d3",
   "metadata": {},
   "source": [
    "# Add total unique words for non-stop words feature (feature 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "621c29a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_unq_words_stop(row):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    return len([x for x in set(str(row[3]).lower().split()).union(set(str(row[4]).lower().split())) if x not in stops])\n",
    "\n",
    "def total_unq_words_stop_test(row):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    return len([x for x in set(str(row[1]).lower().split()).union(set(str(row[2]).lower().split())) if x not in stops])\n",
    "\n",
    "train_df['total_unq_words_stop'] = train_df.apply(total_unq_words_stop, axis=1, raw=True)\n",
    "test_df['total_unq_words_stop'] = test_df.apply(total_unq_words_stop_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a50841",
   "metadata": {},
   "source": [
    "# Add word count difference feature (feature 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dc9f747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wc_diff(row):\n",
    "    return abs(len(str(row[3]).lower().split()) - len(str(row[4]).lower().split()))\n",
    "\n",
    "def wc_diff_test(row):\n",
    "    return abs(len(str(row[1]).lower().split()) - len(str(row[2]).lower().split()))\n",
    "\n",
    "train_df['wc_diff'] = train_df.apply(wc_diff, axis=1, raw=True)\n",
    "test_df['wc_diff'] = test_df.apply(wc_diff_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c965f84f",
   "metadata": {},
   "source": [
    "# Add word count ratio feature (feature 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c31c4396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wc_ratio(row):\n",
    "    l1 = len(str(row[3]).lower().split())\n",
    "    l2 = len(str(row[4]).lower().split())\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "def wc_ratio_test(row):\n",
    "    l1 = len(str(row[1]).lower().split())\n",
    "    l2 = len(str(row[2]).lower().split())\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "train_df['wc_ratio'] = train_df.apply(wc_ratio, axis=1, raw=True)\n",
    "test_df['wc_ratio'] = test_df.apply(wc_ratio_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ea4a97",
   "metadata": {},
   "source": [
    "# Add word count difference for unique words feature (feature 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9159455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wc_diff_unique(row):\n",
    "    return abs(len(set(str(row[3]).lower().split())) - len(set(str(row[4]).lower().split())))\n",
    "\n",
    "def wc_diff_unique_test(row):\n",
    "    return abs(len(set(str(row[1]).lower().split())) - len(set(str(row[2]).lower().split())))\n",
    "\n",
    "train_df['wc_diff_unique'] = train_df.apply(wc_diff_unique, axis=1, raw=True)\n",
    "test_df['wc_diff_unique'] = test_df.apply(wc_diff_unique_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96297c2",
   "metadata": {},
   "source": [
    "# Add word count ratio for unique words feature (feature 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a61fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wc_ratio_unique(row):\n",
    "    l1 = len(set(str(row[3]).lower().split()))\n",
    "    l2 = len(set(str(row[4]).lower().split()))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "def wc_ratio_unique_test(row):\n",
    "    l1 = len(set(str(row[1]).lower().split()))\n",
    "    l2 = len(set(str(row[2]).lower().split()))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "train_df['wc_ratio_unique'] = train_df.apply(wc_ratio_unique, axis=1, raw=True)\n",
    "test_df['wc_ratio_unique'] = test_df.apply(wc_ratio_unique_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d8c876",
   "metadata": {},
   "source": [
    "# Add word count difference for non-stop words feature (feature 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a9fcafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wc_diff_unique_stop(row):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    return abs(len([x for x in set(str(row[3]).lower().split()) if x not in stops]) - len([x for x in set(str(row[4]).lower().split()) if x not in stops]))\n",
    "\n",
    "def wc_diff_unique_stop_test(row):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    return abs(len([x for x in set(str(row[1]).lower().split()) if x not in stops]) - len([x for x in set(str(row[2]).lower().split()) if x not in stops]))\n",
    "\n",
    "train_df['wc_diff_unique_stop'] = train_df.apply(wc_diff_unique_stop, axis=1, raw=True)\n",
    "test_df['wc_diff_unique_stop'] = test_df.apply(wc_diff_unique_stop_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a12dad",
   "metadata": {},
   "source": [
    "# Add wc ratio for stop words feature (feature 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bcfa927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wc_ratio_unique_stop(row):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    l1 = len([x for x in set(str(row[3]).lower().split()) if x not in stops]) \n",
    "    l2 = len([x for x in set(str(row[4]).lower().split()) if x not in stops])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2 > 1:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "def wc_ratio_unique_stop_test(row):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    l1 = len([x for x in set(str(row[1]).lower().split()) if x not in stops])\n",
    "    l2 = len([x for x in set(str(row[2]).lower().split()) if x not in stops])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2 > 1:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "train_df['wc_ratio_unique_stop'] = train_df.apply(wc_ratio_unique_stop, axis=1, raw=True)\n",
    "test_df['wc_ratio_unique_stop'] = test_df.apply(wc_ratio_unique_stop_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0354eef0",
   "metadata": {},
   "source": [
    "# Add same start word feature (feature 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "657ad48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_start_word(row):\n",
    "    if len(str(row[3]).lower().split()) == 0 or len(str(row[4]).lower().split()) == 0:\n",
    "        return np.nan\n",
    "    return int(str(row[3]).lower().split()[0] == str(row[4]).lower().split()[0])\n",
    "\n",
    "def same_start_word_test(row):\n",
    "    if len(str(row[1]).lower().split()) == 0 or len(str(row[2]).lower().split()) == 0:\n",
    "        return np.nan\n",
    "    return int(str(row[1]).lower().split()[0] == str(row[2]).lower().split()[0])\n",
    "\n",
    "train_df['same_start_word'] = train_df.apply(same_start_word, axis=1, raw=True)\n",
    "test_df['same_start_word'] = test_df.apply(same_start_word_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af82854f",
   "metadata": {},
   "source": [
    "# Add character difference feature (feature 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5372c626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_diff(row):\n",
    "    return abs(len(''.join(str(row[3]).lower())) - len(''.join(str(row[4]).lower())))\n",
    "\n",
    "def char_diff_test(row):\n",
    "    return abs(len(''.join(str(row[1]).lower())) - len(''.join(str(row[2]).lower())))\n",
    "\n",
    "train_df['char_diff'] = train_df.apply(char_diff, axis=1, raw=True)\n",
    "test_df['char_diff'] = test_df.apply(char_diff_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf798e2",
   "metadata": {},
   "source": [
    "# Add character length ratio feature (feature 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c34e7c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_ratio(row):\n",
    "    l1 = len(''.join(str(row[3]).lower())) \n",
    "    l2 = len(''.join(str(row[4]).lower()))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2 > 1:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "def char_ratio_test(row):\n",
    "    l1 = len(''.join(str(row[1]).lower())) \n",
    "    l2 = len(''.join(str(row[2]).lower()))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2 > 1:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "train_df['char_ratio'] = train_df.apply(char_ratio, axis=1, raw=True)\n",
    "test_df['char_ratio'] = test_df.apply(char_ratio_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf325962",
   "metadata": {},
   "source": [
    "# Add number of character difference for non-stop words feature (feature 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4346cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_diff_unique_stop(row):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    return abs(len(''.join([x for x in set(str(row[3]).lower().split()) if x not in stops])) - len(''.join([x for x in set(str(row[4]).lower().split()) if x not in stops])))\n",
    "\n",
    "def char_diff_unique_stop_test(row):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    return abs(len(''.join([x for x in set(str(row[1]).lower().split()) if x not in stops])) - len(''.join([x for x in set(str(row[2]).lower().split()) if x not in stops])))\n",
    "\n",
    "train_df['char_diff_unique_stop'] = train_df.apply(char_diff_unique_stop, axis=1, raw=True)\n",
    "test_df['char_diff_unique_stop'] = test_df.apply(char_diff_unique_stop_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4cdf12",
   "metadata": {},
   "source": [
    "# Add Q1 to Q2 word count difference feature (feature 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ef915a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_to_q2_wc_diff(row):\n",
    "    return len(str(row[3]).lower().split()) - len(str(row[4]).lower().split())\n",
    "\n",
    "def q1_to_q2_wc_diff_test(row):\n",
    "    return len(str(row[1]).lower().split()) - len(str(row[2]).lower().split())\n",
    "\n",
    "train_df['q1_to_q2_wc_diff'] = train_df.apply(q1_to_q2_wc_diff, axis=1, raw=True)\n",
    "test_df['q1_to_q2_wc_diff'] = test_df.apply(q1_to_q2_wc_diff_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04424d9a",
   "metadata": {},
   "source": [
    "# Add Q1 to Q2 word count ratio feature (feature 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55b94c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_to_q2_wc_ratio(row):\n",
    "    l1 = len(str(row[3]).lower().split())\n",
    "    l2 = len(str(row[4]).lower().split())\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "def q1_to_q2_wc_ratio_test(row):\n",
    "    l1 = len(str(row[1]).lower().split())\n",
    "    l2 = len(str(row[2]).lower().split())\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "train_df['q1_to_q2_wc_ratio'] = train_df.apply(q1_to_q2_wc_ratio, axis=1, raw=True)\n",
    "test_df['q1_to_q2_wc_ratio'] = test_df.apply(q1_to_q2_wc_ratio_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953139e6",
   "metadata": {},
   "source": [
    "# Add Q1 to Q2 word count difference for unique words feature (feature 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "947e8d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_to_q2_wc_diff_unique(row):\n",
    "    return len(set(str(row[3]).lower().split())) - len(set(str(row[4]).lower().split()))\n",
    "\n",
    "def q1_to_q2_wc_diff_unique_test(row):\n",
    "    return len(set(str(row[1]).lower().split())) - len(set(str(row[2]).lower().split()))\n",
    "\n",
    "train_df['q1_to_q2_wc_diff_unique'] = train_df.apply(q1_to_q2_wc_diff_unique, axis=1, raw=True)\n",
    "test_df['q1_to_q2_wc_diff_unique'] = test_df.apply(q1_to_q2_wc_diff_unique_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216f830d",
   "metadata": {},
   "source": [
    "# Add Q1 to Q2 word count ratio for unique words feature (feature 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4832d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_to_q2_wc_ratio_unique(row):\n",
    "    l1 = len(set(str(row[3]).lower().split()))\n",
    "    l2 = len(set(str(row[4]).lower().split()))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "def q1_to_q2_wc_ratio_unique_test(row):\n",
    "    l1 = len(set(str(row[1]).lower().split()))\n",
    "    l2 = len(set(str(row[2]).lower().split()))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "train_df['q1_to_q2_wc_ratio_unique'] = train_df.apply(q1_to_q2_wc_ratio_unique, axis=1, raw=True)\n",
    "test_df['q1_to_q2_wc_ratio_unique'] = test_df.apply(q1_to_q2_wc_ratio_unique_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f352eb2",
   "metadata": {},
   "source": [
    "# Add Q1 to Q2 word count difference for non-stop words feature (feature 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72175e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_to_q2_wc_diff_unique_stop(row):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    return len([x for x in set(str(row[3]).lower().split()) if x not in stops]) - len([x for x in set(str(row[4]).lower().split()) if x not in stops])\n",
    "\n",
    "def q1_to_q2_wc_diff_unique_stop_test(row):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    return len([x for x in set(str(row[1]).lower().split()) if x not in stops]) - len([x for x in set(str(row[2]).lower().split()) if x not in stops])\n",
    "\n",
    "train_df['q1_to_q2_wc_diff_unique_stop'] = train_df.apply(q1_to_q2_wc_diff_unique_stop, axis=1, raw=True)\n",
    "test_df['q1_to_q2_wc_diff_unique_stop'] = test_df.apply(q1_to_q2_wc_diff_unique_stop_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4536c8",
   "metadata": {},
   "source": [
    "# Add Q1 to Q2 wc ratio for stop words feature (feature 29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33ba0e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_to_q2_wc_ratio_unique_stop(row):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    l1 = len([x for x in set(str(row[3]).lower().split()) if x not in stops]) \n",
    "    l2 = len([x for x in set(str(row[4]).lower().split()) if x not in stops])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "def q1_to_q2_wc_ratio_unique_stop_test(row):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    l1 = len([x for x in set(str(row[1]).lower().split()) if x not in stops])\n",
    "    l2 = len([x for x in set(str(row[2]).lower().split()) if x not in stops])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "train_df['q1_to_q2_wc_ratio_unique_stop'] = train_df.apply(q1_to_q2_wc_ratio_unique_stop, axis=1, raw=True)\n",
    "test_df['q1_to_q2_wc_ratio_unique_stop'] = test_df.apply(q1_to_q2_wc_ratio_unique_stop_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f6d71",
   "metadata": {},
   "source": [
    "# Add Q1 to Q2 character difference feature (feature 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b96985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_to_q2_char_diff(row):\n",
    "    return len(''.join(str(row[3]).lower())) - len(''.join(str(row[4]).lower()))\n",
    "\n",
    "def q1_to_q2_char_diff_test(row):\n",
    "    return len(''.join(str(row[1]).lower())) - len(''.join(str(row[2]).lower()))\n",
    "\n",
    "train_df['q1_to_q2_char_diff'] = train_df.apply(q1_to_q2_char_diff, axis=1, raw=True)\n",
    "test_df['q1_to_q2_char_diff'] = test_df.apply(q1_to_q2_char_diff_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4f91b",
   "metadata": {},
   "source": [
    "# Add Q1 to Q2 character length ratio feature (feature 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "112557dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_to_q2_char_ratio(row):\n",
    "    l1 = len(''.join(str(row[3]).lower())) \n",
    "    l2 = len(''.join(str(row[4]).lower()))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "def q1_to_q2_char_ratio_test(row):\n",
    "    l1 = len(''.join(str(row[1]).lower())) \n",
    "    l2 = len(''.join(str(row[2]).lower()))\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "train_df['q1_to_q2_char_ratio'] = train_df.apply(q1_to_q2_char_ratio, axis=1, raw=True)\n",
    "test_df['q1_to_q2_char_ratio'] = test_df.apply(q1_to_q2_char_ratio_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cc99f5",
   "metadata": {},
   "source": [
    "# Add Q1 to Q2 number of character difference for non-stop words feature (feature 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72d820b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_to_q2_char_diff_unique_stop(row):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    return len(''.join([x for x in set(str(row[3]).lower().split()) if x not in stops])) - len(''.join([x for x in set(str(row[4]).lower().split()) if x not in stops]))\n",
    "\n",
    "def q1_to_q2_char_diff_unique_stop_test(row):\n",
    "    stops = set(stopwords.words('english'))\n",
    "    return len(''.join([x for x in set(str(row[1]).lower().split()) if x not in stops])) - len(''.join([x for x in set(str(row[2]).lower().split()) if x not in stops]))\n",
    "\n",
    "train_df['q1_to_q2_char_diff_unique_stop'] = train_df.apply(q1_to_q2_char_diff_unique_stop, axis=1, raw=True)\n",
    "test_df['q1_to_q2_char_diff_unique_stop'] = test_df.apply(q1_to_q2_char_diff_unique_stop_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b42626",
   "metadata": {},
   "source": [
    "# Add word match share feature using alternative stop words (feature 33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7436ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_match_share_alternative_stop(row):\n",
    "    stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row[3]).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row[4]).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        \n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def word_match_share_alternative_stop_test(row):\n",
    "    stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row[1]).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row[2]).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        \n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "train_df['word_match_share_alternative_stop'] = train_df.apply(word_match_share_alternative_stop, axis=1, raw=True)\n",
    "test_df['word_match_share_alternative_stop'] = test_df.apply(word_match_share_alternative_stop_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f345e4f",
   "metadata": {},
   "source": [
    "# Add TF-IDF word match share for stop words feature using alternative stop words (feature 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7f52a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LTX_\\AppData\\Local\\Temp/ipykernel_22996/3161323762.py:20: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  R = np.sum(shared_weights) / np.sum(total_weights)\n"
     ]
    }
   ],
   "source": [
    "def tfidf_word_match_share_alternative_stop(row):\n",
    "    stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row[3]).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row[4]).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "def tfidf_word_match_share_alternative_stop_test(row):\n",
    "    stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row[1]).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row[2]).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "train_df['tfidf_word_match_share_alternative_stop'] = train_df.apply(tfidf_word_match_share_alternative_stop, axis=1, raw=True)\n",
    "test_df['tfidf_word_match_share_alternative_stop'] = test_df.apply(tfidf_word_match_share_alternative_stop_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293baef2",
   "metadata": {},
   "source": [
    "# Add common words for non-stop words feature using alternative stop words (feature 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f856105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_words_alternative_stop(row):\n",
    "    stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    return len([x for x in set(str(row[3]).lower().split()).intersection(set(str(row[4]).lower().split())) if x not in stops])\n",
    "\n",
    "def common_words_alternative_stop_test(row):\n",
    "    stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    return len([x for x in set(str(row[1]).lower().split()).intersection(set(str(row[2]).lower().split())) if x not in stops])\n",
    "\n",
    "train_df['common_words_alternative_stop'] = train_df.apply(common_words_alternative_stop, axis=1, raw=True)\n",
    "test_df['common_words_alternative_stop'] = test_df.apply(common_words_alternative_stop_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2212ba44",
   "metadata": {},
   "source": [
    "# Add total unique words for non-stop words feature using alternative stop words (feature 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d639e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_unq_words_alternative_stop(row):\n",
    "    stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    return len([x for x in set(str(row[3]).lower().split()).union(set(str(row[4]).lower().split())) if x not in stops])\n",
    "\n",
    "def total_unq_words_alternative_stop_test(row):\n",
    "    stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    return len([x for x in set(str(row[1]).lower().split()).union(set(str(row[2]).lower().split())) if x not in stops])\n",
    "\n",
    "train_df['total_unq_words_alternative_stop'] = train_df.apply(total_unq_words_alternative_stop, axis=1, raw=True)\n",
    "test_df['total_unq_words_alternative_stop'] = test_df.apply(total_unq_words_alternative_stop_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d50a1d",
   "metadata": {},
   "source": [
    "# Add word count difference for non-stop words feature using alternative stop words (feature 37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51422cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wc_diff_unique_alternative_stop(row):\n",
    "    stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    return abs(len([x for x in set(str(row[3]).lower().split()) if x not in stops]) - len([x for x in set(str(row[4]).lower().split()) if x not in stops]))\n",
    "\n",
    "def wc_diff_unique_alternative_stop_test(row):\n",
    "    stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    return abs(len([x for x in set(str(row[1]).lower().split()) if x not in stops]) - len([x for x in set(str(row[2]).lower().split()) if x not in stops]))\n",
    "\n",
    "train_df['wc_diff_unique_alternative_stop'] = train_df.apply(wc_diff_unique_alternative_stop, axis=1, raw=True)\n",
    "test_df['wc_diff_unique_alternative_stop'] = test_df.apply(wc_diff_unique_alternative_stop_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57321067",
   "metadata": {},
   "source": [
    "# Add wc ratio for stop words feature using alternative stop words (feature 38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c25502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wc_ratio_unique_alternative_stop(row):\n",
    "    stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    l1 = len([x for x in set(str(row[3]).lower().split()) if x not in stops]) \n",
    "    l2 = len([x for x in set(str(row[4]).lower().split()) if x not in stops])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2 > 1:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "def wc_ratio_unique_alternative_stop_test(row):\n",
    "    stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    l1 = len([x for x in set(str(row[1]).lower().split()) if x not in stops])\n",
    "    l2 = len([x for x in set(str(row[2]).lower().split()) if x not in stops])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    if l1 / l2 > 1:\n",
    "        return l2 / l1\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "train_df['wc_ratio_unique_alternative_stop'] = train_df.apply(wc_ratio_unique_alternative_stop, axis=1, raw=True)\n",
    "test_df['wc_ratio_unique_alternative_stop'] = test_df.apply(wc_ratio_unique_alternative_stop_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305269a7",
   "metadata": {},
   "source": [
    "# Add number of character difference for non-stop words feature using alternative stop words (feature 39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aa24a563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_diff_unique_alternative_stop(row):\n",
    "    stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    return abs(len(''.join([x for x in set(str(row[3]).lower().split()) if x not in stops])) - len(''.join([x for x in set(str(row[4]).lower().split()) if x not in stops])))\n",
    "\n",
    "def char_diff_unique_alternative_stop_test(row):\n",
    "    stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    return abs(len(''.join([x for x in set(str(row[1]).lower().split()) if x not in stops])) - len(''.join([x for x in set(str(row[2]).lower().split()) if x not in stops])))\n",
    "\n",
    "train_df['char_diff_unique_alternative_stop'] = train_df.apply(char_diff_unique_alternative_stop, axis=1, raw=True)\n",
    "test_df['char_diff_unique_alternative_stop'] = test_df.apply(char_diff_unique_alternative_stop_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09b1e2",
   "metadata": {},
   "source": [
    "# Add Q1 to Q2 word count difference for non-stop words feature using alternative stop words (feature 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4fdcbe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_to_q2_wc_diff_unique_alternative_stop(row):\n",
    "    stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    return len([x for x in set(str(row[3]).lower().split()) if x not in stops]) - len([x for x in set(str(row[4]).lower().split()) if x not in stops])\n",
    "\n",
    "def q1_to_q2_wc_diff_unique_alternative_stop_test(row):\n",
    "    stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    return len([x for x in set(str(row[1]).lower().split()) if x not in stops]) - len([x for x in set(str(row[2]).lower().split()) if x not in stops])\n",
    "\n",
    "train_df['q1_to_q2_wc_diff_unique_alternative_stop'] = train_df.apply(q1_to_q2_wc_diff_unique_alternative_stop, axis=1, raw=True)\n",
    "test_df['q1_to_q2_wc_diff_unique_alternative_stop'] = test_df.apply(q1_to_q2_wc_diff_unique_alternative_stop_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2cee29",
   "metadata": {},
   "source": [
    "# Add Q1 to Q2 wc ratio for stop words feature using alternative stop words (feature 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3492a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_to_q2_wc_ratio_unique_alternative_stop(row):\n",
    "    stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    l1 = len([x for x in set(str(row[3]).lower().split()) if x not in stops]) \n",
    "    l2 = len([x for x in set(str(row[4]).lower().split()) if x not in stops])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "def q1_to_q2_wc_ratio_unique_alternative_stop_test(row):\n",
    "    stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    l1 = len([x for x in set(str(row[1]).lower().split()) if x not in stops])\n",
    "    l2 = len([x for x in set(str(row[2]).lower().split()) if x not in stops])\n",
    "    if l2 == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return l1 / l2\n",
    "    \n",
    "train_df['q1_to_q2_wc_ratio_unique_alternative_stop'] = train_df.apply(q1_to_q2_wc_ratio_unique_alternative_stop, axis=1, raw=True)\n",
    "test_df['q1_to_q2_wc_ratio_unique_alternative_stop'] = test_df.apply(q1_to_q2_wc_ratio_unique_alternative_stop_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c8f9a7",
   "metadata": {},
   "source": [
    "# Add Q1 to Q2 number of character difference for non-stop words feature using alternative stop words (feature 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "323f8b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_to_q2_char_diff_unique_alternative_stop(row):\n",
    "    stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    return len(''.join([x for x in set(str(row[3]).lower().split()) if x not in stops])) - len(''.join([x for x in set(str(row[4]).lower().split()) if x not in stops]))\n",
    "\n",
    "def q1_to_q2_char_diff_unique_alternative_stop_test(row):\n",
    "    stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n",
    "              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n",
    "              'Is','If','While','This']\n",
    "    return len(''.join([x for x in set(str(row[1]).lower().split()) if x not in stops])) - len(''.join([x for x in set(str(row[2]).lower().split()) if x not in stops]))\n",
    "\n",
    "train_df['q1_to_q2_char_diff_unique_alternative_stop'] = train_df.apply(q1_to_q2_char_diff_unique_alternative_stop, axis=1, raw=True)\n",
    "test_df['q1_to_q2_char_diff_unique_alternative_stop'] = test_df.apply(q1_to_q2_char_diff_unique_alternative_stop_test, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d144af3f",
   "metadata": {},
   "source": [
    "# Export features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c98d742",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train_with_features' + data_clean_type + '.csv', index=False)\n",
    "test_df.to_csv('test_with_features' + data_clean_type + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
