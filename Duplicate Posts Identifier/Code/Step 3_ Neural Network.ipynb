{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3999279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GlobalAveragePooling1D, Lambda, Concatenate\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization.batch_normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras.backend as K\n",
    "\n",
    "# from BertEmbeddings import BertEmbeddings\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "from transformers import DebertaV2Tokenizer, TFDebertaV2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "216f4cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_with_features (cleaned).csv\n"
     ]
    }
   ],
   "source": [
    "print(Train_Data_File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ac07a9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants and parameters\n",
    "\n",
    "# Supported data_clean_type (DO NOT forget to put on \"()\", also, if data_clean_type is NOT empty string, please put a \" \" before (cleaned)):\n",
    "# empty string, no character\n",
    "# (cleaned)\n",
    "# (cleaned)(hyper_cleaned)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)(words_shortened)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)(alternative_stopwords_used)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)(alternative_stopwords_used)(words_shortened)\n",
    "data_clean_type = \" (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)(alternative_stopwords_used)(words_shortened)\"\n",
    "\n",
    "Train_Data_File = 'train_with_features' + data_clean_type + '.csv'\n",
    "Test_Data_File = 'test_with_features' + data_clean_type + '.csv'\n",
    "Max_Sequence_Length = 60\n",
    "Max_Num_Words = 200000\n",
    "Validation_Split_Ratio = 0.2\n",
    "\n",
    "Num_Lstm = np.random.randint(175, 275)\n",
    "Num_Dense = np.random.randint(100, 150)\n",
    "Rate_Drop_Lstm = 0.15 + np.random.rand() * 0.25\n",
    "Rate_Drop_Dense = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "act_f = 'relu'\n",
    "re_weight = False\n",
    "use_more_features = False\n",
    "fine_tuned_model = False\n",
    "\n",
    "# embedding_method:\n",
    "# 1. Bert (will use Bert large)\n",
    "# 2. DeBerta (will use DeBerta-v2-xlarge)\n",
    "# 3. GloVe\n",
    "# 4. RoBerta (will use RoBerta-large)\n",
    "# 5. Word2Vec\n",
    "embedding_model = \"GloVe\"\n",
    "\n",
    "# Support neural network:\n",
    "# LSTM\n",
    "# CNN\n",
    "neural_network = \"LSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "454610cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LTX_\\AppData\\Local\\Temp\\ipykernel_30008\\994337999.py:5: DtypeWarning: Columns (5,6,7,9,11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_train = pd.read_csv(Train_Data_File, encoding='utf-8')\n"
     ]
    }
   ],
   "source": [
    "# Process text in dataset\n",
    "print('Processing text dataset')\n",
    "\n",
    "# load data and process with text_to_wordlist\n",
    "df_train = pd.read_csv(Train_Data_File, encoding='utf-8')\n",
    "#df_train = df_train.dropna()\n",
    "#df_train = df_train.fillna('empty')\n",
    "\n",
    "df_train = df_train[['question1', 'question2', 'is_duplicate', 'q1_q2_intersect', 'q1_freq', 'q2_freq']]\n",
    "df_train = df_train.dropna()\n",
    "\n",
    "# train_texts_1 = df_train['question1'].tolist()\n",
    "# train_texts_2 = df_train['question2'].tolist()\n",
    "# train_labels = df_train['is_duplicate'].tolist()\n",
    "\n",
    "df_test = pd.read_csv(Test_Data_File, encoding='utf-8')\n",
    "df_test = df_test.dropna()\n",
    "#df_test = df_test.fillna('empty')\n",
    "\n",
    "test_texts_1 = df_test['question1'].tolist()\n",
    "test_texts_2 = df_test['question2'].tolist()\n",
    "test_ids = df_test['test_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "e019a712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "cad447e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore all non 0 or 1 val in train_val\n",
    "temp_train = []\n",
    "isDup = df_train['is_duplicate'].tolist()\n",
    "\n",
    "for s in range (len(df_train)):\n",
    "    if (isDup[s] == '0' or isDup[s] == '1'):\n",
    "        temp_train.append(s)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "dcc7dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train[['question1', 'question2', 'is_duplicate', 'q1_q2_intersect', 'q1_freq', 'q2_freq']].iloc[temp_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "9daadc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.dropna()\n",
    "\n",
    "train_texts_1 = df_train['question1'].tolist()\n",
    "train_texts_2 = df_train['question2'].tolist()\n",
    "train_labels = df_train['is_duplicate'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "d8be0ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = [int(numeric_string) for numeric_string in train_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f9da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "c302dc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4288\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(test_texts_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "842d664e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58424 unique tokens are found\n",
      "Shape of train data tensor: (311149, 60)\n",
      "Shape of train labels tensor: (311149,)\n",
      "Shape of test data tensor: (4288, 60)\n",
      "Shape of test ids tensor: (4288,)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize words in all sentences\n",
    "tokenizer = Tokenizer(num_words=Max_Num_Words)\n",
    "tokenizer.fit_on_texts(train_texts_1 + train_texts_2 + test_texts_1 + test_texts_2)\n",
    "\n",
    "train_sequences_1 = tokenizer.texts_to_sequences(train_texts_1)\n",
    "train_sequences_2 = tokenizer.texts_to_sequences(train_texts_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('{} unique tokens are found'.format(len(word_index)))\n",
    "\n",
    "# pad all train with Max_Sequence_Length\n",
    "train_data_1 = pad_sequences(train_sequences_1, maxlen=Max_Sequence_Length)\n",
    "train_data_2 = pad_sequences(train_sequences_2, maxlen=Max_Sequence_Length)\n",
    "train_labels = np.array(train_labels)\n",
    "print('Shape of train data tensor:', train_data_1.shape)\n",
    "print('Shape of train labels tensor:', train_labels.shape)\n",
    "\n",
    "# pad all test with Max_Sequence_Length\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=Max_Sequence_Length)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=Max_Sequence_Length)\n",
    "test_ids = np.array(test_ids)\n",
    "print('Shape of test data tensor:', test_data_2.shape)\n",
    "print('Shape of test ids tensor:', test_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "216a5548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_texts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "03e25214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating GloVe word embedding dictionary...\n",
      "Found 2195892 word vectors of glove.\n"
     ]
    }
   ],
   "source": [
    "# Word embedding\n",
    "if embedding_model == \"GloVe\":\n",
    "    Embedding_Dim = 300 # Dimension of GloVe-embedding\n",
    "    Embedding_File = 'glove.840B.300d.txt' # You may need to add the complete directory if the text file is NOT in the same folder as this ipynb file\n",
    "    # Create word embedding dictionary from 'glove.840B.300d.txt'\n",
    "    print('Creating GloVe word embedding dictionary...')\n",
    "\n",
    "    embeddings_index = {}\n",
    "    f = open(Embedding_File, encoding='utf-8')\n",
    "\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        # word = values[0]\n",
    "        word = ''.join(values[:-300])   \n",
    "        coefs = np.asarray(values[-300:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Found {} word vectors of glove.'.format(len(embeddings_index)))\n",
    "    \n",
    "if embedding_model == \"Word2Vec\":\n",
    "    Embedding_Dim = 300 # Dimension of Word2Vec-embedding\n",
    "    Embedding_File = 'GoogleNews-vectors-negative300.bin' # You may need to add the complete directory if the bin file is NOT in the same folder as this ipynb file\n",
    "    print('Creating Word2Vec word embedding dictionary...')\n",
    "\n",
    "    word2vec = KeyedVectors.load_word2vec_format(datapath(Embedding_File), binary=True)\n",
    "    print('Found %s word vectors of word2vec' % len(word2vec))\n",
    "\n",
    "if embedding_model == \"Bert\":\n",
    "    Embedding_Dim = 1024 # Dimension of Bert-embedding\n",
    "    if fine_tuned_model:\n",
    "        bert_embeddings = BertEmbeddings(model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "    else:\n",
    "        bert_embeddings = BertEmbeddings(model_name = 'bert-large-uncased-whole-word-masking')\n",
    "    # This will create a tensor too large for a single computer, a more powerful one is needed\n",
    "    '''\n",
    "    embeddings_index = {}\n",
    "    for word in word_index:\n",
    "        output = bert_embeddings([word])\n",
    "        for value in output[0]['embeddings_map'].values():\n",
    "            embeddings_index[word] = np.array(value)\n",
    "    print('Found {} word vectors of bert.'.format(len(embeddings_index)))\n",
    "    '''\n",
    "    # Use this instead\n",
    "    embeddings_index = {}\n",
    "    for word in word_index:\n",
    "        output = bert_embeddings([word])\n",
    "        result = np.array(output[0]['hidden_states'])[0][-1] + np.array(output[0]['hidden_states'])[0][-2] + np.array(output[0]['hidden_states'])[0][-3] + np.array(output[0]['hidden_states'])[0][-4]\n",
    "        embeddings_index[word] = result\n",
    "    print('Found {} word vectors of bert.'.format(len(embeddings_index)))\n",
    "    \n",
    "if embedding_model == \"RoBerta\":\n",
    "    Embedding_Dim = 1024 # Dimension of Bert-embedding\n",
    "    print('Creating RoBerta word embedding dictionary...')\n",
    "    roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "    roberta_model = TFRobertaModel.from_pretrained('roberta-large')\n",
    "    embeddings_index = {}\n",
    "    for word in word_index:\n",
    "        inputs = roberta_tokenizer(word, return_tensors=\"tf\")\n",
    "        outputs = roberta_model(inputs)\n",
    "        result = np.array(outputs.last_hidden_state[0][1:-1])\n",
    "        embeddings_index[word] = result\n",
    "    print('Found {} word vectors of RoBerta.'.format(len(embeddings_index)))\n",
    "    \n",
    "if embedding_model == \"DeBerta\":\n",
    "    Embedding_Dim = 1536 # Dimension of Bert-embedding\n",
    "    print('Creating DeBerta word embedding dictionary...')\n",
    "    deberta_tokenizer = DebertaV2Tokenizer.from_pretrained('kamalkraj/deberta-v2-xlarge')\n",
    "    deberta_model = TFDebertaV2Model.from_pretrained('kamalkraj/deberta-v2-xlarge')\n",
    "    embeddings_index = {}\n",
    "    for word in word_index:\n",
    "        inputs = deberta_tokenizer(word, return_tensors=\"tf\")\n",
    "        outputs = deberta_model(inputs)\n",
    "        result = np.array(outputs.last_hidden_state[0][1:-1])\n",
    "        embeddings_index[word] = result\n",
    "    print('Found {} word vectors of DeBerta.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "3b824497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LTX_\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\LTX_\\anaconda3\\lib\\site-packages\\sklearn\\base.py:443: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "leaks = df_train[['q1_q2_intersect', 'q1_freq', 'q2_freq']]\n",
    "test_leaks = df_test[['q1_q2_intersect', 'q1_freq', 'q2_freq']]\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((leaks, test_leaks)))\n",
    "leaks = ss.transform(leaks)\n",
    "test_leaks = ss.transform(test_leaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "a00daeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311149\n"
     ]
    }
   ],
   "source": [
    "print(len(leaks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "cd76f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized_feature_array = [1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "1e1c8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add extra features input (optimal feature returned by XGBoost)\n",
    "if use_more_features:\n",
    "    #extra_features = df_train[['word_match_share', 'tfidf_word_match_share', 'tfidf_word_match', 'unigrams_common_count', 'unigrams_common_ratio',\n",
    "    #                      'jaccard', 'common_words', 'common_words_stop', 'total_unique_words', 'total_unq_words_stop', 'wc_diff', 'wc_ratio', \n",
    "    #                      'wc_diff_unique', 'wc_ratio_unique', 'wc_diff_unique_stop', 'wc_ratio_unique_stop', 'same_start_word', 'char_diff', \n",
    "    #                      'char_ratio', 'char_diff_unique_stop']]\n",
    "    extra_features = df_train[['word_match_share', 'tfidf_word_match', 'jaccard', 'total_unique_words', 'total_unq_words_stop', 'wc_diff', 'wc_ratio', \n",
    "                           'wc_diff_unique', 'wc_ratio_unique', 'same_start_word', 'char_diff', 'char_diff_unique_stop', 'q1_to_q2_wc_ratio_unique',\n",
    "                              'q1_to_q2_char_diff', 'q1_to_q2_char_diff_unique_stop', 'word_match_share_alternative_stop', 'common_words_alternative_stop',\n",
    "                              'total_unq_words_alternative_stop', 'wc_diff_unique_alternative_stop', 'char_diff_unique_alternative_stop',\n",
    "                              'q1_to_q2_wc_diff_unique_alternative_stop', 'q1_to_q2_wc_ratio_unique_alternative_stop', 'q1_to_q2_char_diff_unique_alternative_stop']]\n",
    "    #extra_features_test = df_test[['word_match_share', 'tfidf_word_match_share', 'tfidf_word_match', 'unigrams_common_count', 'unigrams_common_ratio',\n",
    "    #                      'jaccard', 'common_words', 'common_words_stop', 'total_unique_words', 'total_unq_words_stop', 'wc_diff', 'wc_ratio', \n",
    "    #                      'wc_diff_unique', 'wc_ratio_unique', 'wc_diff_unique_stop', 'wc_ratio_unique_stop', 'same_start_word', 'char_diff', \n",
    "    #                      'char_ratio', 'char_diff_unique_stop']]\n",
    "    extra_features_test = df_test[['word_match_share', 'tfidf_word_match', 'jaccard', 'total_unique_words', 'total_unq_words_stop', 'wc_diff', 'wc_ratio', \n",
    "                           'wc_diff_unique', 'wc_ratio_unique', 'same_start_word', 'char_diff', 'char_diff_unique_stop', 'q1_to_q2_wc_ratio_unique',\n",
    "                              'q1_to_q2_char_diff', 'q1_to_q2_char_diff_unique_stop', 'word_match_share_alternative_stop', 'common_words_alternative_stop',\n",
    "                              'total_unq_words_alternative_stop', 'wc_diff_unique_alternative_stop', 'char_diff_unique_alternative_stop',\n",
    "                              'q1_to_q2_wc_diff_unique_alternative_stop', 'q1_to_q2_wc_ratio_unique_alternative_stop', 'q1_to_q2_char_diff_unique_alternative_stop']]\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    ss.fit(np.vstack((extra_features, extra_features_test)))\n",
    "    extra_features = ss.transform(extra_features)\n",
    "    extra_features_test = ss.transform(extra_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "1edaa9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: \n"
     ]
    }
   ],
   "source": [
    "# Create embedding matrix for embedding layer\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "num_words = min(Max_Num_Words, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, Embedding_Dim))\n",
    "if embedding_model == \"GloVe\" or embedding_model == \"Bert\":\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "if embedding_model == \"Word2Vec\":\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_vector = word2vec.get_vector(word)\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except:\n",
    "            continue\n",
    "if embedding_model == \"RoBerta\" or embedding_model == \"DeBerta\":\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)[0]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: '.format(np.sum(np.sum(embedding_matrix, axis=1) == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "c7759ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Validation split\n",
    "perm = np.random.permutation(len(train_data_1))\n",
    "idx_train = perm[:int(len(train_data_1)*(1-Validation_Split_Ratio))]\n",
    "idx_val = perm[int(len(train_data_1)*(1-Validation_Split_Ratio)):]\n",
    "\n",
    "data_1_train = np.vstack((train_data_1[idx_train], train_data_2[idx_train]))\n",
    "data_2_train = np.vstack((train_data_2[idx_train], train_data_1[idx_train]))\n",
    "leaks_train = np.vstack((leaks[idx_train], leaks[idx_train]))\n",
    "if use_more_features:\n",
    "    feature_train = np.vstack((extra_features[idx_train], extra_features[idx_train]))\n",
    "labels_train = np.concatenate((train_labels[idx_train], train_labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((train_data_1[idx_val], train_data_2[idx_val]))\n",
    "data_2_val = np.vstack((train_data_2[idx_val], train_data_1[idx_val]))\n",
    "leaks_val = np.vstack((leaks[idx_val], leaks[idx_val]))\n",
    "if use_more_features:\n",
    "    feature_val = np.vstack((extra_features[idx_val], extra_features[idx_val]))\n",
    "labels_val = np.concatenate((train_labels[idx_val], train_labels[idx_val]))\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.471544715\n",
    "    weight_val[labels_val==0] = 1.309033281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "afcd9067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "543d3d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f40071a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     b \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant([\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m2.0\u001b[39m, \u001b[38;5;241m3.0\u001b[39m, \u001b[38;5;241m4.0\u001b[39m, \u001b[38;5;241m5.0\u001b[39m, \u001b[38;5;241m6.0\u001b[39m], shape\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m], name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m     c \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmatmul(a, b)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSession\u001b[49m() \u001b[38;5;28;01mas\u001b[39;00m sess:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m (sess\u001b[38;5;241m.\u001b[39mrun(c))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print (sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "e4e70404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LTX_\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:264: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244/244 [==============================] - ETA: 0s - loss: 0.3999 - acc: 0.8270 WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "244/244 [==============================] - 2875s 12s/step - loss: 0.3999 - acc: 0.8270 - val_loss: 0.4530 - val_acc: 0.8163\n",
      "Epoch 2/200\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.3411 - acc: 0.8518 WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "244/244 [==============================] - 2769s 11s/step - loss: 0.3411 - acc: 0.8518 - val_loss: 0.3753 - val_acc: 0.8265\n",
      "Epoch 3/200\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.3237 - acc: 0.8578 WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "244/244 [==============================] - 2747s 11s/step - loss: 0.3237 - acc: 0.8578 - val_loss: 0.3201 - val_acc: 0.8578\n",
      "Epoch 4/200\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.3113 - acc: 0.8625 WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "244/244 [==============================] - 2775s 11s/step - loss: 0.3113 - acc: 0.8625 - val_loss: 0.3114 - val_acc: 0.8611\n",
      "Epoch 5/200\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.3007 - acc: 0.8669 WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "244/244 [==============================] - 2743s 11s/step - loss: 0.3007 - acc: 0.8669 - val_loss: 0.3166 - val_acc: 0.8563\n",
      "Epoch 6/200\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.2917 - acc: 0.8708 WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "244/244 [==============================] - 3070s 13s/step - loss: 0.2917 - acc: 0.8708 - val_loss: 0.3067 - val_acc: 0.8617\n",
      "Epoch 7/200\n",
      "244/244 [==============================] - ETA: 0s - loss: 0.2826 - acc: 0.8748 WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [295]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     78\u001b[0m         hist \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit([data_1_train, data_2_train, leaks_train, feature_train], labels_train, \\\n\u001b[0;32m     79\u001b[0m             validation_data\u001b[38;5;241m=\u001b[39m([data_1_val, data_2_val, leaks_val, feature_val], labels_val, weight_val), \\\n\u001b[0;32m     80\u001b[0m             epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \\\n\u001b[0;32m     81\u001b[0m             class_weight\u001b[38;5;241m=\u001b[39mclass_weight, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping, model_checkpoint])\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m         hist \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_1_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_2_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleaks_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_1_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_2_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleaks_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m neural_network \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNN\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     89\u001b[0m \n\u001b[0;32m     90\u001b[0m     \u001b[38;5;66;03m# The embedding layer containing the word vectors\u001b[39;00m\n\u001b[0;32m     91\u001b[0m     emb_layer \u001b[38;5;241m=\u001b[39m Embedding(\n\u001b[0;32m     92\u001b[0m         input_dim\u001b[38;5;241m=\u001b[39mnum_words,\n\u001b[0;32m     93\u001b[0m         output_dim\u001b[38;5;241m=\u001b[39mEmbedding_Dim,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     96\u001b[0m         trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1445\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_eval_data_handler\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1432\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39mget_data_handler(\n\u001b[0;32m   1433\u001b[0m       x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m   1434\u001b[0m       y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1443\u001b[0m       model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1444\u001b[0m       steps_per_execution\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution)\n\u001b[1;32m-> 1445\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1447\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1450\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1454\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1457\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m   1458\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1756\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, step_num\u001b[38;5;241m=\u001b[39mstep, _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1755\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m-> 1756\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1757\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1758\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1557\u001b[0m, in \u001b[0;36mModel.make_test_function.<locals>.test_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_function\u001b[39m(iterator):\n\u001b[0;32m   1556\u001b[0m   \u001b[38;5;124;03m\"\"\"Runs a test execution with a single step.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1557\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstep_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1546\u001b[0m, in \u001b[0;36mModel.make_test_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1542\u001b[0m   run_step \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[0;32m   1543\u001b[0m       run_step, jit_compile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reduce_retracing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1545\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[1;32m-> 1546\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1547\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[0;32m   1548\u001b[0m     outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1312\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m   1308\u001b[0m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[0;32m   1309\u001b[0m   \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[0;32m   1310\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[0;32m   1311\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1312\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2888\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2886\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   2887\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m-> 2888\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3689\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3687\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[0;32m   3688\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m-> 3689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:595\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    594\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[1;32m--> 595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1535\u001b[0m, in \u001b[0;36mModel.make_test_function.<locals>.step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(data):\n\u001b[1;32m-> 1535\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1536\u001b[0m   \u001b[38;5;66;03m# Ensure counter is updated only if `test_step` succeeds.\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1499\u001b[0m, in \u001b[0;36mModel.test_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1475\u001b[0m \u001b[38;5;124;03m\"\"\"The logic for one evaluation step.\u001b[39;00m\n\u001b[0;32m   1476\u001b[0m \n\u001b[0;32m   1477\u001b[0m \u001b[38;5;124;03mThis method can be overridden to support custom evaluation logic.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1495\u001b[0m \u001b[38;5;124;03m  values of the `Model`'s metrics are returned.\u001b[39;00m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m x, y, sample_weight \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39munpack_x_y_sample_weight(data)\n\u001b[1;32m-> 1499\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1500\u001b[0m \u001b[38;5;66;03m# Updates stateful loss metrics.\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(x, y, y_pred, sample_weight)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:490\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39mcopied_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    488\u001b[0m   layout_map_lib\u001b[38;5;241m.\u001b[39m_map_subclass_model_variable(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layout_map)\n\u001b[1;32m--> 490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py:1014\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1013\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1014\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1017\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py:458\u001b[0m, in \u001b[0;36mFunctional.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;129m@doc_controls\u001b[39m\u001b[38;5;241m.\u001b[39mdo_not_doc_inheritable\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    441\u001b[0m   \u001b[38;5;124;03m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[0;32m    442\u001b[0m \n\u001b[0;32m    443\u001b[0m \u001b[38;5;124;03m  In this case `call` just reapplies\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;124;03m      a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 458\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_internal_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\functional.py:596\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    593\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[1;32m--> 596\u001b[0m outputs \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mlayer(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    598\u001b[0m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_id, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node\u001b[38;5;241m.\u001b[39mflat_output_ids, tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\rnn\\base_rnn.py:515\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    511\u001b[0m inputs, initial_state, constants \u001b[38;5;241m=\u001b[39m rnn_utils\u001b[38;5;241m.\u001b[39mstandardize_args(\n\u001b[0;32m    512\u001b[0m     inputs, initial_state, constants, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_constants)\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m initial_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m constants \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(RNN, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    517\u001b[0m \u001b[38;5;66;03m# If any of `initial_state` or `constants` are specified and are Keras\u001b[39;00m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;66;03m# tensors, then add them to the inputs and temporarily modify the\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# input_spec to include them.\u001b[39;00m\n\u001b[0;32m    521\u001b[0m additional_inputs \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py:1014\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1013\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1014\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1017\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\rnn\\lstm.py:574\u001b[0m, in \u001b[0;36mLSTM.call\u001b[1;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[0;32m    571\u001b[0m   \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(inputs, states):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell(inputs, states, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 574\u001b[0m   last_output, outputs, states \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m      \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitial_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m      \u001b[49m\u001b[43mconstants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m      \u001b[49m\u001b[43mgo_backwards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgo_backwards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m      \u001b[49m\u001b[43munroll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munroll\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_lengths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow_lengths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtime_major\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_major\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m      \u001b[49m\u001b[43mzero_output_for_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_output_for_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    585\u001b[0m \u001b[43m      \u001b[49m\u001b[43mreturn_all_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_sequences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    586\u001b[0m   runtime \u001b[38;5;241m=\u001b[39m gru_lstm_utils\u001b[38;5;241m.\u001b[39mruntime(gru_lstm_utils\u001b[38;5;241m.\u001b[39mRUNTIME_UNKNOWN)\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    588\u001b[0m   \u001b[38;5;66;03m# Use the new defun approach for backend implementation swap.\u001b[39;00m\n\u001b[0;32m    589\u001b[0m   \u001b[38;5;66;03m# Note that different implementations need to have same function\u001b[39;00m\n\u001b[0;32m    590\u001b[0m   \u001b[38;5;66;03m# signature, eg, the tensor parameters need to have same shape and dtypes.\u001b[39;00m\n\u001b[0;32m    591\u001b[0m   \u001b[38;5;66;03m# Since the cuDNN has an extra set of bias, those bias will be passed to\u001b[39;00m\n\u001b[0;32m    592\u001b[0m   \u001b[38;5;66;03m# both normal and cuDNN implementations.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend.py:4776\u001b[0m, in \u001b[0;36mrnn\u001b[1;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask, return_all_outputs)\u001b[0m\n\u001b[0;32m   4773\u001b[0m     new_states \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(initial_states, flat_new_state)\n\u001b[0;32m   4774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (time \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, output_ta_t) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mtuple\u001b[39m(new_states)\n\u001b[1;32m-> 4776\u001b[0m   final_outputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mwhile_loop(\n\u001b[0;32m   4777\u001b[0m       body\u001b[38;5;241m=\u001b[39m_step,\n\u001b[0;32m   4778\u001b[0m       loop_vars\u001b[38;5;241m=\u001b[39m(time, output_ta) \u001b[38;5;241m+\u001b[39m states,\n\u001b[0;32m   4779\u001b[0m       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mwhile_loop_kwargs)\n\u001b[0;32m   4780\u001b[0m   new_states \u001b[38;5;241m=\u001b[39m final_outputs[\u001b[38;5;241m2\u001b[39m:]\n\u001b[0;32m   4782\u001b[0m output_ta \u001b[38;5;241m=\u001b[39m final_outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:2754\u001b[0m, in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m   2751\u001b[0m loop_var_structure \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(type_spec\u001b[38;5;241m.\u001b[39mtype_spec_from_value,\n\u001b[0;32m   2752\u001b[0m                                         \u001b[38;5;28mlist\u001b[39m(loop_vars))\n\u001b[0;32m   2753\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cond(\u001b[38;5;241m*\u001b[39mloop_vars):\n\u001b[1;32m-> 2754\u001b[0m   loop_vars \u001b[38;5;241m=\u001b[39m \u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloop_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2755\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m try_to_pack \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loop_vars, (\u001b[38;5;28mlist\u001b[39m, _basetuple)):\n\u001b[0;32m   2756\u001b[0m     packed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend.py:4759\u001b[0m, in \u001b[0;36mrnn.<locals>._step\u001b[1;34m(time, output_ta_t, *states)\u001b[0m\n\u001b[0;32m   4757\u001b[0m current_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(ta\u001b[38;5;241m.\u001b[39mread(time) \u001b[38;5;28;01mfor\u001b[39;00m ta \u001b[38;5;129;01min\u001b[39;00m input_ta)\n\u001b[0;32m   4758\u001b[0m current_input \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mpack_sequence_as(inputs, current_input)\n\u001b[1;32m-> 4759\u001b[0m output, new_states \u001b[38;5;241m=\u001b[39m \u001b[43mstep_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4760\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconstants\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4761\u001b[0m flat_state \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(states)\n\u001b[0;32m   4762\u001b[0m flat_new_state \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(new_states)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\rnn\\lstm.py:572\u001b[0m, in \u001b[0;36mLSTM.call.<locals>.step\u001b[1;34m(inputs, states)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(inputs, states):\n\u001b[1;32m--> 572\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell(inputs, states, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py:1014\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1010\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1013\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1014\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1017\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     90\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     94\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\layers\\rnn\\lstm.py:270\u001b[0m, in \u001b[0;36mLSTMCell.call\u001b[1;34m(self, inputs, states, training)\u001b[0m\n\u001b[0;32m    268\u001b[0m   x_i \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mbias_add(x_i, b_i)\n\u001b[0;32m    269\u001b[0m   x_f \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mbias_add(x_f, b_f)\n\u001b[1;32m--> 270\u001b[0m   x_c \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_c\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m   x_o \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mbias_add(x_o, b_o)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurrent_dropout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1.\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\backend.py:6365\u001b[0m, in \u001b[0;36mbias_add\u001b[1;34m(x, bias, data_format)\u001b[0m\n\u001b[0;32m   6363\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannels_first\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   6364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mbias_add(x, bias, data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNCHW\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 6365\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNHWC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ndim(x) \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   6367\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannels_first\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1082\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1084\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:3523\u001b[0m, in \u001b[0;36mbias_add\u001b[1;34m(value, bias, data_format, name)\u001b[0m\n\u001b[0;32m   3520\u001b[0m   value \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(value, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3521\u001b[0m   bias \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(bias, dtype\u001b[38;5;241m=\u001b[39mvalue\u001b[38;5;241m.\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 3523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_nn_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_add\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:673\u001b[0m, in \u001b[0;36mbias_add\u001b[1;34m(value, bias, data_format, name)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m    672\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 673\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBiasAdd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    676\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if neural_network == \"LSTM\":\n",
    "    # The embedding layer containing the word vectors\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=num_words,\n",
    "        output_dim=Embedding_Dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=Max_Sequence_Length,\n",
    "        trainable=False\n",
    "    )    \n",
    "\n",
    "\n",
    "    # LSTM layer\n",
    "\n",
    "    lstm_layer = LSTM(Num_Lstm, dropout=Rate_Drop_Lstm, recurrent_dropout=Rate_Drop_Lstm)\n",
    "\n",
    "    # Define inputs\n",
    "    seq1 = Input(shape=(Max_Sequence_Length,), dtype='int32')\n",
    "    seq2 = Input(shape=(Max_Sequence_Length,), dtype='int32')\n",
    "\n",
    "    # Run inputs through embedding\n",
    "    emb1 = emb_layer(seq1)\n",
    "    emb2 = emb_layer(seq2)\n",
    "\n",
    "    # Run through LSTM layers\n",
    "    lstm_a = lstm_layer(emb1)\n",
    "    # glob1a = GlobalAveragePooling1D()(lstm_a)\n",
    "    lstm_b = lstm_layer(emb2)\n",
    "    # glob1b = GlobalAveragePooling1D()(lstm_b)\n",
    "\n",
    "    magic_input = Input(shape=(leaks.shape[1],))\n",
    "    magic_dense = BatchNormalization()(magic_input)\n",
    "    magic_dense = Dense(int(Num_Dense/2), activation=act_f)(magic_input)\n",
    "\n",
    "    if use_more_features:\n",
    "        feature_input = Input(shape=(extra_features.shape[1],))\n",
    "        feature_dense = BatchNormalization()(feature_input)\n",
    "        feature_dense = Dense(128, activation='relu')(feature_dense)\n",
    "\n",
    "    if use_more_features:\n",
    "        merged = concatenate([lstm_a, lstm_b, magic_dense, feature_dense])\n",
    "    else:\n",
    "        merged = concatenate([lstm_a, lstm_b, magic_dense])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(Rate_Drop_Dense)(merged)\n",
    "\n",
    "    merged = Dense(Num_Dense, activation=act_f)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(Rate_Drop_Dense)(merged)\n",
    "\n",
    "    preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "    # Add class weight\n",
    "    if re_weight:\n",
    "        class_weight = {0: 1.309033281, 1: 0.471544715}\n",
    "    else:\n",
    "        class_weight = None\n",
    "        \n",
    "    # Train the model\n",
    "    if use_more_features:\n",
    "        model = Model(inputs=[seq1, seq2, magic_input, feature_input], \\\n",
    "            outputs=preds)\n",
    "    else:\n",
    "        model = Model(inputs=[seq1, seq2, magic_input], \\\n",
    "            outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "\n",
    "    # Set early stopping (large patience should be useful)\n",
    "    early_stopping =EarlyStopping(monitor='val_acc', patience=6)\n",
    "    output_model_name = embedding_model + \" + \" + neural_network\n",
    "    if use_more_features:\n",
    "        output_model_name = output_model_name + \"(extra features)\"\n",
    "    if embedding_model == \"Bert\" and fine_tuned_model:\n",
    "        output_model_name = output_model_name + \"(Bert fine_tunned)\"\n",
    "    output_model_name = output_model_name + '.h5'\n",
    "    model_checkpoint = ModelCheckpoint(output_model_name, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    if use_more_features:\n",
    "        hist = model.fit([data_1_train, data_2_train, leaks_train, feature_train], labels_train, \\\n",
    "            validation_data=([data_1_val, data_2_val, leaks_val, feature_val], labels_val, weight_val), \\\n",
    "            epochs=200, batch_size=2048, shuffle=True, \\\n",
    "            class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "    else:\n",
    "        hist = model.fit([data_1_train, data_2_train, leaks_train], labels_train, \\\n",
    "            validation_data=([data_1_val, data_2_val, leaks_val], labels_val, weight_val), \\\n",
    "            epochs=200, batch_size=2048, shuffle=True, \\\n",
    "            class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "    \n",
    "if neural_network == \"CNN\":\n",
    "\n",
    "    # The embedding layer containing the word vectors\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=num_words,\n",
    "        output_dim=Embedding_Dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=Max_Sequence_Length,\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    # 1D convolutions that can iterate over the word vectors\n",
    "    conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "    conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\n",
    "    conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "    conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\n",
    "    conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n",
    "    conv6 = Conv1D(filters=32, kernel_size=6, padding='same', activation='relu')\n",
    "\n",
    "    # Define inputs\n",
    "    seq1 = Input(shape=(60,))\n",
    "    seq2 = Input(shape=(60,))\n",
    "\n",
    "    # Run inputs through embedding\n",
    "    emb1 = emb_layer(seq1)\n",
    "    emb2 = emb_layer(seq2)\n",
    "\n",
    "    # Run through CONV + GAP layers\n",
    "    conv1a = conv1(emb1)\n",
    "    glob1a = GlobalAveragePooling1D()(conv1a)\n",
    "    conv1b = conv1(emb2)\n",
    "    glob1b = GlobalAveragePooling1D()(conv1b)\n",
    "\n",
    "    conv2a = conv2(emb1)\n",
    "    glob2a = GlobalAveragePooling1D()(conv2a)\n",
    "    conv2b = conv2(emb2)\n",
    "    glob2b = GlobalAveragePooling1D()(conv2b)\n",
    "\n",
    "    conv3a = conv3(emb1)\n",
    "    glob3a = GlobalAveragePooling1D()(conv3a)\n",
    "    conv3b = conv3(emb2)\n",
    "    glob3b = GlobalAveragePooling1D()(conv3b)\n",
    "\n",
    "    conv4a = conv4(emb1)\n",
    "    glob4a = GlobalAveragePooling1D()(conv4a)\n",
    "    conv4b = conv4(emb2)\n",
    "    glob4b = GlobalAveragePooling1D()(conv4b)\n",
    "\n",
    "    conv5a = conv5(emb1)\n",
    "    glob5a = GlobalAveragePooling1D()(conv5a)\n",
    "    conv5b = conv5(emb2)\n",
    "    glob5b = GlobalAveragePooling1D()(conv5b)\n",
    "\n",
    "    conv6a = conv6(emb1)\n",
    "    glob6a = GlobalAveragePooling1D()(conv6a)\n",
    "    conv6b = conv6(emb2)\n",
    "    glob6b = GlobalAveragePooling1D()(conv6b)\n",
    "\n",
    "    mergea = concatenate([glob1a, glob2a, glob3a, glob4a, glob5a, glob6a])\n",
    "    mergeb = concatenate([glob1b, glob2b, glob3b, glob4b, glob5b, glob6b])\n",
    "\n",
    "    diff = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "    mul = Lambda(lambda x: x[0] * x[1], output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "\n",
    "    magic_input = Input(shape=(leaks.shape[1],))\n",
    "    magic_dense = BatchNormalization()(magic_input)\n",
    "    magic_dense = Dense(64, activation='relu')(magic_dense)\n",
    "\n",
    "    if use_more_features:\n",
    "        feature_input = Input(shape=(extra_features.shape[1],))\n",
    "        feature_dense = BatchNormalization()(feature_input)\n",
    "        feature_dense = Dense(128, activation='relu')(feature_dense)\n",
    "\n",
    "    if use_more_features:\n",
    "        merge = concatenate([diff, mul, magic_dense, feature_dense])\n",
    "    else:\n",
    "        merge = concatenate([diff, mul, magic_dense])\n",
    "\n",
    "    if re_weight:\n",
    "        class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "    else:\n",
    "        class_weight = None\n",
    "\n",
    "    # The MLP that determines the outcome\n",
    "    x = Dropout(0.2)(merge)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(300, activation='relu')(x)\n",
    "\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    pred = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    if use_more_features:\n",
    "        model = Model(inputs=[seq1, seq2, magic_input, feature_input], outputs=pred)\n",
    "    else:\n",
    "        model = Model(inputs=[seq1, seq2, magic_input], outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "    # Set early stopping (large patience should be useful)\n",
    "    early_stopping =EarlyStopping(monitor='val_acc', patience=6)\n",
    "    output_model_name = embedding_model + \" + \" + neural_network\n",
    "    if use_more_features:\n",
    "        output_model_name = output_model_name + \"(extra features)\"\n",
    "    if embedding_model == \"Bert\" and fine_tuned_model:\n",
    "        output_model_name = output_model_name + \"(Bert fine_tunned)\"\n",
    "    output_model_name = output_model_name + '.h5' \n",
    "    model_checkpoint = ModelCheckpoint(output_model_name, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    if use_more_features:\n",
    "        hist = model.fit([data_1_train, data_2_train, leaks_train, feature_train], labels_train, \\\n",
    "            validation_data=([data_1_val, data_2_val, leaks_val, feature_val], labels_val, weight_val), \\\n",
    "            epochs=200, batch_size=2048, shuffle=True, \\\n",
    "            class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "    else:\n",
    "        hist = model.fit([data_1_train, data_2_train, leaks_train], labels_train, \\\n",
    "            validation_data=([data_1_val, data_2_val, leaks_val], labels_val, weight_val), \\\n",
    "            epochs=200, batch_size=2048, shuffle=True, \\\n",
    "            class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b692b195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(data_1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c60bef00",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ellipsis' object has no attribute 'distribute_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [75]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_eagerly\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:625\u001b[0m, in \u001b[0;36mModel.compile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, jit_compile, **kwargs)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;124;03m\"\"\"Configures the model for training.\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \n\u001b[0;32m    536\u001b[0m \u001b[38;5;124;03mExample:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;124;03m    **kwargs: Arguments supported for backwards compatibility only.\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    624\u001b[0m base_layer\u001b[38;5;241m.\u001b[39mkeras_api_gauge\u001b[38;5;241m.\u001b[39mget_cell(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompile\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m    626\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperimental_steps_per_execution\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[0;32m    627\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe argument `steps_per_execution` is no longer \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    628\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperimental. Pass `steps_per_execution` instead of \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    629\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`experimental_steps_per_execution`.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ellipsis' object has no attribute 'distribute_strategy'"
     ]
    }
   ],
   "source": [
    "Model.compile(..., run_eagerly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d294243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the model training has finished. Save the model for future use\n",
    "model.save(output_model_name) # store model parameters in .h5 file\n",
    "bst_val_score = min(hist.history['val_acc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
