{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GlobalAveragePooling1D, Lambda\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization.batch_normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_178_124_0.31_0.26\n"
     ]
    }
   ],
   "source": [
    "# Define constants and parameters\n",
    "\n",
    "# Supported data_clean_type (DO NOT forget to put on \"()\", also, if data_clean_type is NOT empty string, please put a \" \" before (cleaned)):\n",
    "# empty string, no character\n",
    "# (cleaned)\n",
    "# (cleaned)(hyper_cleaned)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)(words_shortened)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)(alternative_stopwords_used)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)(alternative_stopwords_used)(words_shortened)\n",
    "data_clean_type = \" (cleaned)\"\n",
    "\n",
    "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin'\n",
    "Train_Data_File = 'train_with_features' + data_clean_type + '.csv'\n",
    "Test_Data_File = 'test_with_features' + data_clean_type + '.csv'\n",
    "Max_Sequence_Length = 60\n",
    "Max_Num_Words = 200000 # There are about 201000 unique words in training dataset, 200000 is enough for tokenization\n",
    "Embedding_Dim = 300\n",
    "Validation_Split_Ratio = 0.2\n",
    "\n",
    "Num_Lstm = np.random.randint(175, 275)\n",
    "Num_Dense = np.random.randint(100, 150)\n",
    "Rate_Drop_Lstm = 0.15 + np.random.rand() * 0.25\n",
    "Rate_Drop_Dense = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "Lstm_Struc = 'lstm_{:d}_{:d}_{:.2f}_{:.2f}'.format(Num_Lstm, Num_Dense, Rate_Drop_Lstm, \\\n",
    "Rate_Drop_Dense)\n",
    "print(Lstm_Struc)\n",
    "\n",
    "act_f = 'relu'\n",
    "re_weight = False # whether to re-weight classes to fit the 17.4% share in test set\n",
    "use_more_features = False # If true, add other 19 features. If false, only use leaky features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 3000000 word vectors of word2vec\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors')\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "        binary=True)\n",
    "print('Found %s word vectors of word2vec' % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "# Process text in dataset\n",
    "print('Processing text dataset')\n",
    "\n",
    "# load data and process with text_to_wordlist\n",
    "df_train = pd.read_csv(Train_Data_File, encoding='utf-8')\n",
    "df_train = df_train.dropna()\n",
    "#df_train = df_train.fillna('empty')\n",
    "\n",
    "train_texts_1 = df_train['question1'].tolist()\n",
    "train_texts_2 = df_train['question2'].tolist()\n",
    "train_labels = df_train['is_duplicate'].tolist()\n",
    "\n",
    "df_test = pd.read_csv(Test_Data_File, encoding='utf-8')\n",
    "df_test = df_test.dropna()\n",
    "#df_test = df_test.fillna('empty')\n",
    "\n",
    "test_texts_1 = df_test['question1'].tolist()\n",
    "test_texts_2 = df_test['question2'].tolist()\n",
    "test_ids = df_test['test_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85496 unique tokens are found\n",
      "Shape of train data tensor: (399922, 60)\n",
      "Shape of train labels tensor: (399922,)\n",
      "Shape of test data tensor: (4290, 60)\n",
      "Shape of test ids tensor: (4290,)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize words in all sentences\n",
    "tokenizer = Tokenizer(num_words=Max_Num_Words)\n",
    "tokenizer.fit_on_texts(train_texts_1 + train_texts_2 + test_texts_1 + test_texts_2)\n",
    "\n",
    "train_sequences_1 = tokenizer.texts_to_sequences(train_texts_1)\n",
    "train_sequences_2 = tokenizer.texts_to_sequences(train_texts_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('{} unique tokens are found'.format(len(word_index)))\n",
    "\n",
    "# pad all train with Max_Sequence_Length\n",
    "train_data_1 = pad_sequences(train_sequences_1, maxlen=Max_Sequence_Length)\n",
    "train_data_2 = pad_sequences(train_sequences_2, maxlen=Max_Sequence_Length)\n",
    "train_labels = np.array(train_labels)\n",
    "print('Shape of train data tensor:', train_data_1.shape)\n",
    "print('Shape of train labels tensor:', train_labels.shape)\n",
    "\n",
    "# pad all test with Max_Sequence_Length\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=Max_Sequence_Length)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=Max_Sequence_Length)\n",
    "test_ids = np.array(test_ids)\n",
    "print('Shape of test data tensor:', test_data_2.shape)\n",
    "print('Shape of test ids tensor:', test_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# leaky features\n",
    "leaks = df_train[['q1_q2_intersect', 'q1_freq', 'q2_freq']]\n",
    "test_leaks = df_test[['q1_q2_intersect', 'q1_freq', 'q2_freq']]\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((leaks, test_leaks)))\n",
    "leaks = ss.transform(leaks)\n",
    "test_leaks = ss.transform(test_leaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add extra features input (optimal feature returned by XGBoost)\n",
    "if use_more_features:\n",
    "    #extra_features = df_train[['word_match_share', 'tfidf_word_match_share', 'tfidf_word_match', 'unigrams_common_count', 'unigrams_common_ratio',\n",
    "    #                      'jaccard', 'common_words', 'common_words_stop', 'total_unique_words', 'total_unq_words_stop', 'wc_diff', 'wc_ratio', \n",
    "    #                      'wc_diff_unique', 'wc_ratio_unique', 'wc_diff_unique_stop', 'wc_ratio_unique_stop', 'same_start_word', 'char_diff', \n",
    "    #                      'char_ratio', 'char_diff_unique_stop']]\n",
    "    extra_features = df_train[['word_match_share', 'tfidf_word_match', 'jaccard', 'total_unique_words', 'total_unq_words_stop', 'wc_diff', 'wc_ratio', \n",
    "                           'wc_diff_unique', 'wc_ratio_unique', 'same_start_word', 'char_diff', 'char_diff_unique_stop', 'q1_to_q2_wc_ratio_unique',\n",
    "                              'q1_to_q2_char_diff', 'q1_to_q2_char_diff_unique_stop', 'word_match_share_alternative_stop', 'common_words_alternative_stop',\n",
    "                              'total_unq_words_alternative_stop', 'wc_diff_unique_alternative_stop', 'char_diff_unique_alternative_stop',\n",
    "                              'q1_to_q2_wc_diff_unique_alternative_stop', 'q1_to_q2_wc_ratio_unique_alternative_stop', 'q1_to_q2_char_diff_unique_alternative_stop']]\n",
    "    #extra_features_test = df_test[['word_match_share', 'tfidf_word_match_share', 'tfidf_word_match', 'unigrams_common_count', 'unigrams_common_ratio',\n",
    "    #                      'jaccard', 'common_words', 'common_words_stop', 'total_unique_words', 'total_unq_words_stop', 'wc_diff', 'wc_ratio', \n",
    "    #                      'wc_diff_unique', 'wc_ratio_unique', 'wc_diff_unique_stop', 'wc_ratio_unique_stop', 'same_start_word', 'char_diff', \n",
    "    #                      'char_ratio', 'char_diff_unique_stop']]\n",
    "    extra_features_test = df_test[['word_match_share', 'tfidf_word_match', 'jaccard', 'total_unique_words', 'total_unq_words_stop', 'wc_diff', 'wc_ratio', \n",
    "                           'wc_diff_unique', 'wc_ratio_unique', 'same_start_word', 'char_diff', 'char_diff_unique_stop', 'q1_to_q2_wc_ratio_unique',\n",
    "                              'q1_to_q2_char_diff', 'q1_to_q2_char_diff_unique_stop', 'word_match_share_alternative_stop', 'common_words_alternative_stop',\n",
    "                              'total_unq_words_alternative_stop', 'wc_diff_unique_alternative_stop', 'char_diff_unique_alternative_stop',\n",
    "                              'q1_to_q2_wc_diff_unique_alternative_stop', 'q1_to_q2_wc_ratio_unique_alternative_stop', 'q1_to_q2_char_diff_unique_alternative_stop']]\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    ss.fit(np.vstack((extra_features, extra_features_test)))\n",
    "    extra_features = ss.transform(extra_features)\n",
    "    extra_features_test = ss.transform(extra_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 37372\n"
     ]
    }
   ],
   "source": [
    "# Create embedding matrix for embedding layer\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "num_words = min(Max_Num_Words, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, Embedding_Dim))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec:\n",
    "        embedding_matrix[i] = word2vec.get_vector(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Validation split\n",
    "perm = np.random.permutation(len(train_data_1))\n",
    "idx_train = perm[:int(len(train_data_1)*(1-Validation_Split_Ratio))]\n",
    "idx_val = perm[int(len(train_data_1)*(1-Validation_Split_Ratio)):]\n",
    "\n",
    "data_1_train = np.vstack((train_data_1[idx_train], train_data_2[idx_train]))\n",
    "data_2_train = np.vstack((train_data_2[idx_train], train_data_1[idx_train]))\n",
    "leaks_train = np.vstack((leaks[idx_train], leaks[idx_train]))\n",
    "if use_more_features:\n",
    "    feature_train = np.vstack((extra_features[idx_train], extra_features[idx_train]))\n",
    "labels_train = np.concatenate((train_labels[idx_train], train_labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((train_data_1[idx_val], train_data_2[idx_val]))\n",
    "data_2_val = np.vstack((train_data_2[idx_val], train_data_1[idx_val]))\n",
    "leaks_val = np.vstack((leaks[idx_val], leaks[idx_val]))\n",
    "if use_more_features:\n",
    "    feature_val = np.vstack((extra_features[idx_val], extra_features[idx_val]))\n",
    "labels_val = np.concatenate((train_labels[idx_val], train_labels[idx_val]))\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.471544715\n",
    "    weight_val[labels_val==0] = 1.309033281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "313/313 [==============================] - 63s 184ms/step - loss: 0.2935 - acc: 0.8718 - val_loss: 0.4929 - val_acc: 0.8298\n",
      "Epoch 2/200\n",
      "313/313 [==============================] - 35s 111ms/step - loss: 0.2219 - acc: 0.9050 - val_loss: 0.2695 - val_acc: 0.8760\n",
      "Epoch 3/200\n",
      "313/313 [==============================] - 34s 109ms/step - loss: 0.1779 - acc: 0.9254 - val_loss: 0.2611 - val_acc: 0.8891\n",
      "Epoch 4/200\n",
      "313/313 [==============================] - 36s 115ms/step - loss: 0.1435 - acc: 0.9407 - val_loss: 0.2810 - val_acc: 0.8890\n",
      "Epoch 5/200\n",
      "313/313 [==============================] - 34s 107ms/step - loss: 0.1193 - acc: 0.9516 - val_loss: 0.3121 - val_acc: 0.8917 7s - loss: 0.1188 - acc: 0.95 - ETA: 7s - lo - ETA: 1s - loss: 0.1194 - acc: 0.95 - ETA: 0s - loss: 0.1194 - a\n",
      "Epoch 6/200\n",
      "313/313 [==============================] - 34s 109ms/step - loss: 0.1024 - acc: 0.9587 - val_loss: 0.3279 - val_acc: 0.8901\n",
      "Epoch 7/200\n",
      "313/313 [==============================] - 34s 109ms/step - loss: 0.0902 - acc: 0.9639 - val_loss: 0.3323 - val_acc: 0.8900\n",
      "Epoch 8/200\n",
      "313/313 [==============================] - 35s 111ms/step - loss: 0.0806 - acc: 0.9682 - val_loss: 0.3438 - val_acc: 0.8903\n",
      "Epoch 9/200\n",
      "313/313 [==============================] - 36s 114ms/step - loss: 0.0736 - acc: 0.9713 - val_loss: 0.3800 - val_acc: 0.8912\n",
      "Epoch 10/200\n",
      "313/313 [==============================] - 34s 109ms/step - loss: 0.0677 - acc: 0.9737 - val_loss: 0.3685 - val_acc: 0.8917\n",
      "Epoch 11/200\n",
      "313/313 [==============================] - 39s 123ms/step - loss: 0.0629 - acc: 0.9755 - val_loss: 0.4016 - val_acc: 0.8917\n"
     ]
    }
   ],
   "source": [
    "# The embedding layer containing the word vectors\n",
    "emb_layer = Embedding(\n",
    "    input_dim=num_words,\n",
    "    output_dim=Embedding_Dim,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=Max_Sequence_Length,\n",
    "    trainable=False\n",
    ")\n",
    "\n",
    "# 1D convolutions that can iterate over the word vectors\n",
    "conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\n",
    "conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\n",
    "conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n",
    "conv6 = Conv1D(filters=32, kernel_size=6, padding='same', activation='relu')\n",
    "\n",
    "# Define inputs\n",
    "seq1 = Input(shape=(60,))\n",
    "seq2 = Input(shape=(60,))\n",
    "\n",
    "# Run inputs through embedding\n",
    "emb1 = emb_layer(seq1)\n",
    "emb2 = emb_layer(seq2)\n",
    "\n",
    "# Run through CONV + GAP layers\n",
    "conv1a = conv1(emb1)\n",
    "glob1a = GlobalAveragePooling1D()(conv1a)\n",
    "conv1b = conv1(emb2)\n",
    "glob1b = GlobalAveragePooling1D()(conv1b)\n",
    "\n",
    "conv2a = conv2(emb1)\n",
    "glob2a = GlobalAveragePooling1D()(conv2a)\n",
    "conv2b = conv2(emb2)\n",
    "glob2b = GlobalAveragePooling1D()(conv2b)\n",
    "\n",
    "conv3a = conv3(emb1)\n",
    "glob3a = GlobalAveragePooling1D()(conv3a)\n",
    "conv3b = conv3(emb2)\n",
    "glob3b = GlobalAveragePooling1D()(conv3b)\n",
    "\n",
    "conv4a = conv4(emb1)\n",
    "glob4a = GlobalAveragePooling1D()(conv4a)\n",
    "conv4b = conv4(emb2)\n",
    "glob4b = GlobalAveragePooling1D()(conv4b)\n",
    "\n",
    "conv5a = conv5(emb1)\n",
    "glob5a = GlobalAveragePooling1D()(conv5a)\n",
    "conv5b = conv5(emb2)\n",
    "glob5b = GlobalAveragePooling1D()(conv5b)\n",
    "\n",
    "conv6a = conv6(emb1)\n",
    "glob6a = GlobalAveragePooling1D()(conv6a)\n",
    "conv6b = conv6(emb2)\n",
    "glob6b = GlobalAveragePooling1D()(conv6b)\n",
    "\n",
    "mergea = concatenate([glob1a, glob2a, glob3a, glob4a, glob5a, glob6a])\n",
    "mergeb = concatenate([glob1b, glob2b, glob3b, glob4b, glob5b, glob6b])\n",
    "\n",
    "# We take the explicit absolute difference between the two sentences\n",
    "# Furthermore we take the multiply different entries to get a different measure of equalness\n",
    "diff = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "mul = Lambda(lambda x: x[0] * x[1], output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "\n",
    "# Add the magic features\n",
    "magic_input = Input(shape=(leaks.shape[1],))\n",
    "magic_dense = BatchNormalization()(magic_input)\n",
    "magic_dense = Dense(64, activation='relu')(magic_dense)\n",
    "\n",
    "# Add the distance features (these are now TFIDF (character and word), Fuzzy matching, \n",
    "# nb char 1 and 2, word mover distance and skew/kurtosis of the sentence vector)\n",
    "if use_more_features:\n",
    "    feature_input = Input(shape=(extra_features.shape[1],))\n",
    "    feature_dense = BatchNormalization()(feature_input)\n",
    "    feature_dense = Dense(128, activation='relu')(feature_dense)\n",
    "\n",
    "# Merge the Magic and distance features with the difference layer\n",
    "if use_more_features:\n",
    "    merge = concatenate([diff, mul, magic_dense, feature_dense])\n",
    "else:\n",
    "    merge = concatenate([diff, mul, magic_dense])\n",
    "\n",
    "if re_weight:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "else:\n",
    "    class_weight = None\n",
    "\n",
    "# The MLP that determines the outcome\n",
    "x = Dropout(0.2)(merge)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(300, activation='relu')(x)\n",
    "\n",
    "x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "pred = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "if use_more_features:\n",
    "    model = Model(inputs=[seq1, seq2, magic_input, feature_input], outputs=pred)\n",
    "else:\n",
    "    model = Model(inputs=[seq1, seq2, magic_input], outputs=pred)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# Set early stopping (large patience should be useful)\n",
    "early_stopping =EarlyStopping(monitor='val_acc', patience=6)\n",
    "bst_model_path = Lstm_Struc + '.h5' \n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "if use_more_features:\n",
    "    hist = model.fit([data_1_train, data_2_train, leaks_train, feature_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val, leaks_val, feature_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "else:\n",
    "    hist = model.fit([data_1_train, data_2_train, leaks_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val, leaks_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path) # store model parameters in .h5 file\n",
    "bst_val_score = min(hist.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making the submission\n",
      "1/1 [==============================] - 1s 970ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make the submission\n",
    "print('Making the submission')\n",
    "if use_more_features:\n",
    "    preds = model.predict([test_data_1, test_data_2, test_leaks, extra_features_test], batch_size=8192, verbose=1)\n",
    "    preds += model.predict([test_data_2, test_data_1, test_leaks, extra_features_test], batch_size=8192, verbose=1)\n",
    "    preds /= 2\n",
    "else:\n",
    "    preds = model.predict([test_data_1, test_data_2, test_leaks], batch_size=8192, verbose=1)\n",
    "    preds += model.predict([test_data_2, test_data_1, test_leaks], batch_size=8192, verbose=1)\n",
    "    preds /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 88.974%\n"
     ]
    }
   ],
   "source": [
    "# Convert percentage to binary predictions\n",
    "result = []\n",
    "sub_result = []\n",
    "for i in preds:\n",
    "    if i[0] < 0.5:\n",
    "        sub_result.append(0)\n",
    "    else:\n",
    "        sub_result.append(1)\n",
    "result.append(sub_result)\n",
    "result = np.array(result)\n",
    "\n",
    "# Get the accuracy on the test data\n",
    "true_values = df_test[\"is_duplicate (Ture Value)\"]\n",
    "\n",
    "score = 0\n",
    "for i in range(0, len(sub_result)):\n",
    "    if sub_result[i] == true_values.tolist()[i]:\n",
    "        score = score + 1\n",
    "accuracy = score / len(sub_result)\n",
    "print(\"Accuracy on test data: {}%\".format(round(accuracy*100, 3)))\n",
    "\n",
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':result.ravel()})\n",
    "submission.to_csv(\"Word2Vec + CNN_with_features\" + data_clean_type + \"(accuracy: + \" + str(round(accuracy*100, 3)) + \")\" + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
