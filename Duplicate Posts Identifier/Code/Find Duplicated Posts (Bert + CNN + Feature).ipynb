{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GlobalAveragePooling1D, Lambda\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization.batch_normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras.backend as K\n",
    "\n",
    "from BertEmbeddings import BertEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_201_137_0.35_0.35\n"
     ]
    }
   ],
   "source": [
    "# Define constants and parameters\n",
    "\n",
    "# Supported data_clean_type (DO NOT forget to put on \"()\", also, if data_clean_type is NOT empty string, please put a \" \" before (cleaned)):\n",
    "# empty string, no character\n",
    "# (cleaned)\n",
    "# (cleaned)(hyper_cleaned)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)(words_shortened)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)(alternative_stopwords_used)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)(alternative_stopwords_used)(words_shortened)\n",
    "data_clean_type = \" (cleaned)\"\n",
    "\n",
    "Train_Data_File = 'train_with_features' + data_clean_type + '.csv'\n",
    "Test_Data_File = 'test_with_features' + data_clean_type + '.csv'\n",
    "Max_Sequence_Length = 60\n",
    "Max_Num_Words = 200000 # There are about 201000 unique words in training dataset, 200000 is enough for tokenization\n",
    "Embedding_Dim = 1024 # Dimension of Bert-embedding\n",
    "Validation_Split_Ratio = 0.2\n",
    "\n",
    "Num_Lstm = np.random.randint(175, 275)\n",
    "Num_Dense = np.random.randint(100, 150)\n",
    "Rate_Drop_Lstm = 0.15 + np.random.rand() * 0.25\n",
    "Rate_Drop_Dense = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "Lstm_Struc = 'lstm_{:d}_{:d}_{:.2f}_{:.2f}'.format(Num_Lstm, Num_Dense, Rate_Drop_Lstm, \\\n",
    "Rate_Drop_Dense)\n",
    "print(Lstm_Struc)\n",
    "\n",
    "act_f = 'relu'\n",
    "re_weight = False # whether to re-weight classes to fit the 17.4% share in test set\n",
    "use_more_features = False # If true, add other 19 features. If false, only use leaky features\n",
    "fine_tuned_model = True # If true, use fine-tunned bert-embedding model. If false, use original bert-large-uncased model uploaded by Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "# Process text in dataset\n",
    "print('Processing text dataset')\n",
    "\n",
    "# load data and process with text_to_wordlist\n",
    "df_train = pd.read_csv(Train_Data_File, encoding='utf-8')\n",
    "df_train = df_train.dropna()\n",
    "#df_train = df_train.fillna('empty')\n",
    "\n",
    "train_texts_1 = df_train['question1'].tolist()\n",
    "train_texts_2 = df_train['question2'].tolist()\n",
    "train_labels = df_train['is_duplicate'].tolist()\n",
    "\n",
    "df_test = pd.read_csv(Test_Data_File, encoding='utf-8')\n",
    "df_test = df_test.dropna()\n",
    "#df_test = df_test.fillna('empty')\n",
    "\n",
    "test_texts_1 = df_test['question1'].tolist()\n",
    "test_texts_2 = df_test['question2'].tolist()\n",
    "test_ids = df_test['test_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85496 unique tokens are found\n",
      "Shape of train data tensor: (399922, 60)\n",
      "Shape of train labels tensor: (399922,)\n",
      "Shape of test data tensor: (4290, 60)\n",
      "Shape of test ids tensor: (4290,)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize words in all sentences\n",
    "tokenizer = Tokenizer(num_words=Max_Num_Words)\n",
    "tokenizer.fit_on_texts(train_texts_1 + train_texts_2 + test_texts_1 + test_texts_2)\n",
    "\n",
    "train_sequences_1 = tokenizer.texts_to_sequences(train_texts_1)\n",
    "train_sequences_2 = tokenizer.texts_to_sequences(train_texts_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('{} unique tokens are found'.format(len(word_index)))\n",
    "\n",
    "# pad all train with Max_Sequence_Length\n",
    "train_data_1 = pad_sequences(train_sequences_1, maxlen=Max_Sequence_Length)\n",
    "train_data_2 = pad_sequences(train_sequences_2, maxlen=Max_Sequence_Length)\n",
    "train_labels = np.array(train_labels)\n",
    "print('Shape of train data tensor:', train_data_1.shape)\n",
    "print('Shape of train labels tensor:', train_labels.shape)\n",
    "\n",
    "# pad all test with Max_Sequence_Length\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=Max_Sequence_Length)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=Max_Sequence_Length)\n",
    "test_ids = np.array(test_ids)\n",
    "print('Shape of test data tensor:', test_data_2.shape)\n",
    "print('Shape of test ids tensor:', test_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eeaca7ab0a94addb25e500f3a3dd8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a36ce9a7817474c98f8965c2a7ed33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818a8af4e91f4e1f86492e3ce1c7dd40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 words embeded\n",
      "2000 words embeded\n",
      "3000 words embeded\n",
      "4000 words embeded\n",
      "5000 words embeded\n",
      "6000 words embeded\n",
      "7000 words embeded\n",
      "8000 words embeded\n",
      "9000 words embeded\n",
      "10000 words embeded\n",
      "11000 words embeded\n",
      "12000 words embeded\n",
      "13000 words embeded\n",
      "14000 words embeded\n",
      "15000 words embeded\n",
      "16000 words embeded\n",
      "17000 words embeded\n",
      "18000 words embeded\n",
      "19000 words embeded\n",
      "20000 words embeded\n",
      "21000 words embeded\n",
      "22000 words embeded\n",
      "23000 words embeded\n",
      "24000 words embeded\n",
      "25000 words embeded\n",
      "26000 words embeded\n",
      "27000 words embeded\n",
      "28000 words embeded\n",
      "29000 words embeded\n",
      "30000 words embeded\n",
      "31000 words embeded\n",
      "32000 words embeded\n",
      "33000 words embeded\n",
      "34000 words embeded\n",
      "35000 words embeded\n",
      "36000 words embeded\n",
      "37000 words embeded\n",
      "38000 words embeded\n",
      "39000 words embeded\n",
      "40000 words embeded\n",
      "41000 words embeded\n",
      "42000 words embeded\n",
      "43000 words embeded\n",
      "44000 words embeded\n",
      "45000 words embeded\n",
      "46000 words embeded\n",
      "47000 words embeded\n",
      "48000 words embeded\n",
      "49000 words embeded\n",
      "50000 words embeded\n",
      "51000 words embeded\n",
      "52000 words embeded\n",
      "53000 words embeded\n",
      "54000 words embeded\n",
      "55000 words embeded\n",
      "56000 words embeded\n",
      "57000 words embeded\n",
      "58000 words embeded\n",
      "59000 words embeded\n",
      "60000 words embeded\n",
      "61000 words embeded\n",
      "62000 words embeded\n",
      "63000 words embeded\n",
      "64000 words embeded\n",
      "65000 words embeded\n",
      "66000 words embeded\n",
      "67000 words embeded\n",
      "68000 words embeded\n",
      "69000 words embeded\n",
      "70000 words embeded\n",
      "71000 words embeded\n",
      "72000 words embeded\n",
      "73000 words embeded\n",
      "74000 words embeded\n",
      "75000 words embeded\n",
      "76000 words embeded\n",
      "77000 words embeded\n",
      "78000 words embeded\n",
      "79000 words embeded\n",
      "80000 words embeded\n",
      "81000 words embeded\n",
      "82000 words embeded\n",
      "83000 words embeded\n",
      "84000 words embeded\n",
      "85000 words embeded\n"
     ]
    }
   ],
   "source": [
    "if fine_tuned_model:\n",
    "    bert_embeddings = BertEmbeddings(model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "else:\n",
    "    bert_embeddings = BertEmbeddings(model_name = 'bert-large-uncased-whole-word-masking')\n",
    "# This will create a tensor too large for a single computer, a more powerful one is needed\n",
    "'''\n",
    "embeddings_index = {}\n",
    "progress = 1\n",
    "for word in word_index:\n",
    "    output = bert_embeddings([word])\n",
    "    for value in output[0]['embeddings_map'].values():\n",
    "        embeddings_index[word] = np.array(value)\n",
    "    if (progress % 10000 == 0):\n",
    "        print(str(progress) + \" words embeded\")\n",
    "    progress = progress + 1\n",
    "'''\n",
    "# Use this instead\n",
    "embeddings_index = {}\n",
    "progress = 1\n",
    "for word in word_index:\n",
    "    output = bert_embeddings([word])\n",
    "    result = np.array(output[0]['hidden_states'])[0][-1] + np.array(output[0]['hidden_states'])[0][-2] + np.array(output[0]['hidden_states'])[0][-3] + np.array(output[0]['hidden_states'])[0][-4]\n",
    "    embeddings_index[word] = result\n",
    "    if (progress % 1000 == 0):\n",
    "        print(str(progress) + \" words embeded\")\n",
    "    progress = progress + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# leaky features\n",
    "\n",
    "leaks = df_train[['q1_q2_intersect', 'q1_freq', 'q2_freq']]\n",
    "test_leaks = df_test[['q1_q2_intersect', 'q1_freq', 'q2_freq']]\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((leaks, test_leaks)))\n",
    "leaks = ss.transform(leaks)\n",
    "test_leaks = ss.transform(test_leaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized_feature_array = [1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add extra features input (optimal feature returned by XGBoost)\n",
    "if use_more_features:\n",
    "    #extra_features = df_train[['word_match_share', 'tfidf_word_match_share', 'tfidf_word_match', 'unigrams_common_count', 'unigrams_common_ratio',\n",
    "    #                      'jaccard', 'common_words', 'common_words_stop', 'total_unique_words', 'total_unq_words_stop', 'wc_diff', 'wc_ratio', \n",
    "    #                      'wc_diff_unique', 'wc_ratio_unique', 'wc_diff_unique_stop', 'wc_ratio_unique_stop', 'same_start_word', 'char_diff', \n",
    "    #                      'char_ratio', 'char_diff_unique_stop']]\n",
    "    extra_features = df_train[['word_match_share', 'tfidf_word_match', 'jaccard', 'total_unique_words', 'total_unq_words_stop', 'wc_diff', 'wc_ratio', \n",
    "                           'wc_diff_unique', 'wc_ratio_unique', 'same_start_word', 'char_diff', 'char_diff_unique_stop', 'q1_to_q2_wc_ratio_unique',\n",
    "                              'q1_to_q2_char_diff', 'q1_to_q2_char_diff_unique_stop', 'word_match_share_alternative_stop', 'common_words_alternative_stop',\n",
    "                              'total_unq_words_alternative_stop', 'wc_diff_unique_alternative_stop', 'char_diff_unique_alternative_stop',\n",
    "                              'q1_to_q2_wc_diff_unique_alternative_stop', 'q1_to_q2_wc_ratio_unique_alternative_stop', 'q1_to_q2_char_diff_unique_alternative_stop']]\n",
    "    #extra_features_test = df_test[['word_match_share', 'tfidf_word_match_share', 'tfidf_word_match', 'unigrams_common_count', 'unigrams_common_ratio',\n",
    "    #                      'jaccard', 'common_words', 'common_words_stop', 'total_unique_words', 'total_unq_words_stop', 'wc_diff', 'wc_ratio', \n",
    "    #                      'wc_diff_unique', 'wc_ratio_unique', 'wc_diff_unique_stop', 'wc_ratio_unique_stop', 'same_start_word', 'char_diff', \n",
    "    #                      'char_ratio', 'char_diff_unique_stop']]\n",
    "    extra_features_test = df_test[['word_match_share', 'tfidf_word_match', 'jaccard', 'total_unique_words', 'total_unq_words_stop', 'wc_diff', 'wc_ratio', \n",
    "                           'wc_diff_unique', 'wc_ratio_unique', 'same_start_word', 'char_diff', 'char_diff_unique_stop', 'q1_to_q2_wc_ratio_unique',\n",
    "                              'q1_to_q2_char_diff', 'q1_to_q2_char_diff_unique_stop', 'word_match_share_alternative_stop', 'common_words_alternative_stop',\n",
    "                              'total_unq_words_alternative_stop', 'wc_diff_unique_alternative_stop', 'char_diff_unique_alternative_stop',\n",
    "                              'q1_to_q2_wc_diff_unique_alternative_stop', 'q1_to_q2_wc_ratio_unique_alternative_stop', 'q1_to_q2_char_diff_unique_alternative_stop']]\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    ss.fit(np.vstack((extra_features, extra_features_test)))\n",
    "    extra_features = ss.transform(extra_features)\n",
    "    extra_features_test = ss.transform(extra_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: \n"
     ]
    }
   ],
   "source": [
    "# Create embedding matrix for embedding layer\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "num_words = min(Max_Num_Words, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, Embedding_Dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: '.format(np.sum(np.sum(embedding_matrix, axis=1) == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Validation split\n",
    "perm = np.random.permutation(len(train_data_1))\n",
    "idx_train = perm[:int(len(train_data_1)*(1-Validation_Split_Ratio))]\n",
    "idx_val = perm[int(len(train_data_1)*(1-Validation_Split_Ratio)):]\n",
    "\n",
    "data_1_train = np.vstack((train_data_1[idx_train], train_data_2[idx_train]))\n",
    "data_2_train = np.vstack((train_data_2[idx_train], train_data_1[idx_train]))\n",
    "leaks_train = np.vstack((leaks[idx_train], leaks[idx_train]))\n",
    "if use_more_features:\n",
    "    feature_train = np.vstack((extra_features[idx_train], extra_features[idx_train]))\n",
    "labels_train = np.concatenate((train_labels[idx_train], train_labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((train_data_1[idx_val], train_data_2[idx_val]))\n",
    "data_2_val = np.vstack((train_data_2[idx_val], train_data_1[idx_val]))\n",
    "leaks_val = np.vstack((leaks[idx_val], leaks[idx_val]))\n",
    "if use_more_features:\n",
    "    feature_val = np.vstack((extra_features[idx_val], extra_features[idx_val]))\n",
    "labels_val = np.concatenate((train_labels[idx_val], train_labels[idx_val]))\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.471544715\n",
    "    weight_val[labels_val==0] = 1.309033281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "313/313 [==============================] - 123s 390ms/step - loss: 0.3506 - acc: 0.8465 - val_loss: 0.4213 - val_acc: 0.8298\n",
      "Epoch 2/200\n",
      "313/313 [==============================] - 100s 320ms/step - loss: 0.3060 - acc: 0.8668 - val_loss: 0.3127 - val_acc: 0.8684\n",
      "Epoch 3/200\n",
      "313/313 [==============================] - 100s 321ms/step - loss: 0.2929 - acc: 0.8720 - val_loss: 0.2949 - val_acc: 0.8720\n",
      "Epoch 4/200\n",
      "313/313 [==============================] - 101s 322ms/step - loss: 0.2840 - acc: 0.8755 - val_loss: 0.2852 - val_acc: 0.8754\n",
      "Epoch 5/200\n",
      "313/313 [==============================] - 101s 322ms/step - loss: 0.2780 - acc: 0.8779 - val_loss: 0.2870 - val_acc: 0.8734\n",
      "Epoch 6/200\n",
      "313/313 [==============================] - 102s 327ms/step - loss: 0.2726 - acc: 0.8803 - val_loss: 0.2813 - val_acc: 0.8769\n",
      "Epoch 7/200\n",
      "313/313 [==============================] - 101s 322ms/step - loss: 0.2671 - acc: 0.8826 - val_loss: 0.2805 - val_acc: 0.8771\n",
      "Epoch 8/200\n",
      "313/313 [==============================] - 100s 319ms/step - loss: 0.2631 - acc: 0.8844 - val_loss: 0.2761 - val_acc: 0.8775\n",
      "Epoch 9/200\n",
      "313/313 [==============================] - 100s 318ms/step - loss: 0.2585 - acc: 0.8860 - val_loss: 0.2781 - val_acc: 0.8797\n",
      "Epoch 10/200\n",
      "313/313 [==============================] - 99s 318ms/step - loss: 0.2544 - acc: 0.8882 - val_loss: 0.2743 - val_acc: 0.8811\n",
      "Epoch 11/200\n",
      "313/313 [==============================] - 99s 318ms/step - loss: 0.2500 - acc: 0.8902 - val_loss: 0.2670 - val_acc: 0.8829\n",
      "Epoch 12/200\n",
      "313/313 [==============================] - 99s 318ms/step - loss: 0.2468 - acc: 0.8913 - val_loss: 0.2670 - val_acc: 0.8825\n",
      "Epoch 13/200\n",
      "313/313 [==============================] - 99s 318ms/step - loss: 0.2427 - acc: 0.8933 - val_loss: 0.2685 - val_acc: 0.8825\n",
      "Epoch 14/200\n",
      "313/313 [==============================] - 99s 318ms/step - loss: 0.2383 - acc: 0.8950 - val_loss: 0.2696 - val_acc: 0.8816\n",
      "Epoch 15/200\n",
      "313/313 [==============================] - 99s 318ms/step - loss: 0.2341 - acc: 0.8967 - val_loss: 0.2675 - val_acc: 0.8837\n",
      "Epoch 16/200\n",
      "313/313 [==============================] - 100s 318ms/step - loss: 0.2303 - acc: 0.8987 - val_loss: 0.2661 - val_acc: 0.8832\n",
      "Epoch 17/200\n",
      "313/313 [==============================] - 100s 319ms/step - loss: 0.2269 - acc: 0.8999 - val_loss: 0.2697 - val_acc: 0.8819\n",
      "Epoch 18/200\n",
      "313/313 [==============================] - 100s 319ms/step - loss: 0.2233 - acc: 0.9017 - val_loss: 0.2753 - val_acc: 0.8837\n",
      "Epoch 19/200\n",
      "313/313 [==============================] - 100s 319ms/step - loss: 0.2195 - acc: 0.9036 - val_loss: 0.2819 - val_acc: 0.8813\n",
      "Epoch 20/200\n",
      "313/313 [==============================] - 100s 319ms/step - loss: 0.2160 - acc: 0.9049 - val_loss: 0.2675 - val_acc: 0.8847\n",
      "Epoch 21/200\n",
      "313/313 [==============================] - 99s 318ms/step - loss: 0.2124 - acc: 0.9066 - val_loss: 0.2697 - val_acc: 0.8845\n",
      "Epoch 22/200\n",
      "313/313 [==============================] - 100s 321ms/step - loss: 0.2090 - acc: 0.9083 - val_loss: 0.2704 - val_acc: 0.8834\n",
      "Epoch 23/200\n",
      "313/313 [==============================] - 103s 329ms/step - loss: 0.2062 - acc: 0.9097 - val_loss: 0.2718 - val_acc: 0.8824\n",
      "Epoch 24/200\n",
      "313/313 [==============================] - 103s 329ms/step - loss: 0.2023 - acc: 0.9113 - val_loss: 0.2751 - val_acc: 0.8836\n",
      "Epoch 25/200\n",
      "313/313 [==============================] - 103s 329ms/step - loss: 0.1995 - acc: 0.9127 - val_loss: 0.2711 - val_acc: 0.8832\n",
      "Epoch 26/200\n",
      "313/313 [==============================] - 103s 329ms/step - loss: 0.1968 - acc: 0.9139 - val_loss: 0.2736 - val_acc: 0.8826\n"
     ]
    }
   ],
   "source": [
    "# The embedding layer containing the word vectors\n",
    "emb_layer = Embedding(\n",
    "    input_dim=num_words,\n",
    "    output_dim=Embedding_Dim,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=Max_Sequence_Length,\n",
    "    trainable=False\n",
    ")\n",
    "\n",
    "# 1D convolutions that can iterate over the word vectors\n",
    "conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\n",
    "conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\n",
    "conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n",
    "conv6 = Conv1D(filters=32, kernel_size=6, padding='same', activation='relu')\n",
    "\n",
    "# Define inputs\n",
    "seq1 = Input(shape=(60,))\n",
    "seq2 = Input(shape=(60,))\n",
    "\n",
    "# Run inputs through embedding\n",
    "emb1 = emb_layer(seq1)\n",
    "emb2 = emb_layer(seq2)\n",
    "\n",
    "# Run through CONV + GAP layers\n",
    "conv1a = conv1(emb1)\n",
    "glob1a = GlobalAveragePooling1D()(conv1a)\n",
    "conv1b = conv1(emb2)\n",
    "glob1b = GlobalAveragePooling1D()(conv1b)\n",
    "\n",
    "conv2a = conv2(emb1)\n",
    "glob2a = GlobalAveragePooling1D()(conv2a)\n",
    "conv2b = conv2(emb2)\n",
    "glob2b = GlobalAveragePooling1D()(conv2b)\n",
    "\n",
    "conv3a = conv3(emb1)\n",
    "glob3a = GlobalAveragePooling1D()(conv3a)\n",
    "conv3b = conv3(emb2)\n",
    "glob3b = GlobalAveragePooling1D()(conv3b)\n",
    "\n",
    "conv4a = conv4(emb1)\n",
    "glob4a = GlobalAveragePooling1D()(conv4a)\n",
    "conv4b = conv4(emb2)\n",
    "glob4b = GlobalAveragePooling1D()(conv4b)\n",
    "\n",
    "conv5a = conv5(emb1)\n",
    "glob5a = GlobalAveragePooling1D()(conv5a)\n",
    "conv5b = conv5(emb2)\n",
    "glob5b = GlobalAveragePooling1D()(conv5b)\n",
    "\n",
    "conv6a = conv6(emb1)\n",
    "glob6a = GlobalAveragePooling1D()(conv6a)\n",
    "conv6b = conv6(emb2)\n",
    "glob6b = GlobalAveragePooling1D()(conv6b)\n",
    "\n",
    "mergea = concatenate([glob1a, glob2a, glob3a, glob4a, glob5a, glob6a])\n",
    "mergeb = concatenate([glob1b, glob2b, glob3b, glob4b, glob5b, glob6b])\n",
    "\n",
    "# We take the explicit absolute difference between the two sentences\n",
    "# Furthermore we take the multiply different entries to get a different measure of equalness\n",
    "diff = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "mul = Lambda(lambda x: x[0] * x[1], output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "\n",
    "# Add the magic features\n",
    "magic_input = Input(shape=(leaks.shape[1],))\n",
    "magic_dense = BatchNormalization()(magic_input)\n",
    "magic_dense = Dense(64, activation='relu')(magic_dense)\n",
    "\n",
    "# Add the distance features (these are now TFIDF (character and word), Fuzzy matching, \n",
    "# nb char 1 and 2, word mover distance and skew/kurtosis of the sentence vector)\n",
    "if use_more_features:\n",
    "    feature_input = Input(shape=(extra_features.shape[1],))\n",
    "    feature_dense = BatchNormalization()(feature_input)\n",
    "    feature_dense = Dense(128, activation='relu')(feature_dense)\n",
    "\n",
    "# Merge the Magic and distance features with the difference layer\n",
    "if use_more_features:\n",
    "    merge = concatenate([diff, mul, magic_dense, feature_dense])\n",
    "else:\n",
    "    merge = concatenate([diff, mul, magic_dense])\n",
    "\n",
    "if re_weight:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "else:\n",
    "    class_weight = None\n",
    "\n",
    "# The MLP that determines the outcome\n",
    "x = Dropout(0.2)(merge)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(300, activation='relu')(x)\n",
    "\n",
    "x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "pred = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "if use_more_features:\n",
    "    model = Model(inputs=[seq1, seq2, magic_input, feature_input], outputs=pred)\n",
    "else:\n",
    "    model = Model(inputs=[seq1, seq2, magic_input], outputs=pred)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# Set early stopping (large patience should be useful)\n",
    "early_stopping =EarlyStopping(monitor='val_acc', patience=6)\n",
    "bst_model_path = Lstm_Struc + '.h5' \n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "if use_more_features:\n",
    "    hist = model.fit([data_1_train, data_2_train, leaks_train, feature_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val, leaks_val, feature_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "else:\n",
    "    hist = model.fit([data_1_train, data_2_train, leaks_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val, leaks_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path) # store model parameters in .h5 file\n",
    "bst_val_score = min(hist.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making the submission\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make the submission\n",
    "print('Making the submission')\n",
    "if use_more_features:\n",
    "    preds = model.predict([test_data_1, test_data_2, test_leaks, extra_features_test], batch_size=8192, verbose=1)\n",
    "    preds += model.predict([test_data_2, test_data_1, test_leaks, extra_features_test], batch_size=8192, verbose=1)\n",
    "    preds /= 2\n",
    "else:\n",
    "    preds = model.predict([test_data_1, test_data_2, test_leaks], batch_size=8192, verbose=1)\n",
    "    preds += model.predict([test_data_2, test_data_1, test_leaks], batch_size=8192, verbose=1)\n",
    "    preds /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 88.462%\n"
     ]
    }
   ],
   "source": [
    "# Convert percentage to binary predictions\n",
    "result = []\n",
    "sub_result = []\n",
    "for i in preds:\n",
    "    if i[0] < 0.5:\n",
    "        sub_result.append(0)\n",
    "    else:\n",
    "        sub_result.append(1)\n",
    "result.append(sub_result)\n",
    "result = np.array(result)\n",
    "\n",
    "# Get the accuracy on the test data\n",
    "true_values = df_test[\"is_duplicate (Ture Value)\"]\n",
    "\n",
    "score = 0\n",
    "for i in range(0, len(sub_result)):\n",
    "    if sub_result[i] == true_values.tolist()[i]:\n",
    "        score = score + 1\n",
    "accuracy = score / len(sub_result)\n",
    "print(\"Accuracy on test data: {}%\".format(round(accuracy*100, 3)))\n",
    "\n",
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':result.ravel()})\n",
    "submission.to_csv(\"GloVe + CNN_with_features\" + data_clean_type + \"(accuracy: + \" + str(round(accuracy*100, 3)) + \")\" + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
