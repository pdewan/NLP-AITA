{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GlobalAveragePooling1D, Lambda\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization.batch_normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_214_111_0.30_0.36\n"
     ]
    }
   ],
   "source": [
    "# Define constants and parameters\n",
    "\n",
    "# Supported data_clean_type (DO NOT forget to put on \"()\", also, if data_clean_type is NOT empty string, please put a \" \" before (cleaned)):\n",
    "# empty string, no character\n",
    "# (cleaned)\n",
    "# (cleaned)(hyper_cleaned)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)(words_shortened)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)(alternative_stopwords_used)\n",
    "# (cleaned)(hyper_cleaned)(punctuation_removed)(stopwords_removed)(alternative_stopwords_used)(words_shortened)\n",
    "data_clean_type = \" (cleaned)\"\n",
    "\n",
    "Embedding_File = 'glove.840B.300d.txt'\n",
    "Train_Data_File = 'train_with_features' + data_clean_type + '.csv'\n",
    "Test_Data_File = 'test_with_features' + data_clean_type + '.csv'\n",
    "Max_Sequence_Length = 60\n",
    "Max_Num_Words = 200000 # There are about 201000 unique words in training dataset, 200000 is enough for tokenization\n",
    "Embedding_Dim = 300\n",
    "Validation_Split_Ratio = 0.2\n",
    "\n",
    "Num_Lstm = np.random.randint(175, 275)\n",
    "Num_Dense = np.random.randint(100, 150)\n",
    "Rate_Drop_Lstm = 0.15 + np.random.rand() * 0.25\n",
    "Rate_Drop_Dense = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "Lstm_Struc = 'lstm_{:d}_{:d}_{:.2f}_{:.2f}'.format(Num_Lstm, Num_Dense, Rate_Drop_Lstm, \\\n",
    "Rate_Drop_Dense)\n",
    "print(Lstm_Struc)\n",
    "\n",
    "act_f = 'relu'\n",
    "re_weight = False # whether to re-weight classes to fit the 17.4% share in test set\n",
    "use_more_features = True # If true, add other 19 features. If false, only use leaky features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create word embedding dictionary\n",
      "Found 2195893 word vectors of glove.\n"
     ]
    }
   ],
   "source": [
    "# Create word embedding dictionary from 'glove.840B.300d.txt'\n",
    "print('Create word embedding dictionary')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(Embedding_File, encoding='utf-8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    # word = values[0]\n",
    "    word = ''.join(values[:-300])   \n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found {} word vectors of glove.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "# Process text in dataset\n",
    "print('Processing text dataset')\n",
    "\n",
    "# load data and process with text_to_wordlist\n",
    "df_train = pd.read_csv(Train_Data_File, encoding='utf-8')\n",
    "df_train = df_train.dropna()\n",
    "#df_train = df_train.fillna('empty')\n",
    "\n",
    "train_texts_1 = df_train['question1'].tolist()\n",
    "train_texts_2 = df_train['question2'].tolist()\n",
    "train_labels = df_train['is_duplicate'].tolist()\n",
    "\n",
    "df_test = pd.read_csv(Test_Data_File, encoding='utf-8')\n",
    "df_test = df_test.dropna()\n",
    "#df_test = df_test.fillna('empty')\n",
    "\n",
    "test_texts_1 = df_test['question1'].tolist()\n",
    "test_texts_2 = df_test['question2'].tolist()\n",
    "test_ids = df_test['test_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85509 unique tokens are found\n",
      "Shape of train data tensor: (399991, 60)\n",
      "Shape of train labels tensor: (399991,)\n",
      "Shape of test data tensor: (4290, 60)\n",
      "Shape of test ids tensor: (4290,)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize words in all sentences\n",
    "tokenizer = Tokenizer(num_words=Max_Num_Words)\n",
    "tokenizer.fit_on_texts(train_texts_1 + train_texts_2 + test_texts_1 + test_texts_2)\n",
    "\n",
    "train_sequences_1 = tokenizer.texts_to_sequences(train_texts_1)\n",
    "train_sequences_2 = tokenizer.texts_to_sequences(train_texts_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('{} unique tokens are found'.format(len(word_index)))\n",
    "\n",
    "# pad all train with Max_Sequence_Length\n",
    "train_data_1 = pad_sequences(train_sequences_1, maxlen=Max_Sequence_Length)\n",
    "train_data_2 = pad_sequences(train_sequences_2, maxlen=Max_Sequence_Length)\n",
    "train_labels = np.array(train_labels)\n",
    "print('Shape of train data tensor:', train_data_1.shape)\n",
    "print('Shape of train labels tensor:', train_labels.shape)\n",
    "\n",
    "# pad all test with Max_Sequence_Length\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=Max_Sequence_Length)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=Max_Sequence_Length)\n",
    "test_ids = np.array(test_ids)\n",
    "print('Shape of test data tensor:', test_data_2.shape)\n",
    "print('Shape of test ids tensor:', test_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# leaky features\n",
    "\n",
    "leaks = df_train[['q1_q2_intersect', 'q1_freq', 'q2_freq']]\n",
    "test_leaks = df_test[['q1_q2_intersect', 'q1_freq', 'q2_freq']]\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((leaks, test_leaks)))\n",
    "leaks = ss.transform(leaks)\n",
    "test_leaks = ss.transform(test_leaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized_feature_array = [1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['word_match_share' 'jaccard' 'total_unique_words' 'total_unq_words_stop'\\n 'wc_diff' 'wc_ratio' 'wc_diff_unique' 'wc_ratio_unique' 'same_start_word'\\n 'char_diff' 'char_diff_unique_stop' 'q1_to_q2_wc_ratio_unique'\\n 'q1_to_q2_char_diff' 'q1_to_q2_char_diff_unique_stop'\\n 'word_match_share_alternative_stop' 'common_words_alternative_stop'\\n 'total_unq_words_alternative_stop' 'wc_diff_unique_alternative_stop'\\n 'char_diff_unique_alternative_stop'\\n 'q1_to_q2_wc_diff_unique_alternative_stop'\\n 'q1_to_q2_wc_ratio_unique_alternative_stop'\\n 'q1_to_q2_char_diff_unique_alternative_stop'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-7835fd37ea05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m                               \u001b[1;34m'q1_to_q2_char_diff'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'q1_to_q2_char_diff_unique_stop'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'word_match_share_alternative_stop'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'common_words_alternative_stop'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                               \u001b[1;34m'total_unq_words_alternative_stop'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wc_diff_unique_alternative_stop'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'char_diff_unique_alternative_stop'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m                               'q1_to_q2_wc_diff_unique_alternative_stop', 'q1_to_q2_wc_ratio_unique_alternative_stop', 'q1_to_q2_char_diff_unique_alternative_stop']]\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[1;31m#extra_features_test = df_test[['word_match_share', 'tfidf_word_match_share', 'tfidf_word_match', 'unigrams_common_count', 'unigrams_common_ratio',\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#                      'jaccard', 'common_words', 'common_words_stop', 'total_unique_words', 'total_unq_words_stop', 'wc_diff', 'wc_ratio',\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2680\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2681\u001b[0m             \u001b[1;31m# either boolean or fancy integer index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2682\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2683\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2684\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2724\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2725\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2726\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2727\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_convert_to_indexer\u001b[1;34m(self, obj, axis, is_setter)\u001b[0m\n\u001b[0;32m   1325\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m                     raise KeyError('{mask} not in index'\n\u001b[1;32m-> 1327\u001b[1;33m                                    .format(mask=objarr[mask]))\n\u001b[0m\u001b[0;32m   1328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['word_match_share' 'jaccard' 'total_unique_words' 'total_unq_words_stop'\\n 'wc_diff' 'wc_ratio' 'wc_diff_unique' 'wc_ratio_unique' 'same_start_word'\\n 'char_diff' 'char_diff_unique_stop' 'q1_to_q2_wc_ratio_unique'\\n 'q1_to_q2_char_diff' 'q1_to_q2_char_diff_unique_stop'\\n 'word_match_share_alternative_stop' 'common_words_alternative_stop'\\n 'total_unq_words_alternative_stop' 'wc_diff_unique_alternative_stop'\\n 'char_diff_unique_alternative_stop'\\n 'q1_to_q2_wc_diff_unique_alternative_stop'\\n 'q1_to_q2_wc_ratio_unique_alternative_stop'\\n 'q1_to_q2_char_diff_unique_alternative_stop'] not in index\""
     ]
    }
   ],
   "source": [
    "# Add extra features input (optimal feature returned by XGBoost)\n",
    "if use_more_features:\n",
    "    #extra_features = df_train[['word_match_share', 'tfidf_word_match_share', 'tfidf_word_match', 'unigrams_common_count', 'unigrams_common_ratio',\n",
    "    #                      'jaccard', 'common_words', 'common_words_stop', 'total_unique_words', 'total_unq_words_stop', 'wc_diff', 'wc_ratio', \n",
    "    #                      'wc_diff_unique', 'wc_ratio_unique', 'wc_diff_unique_stop', 'wc_ratio_unique_stop', 'same_start_word', 'char_diff', \n",
    "    #                      'char_ratio', 'char_diff_unique_stop']]\n",
    "    extra_features = df_train[['word_match_share', 'tfidf_word_match', 'jaccard', 'total_unique_words', 'total_unq_words_stop', 'wc_diff', 'wc_ratio', \n",
    "                           'wc_diff_unique', 'wc_ratio_unique', 'same_start_word', 'char_diff', 'char_diff_unique_stop', 'q1_to_q2_wc_ratio_unique',\n",
    "                              'q1_to_q2_char_diff', 'q1_to_q2_char_diff_unique_stop', 'word_match_share_alternative_stop', 'common_words_alternative_stop',\n",
    "                              'total_unq_words_alternative_stop', 'wc_diff_unique_alternative_stop', 'char_diff_unique_alternative_stop',\n",
    "                              'q1_to_q2_wc_diff_unique_alternative_stop', 'q1_to_q2_wc_ratio_unique_alternative_stop', 'q1_to_q2_char_diff_unique_alternative_stop']]\n",
    "    #extra_features_test = df_test[['word_match_share', 'tfidf_word_match_share', 'tfidf_word_match', 'unigrams_common_count', 'unigrams_common_ratio',\n",
    "    #                      'jaccard', 'common_words', 'common_words_stop', 'total_unique_words', 'total_unq_words_stop', 'wc_diff', 'wc_ratio', \n",
    "    #                      'wc_diff_unique', 'wc_ratio_unique', 'wc_diff_unique_stop', 'wc_ratio_unique_stop', 'same_start_word', 'char_diff', \n",
    "    #                      'char_ratio', 'char_diff_unique_stop']]\n",
    "    extra_features_test = df_test[['word_match_share', 'tfidf_word_match', 'jaccard', 'total_unique_words', 'total_unq_words_stop', 'wc_diff', 'wc_ratio', \n",
    "                           'wc_diff_unique', 'wc_ratio_unique', 'same_start_word', 'char_diff', 'char_diff_unique_stop', 'q1_to_q2_wc_ratio_unique',\n",
    "                              'q1_to_q2_char_diff', 'q1_to_q2_char_diff_unique_stop', 'word_match_share_alternative_stop', 'common_words_alternative_stop',\n",
    "                              'total_unq_words_alternative_stop', 'wc_diff_unique_alternative_stop', 'char_diff_unique_alternative_stop',\n",
    "                              'q1_to_q2_wc_diff_unique_alternative_stop', 'q1_to_q2_wc_ratio_unique_alternative_stop', 'q1_to_q2_char_diff_unique_alternative_stop']]\n",
    "    \n",
    "    ss = StandardScaler()\n",
    "    ss.fit(np.vstack((extra_features, extra_features_test)))\n",
    "    extra_features = ss.transform(extra_features)\n",
    "    extra_features_test = ss.transform(extra_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix for embedding layer\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "num_words = min(Max_Num_Words, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, Embedding_Dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: '.format(np.sum(np.sum(embedding_matrix, axis=1) == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Validation split\n",
    "perm = np.random.permutation(len(train_data_1))\n",
    "idx_train = perm[:int(len(train_data_1)*(1-Validation_Split_Ratio))]\n",
    "idx_val = perm[int(len(train_data_1)*(1-Validation_Split_Ratio)):]\n",
    "\n",
    "data_1_train = np.vstack((train_data_1[idx_train], train_data_2[idx_train]))\n",
    "data_2_train = np.vstack((train_data_2[idx_train], train_data_1[idx_train]))\n",
    "leaks_train = np.vstack((leaks[idx_train], leaks[idx_train]))\n",
    "if use_more_features:\n",
    "    feature_train = np.vstack((extra_features[idx_train], extra_features[idx_train]))\n",
    "labels_train = np.concatenate((train_labels[idx_train], train_labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((train_data_1[idx_val], train_data_2[idx_val]))\n",
    "data_2_val = np.vstack((train_data_2[idx_val], train_data_1[idx_val]))\n",
    "leaks_val = np.vstack((leaks[idx_val], leaks[idx_val]))\n",
    "if use_more_features:\n",
    "    feature_val = np.vstack((extra_features[idx_val], extra_features[idx_val]))\n",
    "labels_val = np.concatenate((train_labels[idx_val], train_labels[idx_val]))\n",
    "\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.471544715\n",
    "    weight_val[labels_val==0] = 1.309033281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The embedding layer containing the word vectors\n",
    "emb_layer = Embedding(\n",
    "    input_dim=num_words,\n",
    "    output_dim=Embedding_Dim,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=Max_Sequence_Length,\n",
    "    trainable=False\n",
    ")\n",
    "\n",
    "# 1D convolutions that can iterate over the word vectors\n",
    "conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\n",
    "conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\n",
    "conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n",
    "conv6 = Conv1D(filters=32, kernel_size=6, padding='same', activation='relu')\n",
    "\n",
    "# Define inputs\n",
    "seq1 = Input(shape=(60,))\n",
    "seq2 = Input(shape=(60,))\n",
    "\n",
    "# Run inputs through embedding\n",
    "emb1 = emb_layer(seq1)\n",
    "emb2 = emb_layer(seq2)\n",
    "\n",
    "# Run through CONV + GAP layers\n",
    "conv1a = conv1(emb1)\n",
    "glob1a = GlobalAveragePooling1D()(conv1a)\n",
    "conv1b = conv1(emb2)\n",
    "glob1b = GlobalAveragePooling1D()(conv1b)\n",
    "\n",
    "conv2a = conv2(emb1)\n",
    "glob2a = GlobalAveragePooling1D()(conv2a)\n",
    "conv2b = conv2(emb2)\n",
    "glob2b = GlobalAveragePooling1D()(conv2b)\n",
    "\n",
    "conv3a = conv3(emb1)\n",
    "glob3a = GlobalAveragePooling1D()(conv3a)\n",
    "conv3b = conv3(emb2)\n",
    "glob3b = GlobalAveragePooling1D()(conv3b)\n",
    "\n",
    "conv4a = conv4(emb1)\n",
    "glob4a = GlobalAveragePooling1D()(conv4a)\n",
    "conv4b = conv4(emb2)\n",
    "glob4b = GlobalAveragePooling1D()(conv4b)\n",
    "\n",
    "conv5a = conv5(emb1)\n",
    "glob5a = GlobalAveragePooling1D()(conv5a)\n",
    "conv5b = conv5(emb2)\n",
    "glob5b = GlobalAveragePooling1D()(conv5b)\n",
    "\n",
    "conv6a = conv6(emb1)\n",
    "glob6a = GlobalAveragePooling1D()(conv6a)\n",
    "conv6b = conv6(emb2)\n",
    "glob6b = GlobalAveragePooling1D()(conv6b)\n",
    "\n",
    "mergea = concatenate([glob1a, glob2a, glob3a, glob4a, glob5a, glob6a])\n",
    "mergeb = concatenate([glob1b, glob2b, glob3b, glob4b, glob5b, glob6b])\n",
    "\n",
    "# We take the explicit absolute difference between the two sentences\n",
    "# Furthermore we take the multiply different entries to get a different measure of equalness\n",
    "diff = Lambda(lambda x: K.abs(x[0] - x[1]), output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "mul = Lambda(lambda x: x[0] * x[1], output_shape=(4 * 128 + 2*32,))([mergea, mergeb])\n",
    "\n",
    "# Add the magic features\n",
    "magic_input = Input(shape=(leaks.shape[1],))\n",
    "magic_dense = BatchNormalization()(magic_input)\n",
    "magic_dense = Dense(64, activation='relu')(magic_dense)\n",
    "\n",
    "# Add the distance features (these are now TFIDF (character and word), Fuzzy matching, \n",
    "# nb char 1 and 2, word mover distance and skew/kurtosis of the sentence vector)\n",
    "if use_more_features:\n",
    "    feature_input = Input(shape=(extra_features.shape[1],))\n",
    "    feature_dense = BatchNormalization()(feature_input)\n",
    "    feature_dense = Dense(128, activation='relu')(feature_dense)\n",
    "\n",
    "# Merge the Magic and distance features with the difference layer\n",
    "if use_more_features:\n",
    "    merge = concatenate([diff, mul, magic_dense, feature_dense])\n",
    "else:\n",
    "    merge = concatenate([diff, mul, magic_dense])\n",
    "\n",
    "if re_weight:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "else:\n",
    "    class_weight = None\n",
    "\n",
    "# The MLP that determines the outcome\n",
    "x = Dropout(0.2)(merge)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(300, activation='relu')(x)\n",
    "\n",
    "x = Dropout(0.2)(x)\n",
    "x = BatchNormalization()(x)\n",
    "pred = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "if use_more_features:\n",
    "    model = Model(inputs=[seq1, seq2, magic_input, feature_input], outputs=pred)\n",
    "else:\n",
    "    model = Model(inputs=[seq1, seq2, magic_input], outputs=pred)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "# Set early stopping (large patience should be useful)\n",
    "early_stopping =EarlyStopping(monitor='val_acc', patience=6)\n",
    "bst_model_path = Lstm_Struc + '.h5' \n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "if use_more_features:\n",
    "    hist = model.fit([data_1_train, data_2_train, leaks_train, feature_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val, leaks_val, feature_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "else:\n",
    "    hist = model.fit([data_1_train, data_2_train, leaks_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val, leaks_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path) # store model parameters in .h5 file\n",
    "bst_val_score = min(hist.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the submission\n",
    "print('Making the submission')\n",
    "if use_more_features:\n",
    "    preds = model.predict([test_data_1, test_data_2, test_leaks, extra_features_test], batch_size=8192, verbose=1)\n",
    "    preds += model.predict([test_data_2, test_data_1, test_leaks, extra_features_test], batch_size=8192, verbose=1)\n",
    "    preds /= 2\n",
    "else:\n",
    "    preds = model.predict([test_data_1, test_data_2, test_leaks], batch_size=8192, verbose=1)\n",
    "    preds += model.predict([test_data_2, test_data_1, test_leaks], batch_size=8192, verbose=1)\n",
    "    preds /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert percentage to binary predictions\n",
    "result = []\n",
    "sub_result = []\n",
    "for i in preds:\n",
    "    if i[0] < 0.5:\n",
    "        sub_result.append(0)\n",
    "    else:\n",
    "        sub_result.append(1)\n",
    "result.append(sub_result)\n",
    "result = np.array(result)\n",
    "# Get the accuracy on the test data\n",
    "true_values = df_test[\"is_duplicate (Ture Value)\"]\n",
    "\n",
    "score = 0\n",
    "for i in range(0, len(sub_result)):\n",
    "    if sub_result[i] == true_values.tolist()[i]:\n",
    "        score = score + 1\n",
    "accuracy = score / len(sub_result)\n",
    "print(\"Accuracy on test data: {}%\".format(round(accuracy*100, 3)))\n",
    "\n",
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':result.ravel()})\n",
    "submission.to_csv(\"GloVe + CNN_with_features\" + data_clean_type + \"(accuracy: + \" + str(round(accuracy*100, 3)) + \")\" + \".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
