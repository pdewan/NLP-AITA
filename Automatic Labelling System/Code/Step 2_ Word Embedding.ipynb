{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c308ce17",
   "metadata": {},
   "source": [
    "# Import Packages and Initialize DeBERTa, RoBERTa, BERT, GloVe, and Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337c400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from BertEmbeddings import BertEmbeddings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "from transformers import DebertaV2Tokenizer, TFDebertaV2Model\n",
    "import tensorflow as tf\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a67a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_embeddings = BertEmbeddings(model_name = 'bert-large-uncased-whole-word-masking')\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "roberta_model = TFRobertaModel.from_pretrained('roberta-large')\n",
    "deberta_tokenizer = DebertaV2Tokenizer.from_pretrained('kamalkraj/deberta-v2-xlarge')\n",
    "deberta_model = TFDebertaV2Model.from_pretrained('kamalkraj/deberta-v2-xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_post_Bert_embedding(post):\n",
    "    word = []\n",
    "    embedding = []\n",
    "    summed_embedding = []\n",
    "    try:\n",
    "        result = bert_embeddings([post])        \n",
    "        word = result[0]['tokens']\n",
    "        #word = np.array(word)\n",
    "        for key in result[0]['embeddings_map'].keys():\n",
    "            embedding.append(np.array(result[0]['embeddings_map'][key]))\n",
    "        #embedding = np.array(embedding)\n",
    "        summed_embedding = list(np.sum(embedding, axis = 0))\n",
    "        return word, embedding, summed_embedding\n",
    "    except:\n",
    "        #word = np.array(word)\n",
    "        #embedding = np.array(embedding)\n",
    "        #summed_embedding = np.array(summed_embedding)\n",
    "        return word, embedding, summed_embedding\n",
    "    \n",
    "def single_post_DeBerta_embedding(post):\n",
    "    word = []\n",
    "    embedding = []\n",
    "    summed_embedding = []\n",
    "    try:\n",
    "        inputs = deberta_tokenizer(post, return_tensors=\"tf\")\n",
    "        outputs = deberta_model(inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        nltk_tokens = nltk.word_tokenize(post)\n",
    "        result = np.zeros(outputs.last_hidden_state[0].shape[1])    \n",
    "        word = nltk_tokens\n",
    "        #word = np.array(word)\n",
    "        for embeddings_map in outputs.last_hidden_state[0][1:-1]:\n",
    "            embedding.append(np.array(embeddings_map))\n",
    "            result = result + embeddings_map\n",
    "        #embedding = np.array(embedding)\n",
    "        summed_embedding = list(np.array(result))\n",
    "        return word, embedding, summed_embedding\n",
    "    except:\n",
    "        #word = np.array(word)\n",
    "        #embedding = np.array(embedding)\n",
    "        #summed_embedding = np.array(summed_embedding)\n",
    "        return word, embedding, summed_embedding\n",
    "    \n",
    "def single_post_GloVe_embedding(post, embeddings_index):\n",
    "    word = []\n",
    "    embedding = []\n",
    "    summed_embedding = []\n",
    "    try:\n",
    "        word = post.split(\" \")      \n",
    "        for i in word:\n",
    "            embedding_vector = embeddings_index.get(i)\n",
    "            if embedding_vector is not None:\n",
    "                embedding.append(embedding_vector)\n",
    "            else:\n",
    "                embedding.append(np.zeros(300))\n",
    "        #embedding = np.array(embedding)\n",
    "        summed_embedding = list(np.sum(embedding, axis = 0))\n",
    "        return word, embedding, summed_embedding\n",
    "    except:\n",
    "        #word = np.array(word)\n",
    "        #embedding = np.array(embedding)\n",
    "        #summed_embedding = np.array(summed_embedding)\n",
    "        return word, embedding, summed_embedding\n",
    "    \n",
    "def single_post_RoBerta_embedding(post):\n",
    "    word = []\n",
    "    embedding = []\n",
    "    summed_embedding = []\n",
    "    try:\n",
    "        inputs = roberta_tokenizer(post, return_tensors=\"tf\")\n",
    "        outputs = roberta_model(inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "        nltk_tokens = nltk.word_tokenize(post)\n",
    "        result = np.zeros(outputs.last_hidden_state[0].shape[1])    \n",
    "        word = nltk_tokens\n",
    "        #word = np.array(word)\n",
    "        for embeddings_map in outputs.last_hidden_state[0][1:-1]:\n",
    "            embedding.append(np.array(embeddings_map))\n",
    "            result = result + embeddings_map\n",
    "        #embedding = np.array(embedding)\n",
    "        summed_embedding = list(np.array(result))\n",
    "        return word, embedding, summed_embedding\n",
    "    except:\n",
    "        #word = np.array(word)\n",
    "        #embedding = np.array(embedding)\n",
    "        #summed_embedding = np.array(summed_embedding)\n",
    "        return word, embedding, summed_embedding\n",
    "    \n",
    "def single_post_Word2Vec_embedding(post, embeddings_index):\n",
    "    word = []\n",
    "    embedding = []\n",
    "    summed_embedding = []\n",
    "    word = post.split(\" \")      \n",
    "    for i in word:\n",
    "        try:\n",
    "            embedding_vector = embeddings_index.get_vector(i)\n",
    "            embedding.append(embedding_vector)\n",
    "        except:\n",
    "            embedding.append(np.zeros(300))\n",
    "    #embedding = np.array(embedding)\n",
    "    summed_embedding = list(np.sum(embedding, axis = 0))\n",
    "    return word, embedding, summed_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246ec00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding(input_cvs_file_name_array, data_clean_type, embedding_method):\n",
    "    if embedding_method == \"Bert\":\n",
    "        for input_cvs_file_name in input_cvs_file_name_array:\n",
    "            print(embedding_method + \" embedding \" + input_cvs_file_name.split(\".\")[0] + data_clean_type + \".csv\" + \"...\")\n",
    "            data = pd.read_csv(input_cvs_file_name.split(\".\")[0] + data_clean_type + \".csv\")\n",
    "            words = []\n",
    "            embeddings = []\n",
    "            summed_embeddings = []\n",
    "            problem_index = []\n",
    "            all_posts = data['cleanedContent'].tolist()\n",
    "            for i in range(0, len(all_posts)):\n",
    "                if (i % 10 == 0):\n",
    "                    print(str(i) + \" posts' processing has finished\")\n",
    "                word, embedding, summed_embedding = single_post_Bert_embedding(all_posts[i])\n",
    "                if (len(word) == 0 and len(embedding) == 0 and len(summed_embedding) == 0):\n",
    "                    problem_index.append(i)\n",
    "                words.append(word)\n",
    "                embeddings.append(embedding)\n",
    "                summed_embeddings.append(summed_embedding)\n",
    "\n",
    "            embedded_data = pd.DataFrame(summed_embeddings)\n",
    "            embedded_data.insert(0, \"New Category\", data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"].tolist())\n",
    "            embedded_data.insert(0, \"category\", data[\"category\"].tolist())\n",
    "            embedded_data.insert(0, \"totalFollowUpPosts\", data[\"totalFollowUpPosts\"].tolist())\n",
    "            embedded_data.insert(0, \"timeCreated\", data[\"timeCreated\"].tolist())\n",
    "            embedded_data.insert(0, \"tags\", data[\"tags\"].tolist())\n",
    "            embedded_data.insert(0, \"cleanedContent\", data[\"cleanedContent\"].tolist())\n",
    "            embedded_data.insert(0, \"title\", data[\"title\"].tolist())\n",
    "            embedded_data.insert(0, \"type\", data[\"type\"].tolist())\n",
    "            embedded_data.insert(0, \"id\", data[\"Unnamed: 0\"].tolist())\n",
    "            embedded_data = embedded_data.drop(embedded_data.index[problem_index])\n",
    "\n",
    "            output_cvs_file_name = input_cvs_file_name.split(\".\")[0] + data_clean_type + \"(Bert Embedded)\" \n",
    "            embedded_data.to_csv(output_cvs_file_name + '.csv', index=False)\n",
    "        \n",
    "    elif embedding_method == \"DeBerta\":\n",
    "        for input_cvs_file_name in input_cvs_file_name_array:\n",
    "            print(embedding_method + \" embedding \" + input_cvs_file_name.split(\".\")[0] + data_clean_type + \".csv\" + \"...\")\n",
    "            data = pd.read_csv(input_cvs_file_name.split(\".\")[0] + data_clean_type + \".csv\")\n",
    "            words = []\n",
    "            embeddings = []\n",
    "            summed_embeddings = []\n",
    "            problem_index = []\n",
    "            all_posts = data['cleanedContent'].tolist()\n",
    "            for i in range(0, len(all_posts)):\n",
    "                if (i % 10 == 0):\n",
    "                    print(str(i) + \" posts' processing has finished\")\n",
    "                word, embedding, summed_embedding = single_post_DeBerta_embedding(all_posts[i])\n",
    "                if (len(word) == 0 and len(embedding) == 0 and len(summed_embedding) == 0):\n",
    "                    problem_index.append(i)\n",
    "                words.append(word)\n",
    "                embeddings.append(embedding)\n",
    "                summed_embeddings.append(summed_embedding)\n",
    "\n",
    "            embedded_data = pd.DataFrame(summed_embeddings)\n",
    "            embedded_data.insert(0, \"New Category\", data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"].tolist())\n",
    "            embedded_data.insert(0, \"category\", data[\"category\"].tolist())\n",
    "            embedded_data.insert(0, \"totalFollowUpPosts\", data[\"totalFollowUpPosts\"].tolist())\n",
    "            embedded_data.insert(0, \"timeCreated\", data[\"timeCreated\"].tolist())\n",
    "            embedded_data.insert(0, \"tags\", data[\"tags\"].tolist())\n",
    "            embedded_data.insert(0, \"cleanedContent\", data[\"cleanedContent\"].tolist())\n",
    "            embedded_data.insert(0, \"title\", data[\"title\"].tolist())\n",
    "            embedded_data.insert(0, \"type\", data[\"type\"].tolist())\n",
    "            embedded_data.insert(0, \"id\", data[\"Unnamed: 0\"].tolist())\n",
    "            embedded_data = embedded_data.drop(embedded_data.index[problem_index])\n",
    "\n",
    "            output_cvs_file_name = input_cvs_file_name.split(\".\")[0] + data_clean_type + \"(DeBerta Embedded)\" \n",
    "            embedded_data.to_csv(output_cvs_file_name + '.csv', index=False)\n",
    "    \n",
    "    elif embedding_method == \"GloVe\":\n",
    "        Embedding_Dim = 300 # Dimension of GloVe-embedding\n",
    "        Embedding_File = '/Users/gubow/COMP 691H/Find Duplicates Project/glove.840B.300d.txt'\n",
    "        # Create word embedding dictionary from 'glove.840B.300d.txt'\n",
    "        print('Creating GloVe word embedding dictionary...')\n",
    "\n",
    "        embeddings_index = {}\n",
    "        f = open(Embedding_File, encoding='utf-8')\n",
    "\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            # word = values[0]\n",
    "            word = ''.join(values[:-300])   \n",
    "            coefs = np.asarray(values[-300:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "\n",
    "        print('Found {} word vectors of glove.'.format(len(embeddings_index)))\n",
    "        for input_cvs_file_name in input_cvs_file_name_array:\n",
    "            print(embedding_method + \" embedding \" + input_cvs_file_name + \"...\")\n",
    "            \n",
    "            \n",
    "            data = pd.read_csv(input_cvs_file_name.split(\".\")[0] + data_clean_type + \".csv\")\n",
    "            words = []\n",
    "            embeddings = []\n",
    "            summed_embeddings = []\n",
    "            problem_index = []\n",
    "            all_posts = data['cleanedContent'].tolist()\n",
    "            for i in range(0, len(all_posts)):\n",
    "                if (i % 10 == 0):\n",
    "                    print(str(i) + \" posts' processing has finished\")\n",
    "\n",
    "                word, embedding, summed_embedding = single_post_GloVe_embedding(all_posts[i], embeddings_index)\n",
    "                if (len(word) == 0 and len(embedding) == 0 and len(summed_embedding) == 0):\n",
    "                    problem_index.append(i)\n",
    "                words.append(word)\n",
    "                embeddings.append(embedding)\n",
    "                summed_embeddings.append(summed_embedding)\n",
    "\n",
    "            embedded_data = pd.DataFrame(summed_embeddings)\n",
    "            embedded_data.insert(0, \"New Category\", data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"].tolist())\n",
    "            embedded_data.insert(0, \"category\", data[\"category\"].tolist())\n",
    "            embedded_data.insert(0, \"totalFollowUpPosts\", data[\"totalFollowUpPosts\"].tolist())\n",
    "            embedded_data.insert(0, \"timeCreated\", data[\"timeCreated\"].tolist())\n",
    "            embedded_data.insert(0, \"tags\", data[\"tags\"].tolist())\n",
    "            embedded_data.insert(0, \"cleanedContent\", data[\"cleanedContent\"].tolist())\n",
    "            embedded_data.insert(0, \"title\", data[\"title\"].tolist())\n",
    "            embedded_data.insert(0, \"type\", data[\"type\"].tolist())\n",
    "            embedded_data.insert(0, \"id\", data[\"Unnamed: 0\"].tolist())\n",
    "            embedded_data = embedded_data.drop(embedded_data.index[problem_index])\n",
    "\n",
    "            output_cvs_file_name = input_cvs_file_name.split(\".\")[0] + data_clean_type + \"(GloVe Embedded)\" \n",
    "            embedded_data.to_csv(output_cvs_file_name + '.csv', index=False)\n",
    "            \n",
    "    elif embedding_method == \"RoBerta\":\n",
    "        for input_cvs_file_name in input_cvs_file_name_array:\n",
    "            print(embedding_method + \" embedding \" + input_cvs_file_name.split(\".\")[0] + data_clean_type + \".csv\" + \"...\")\n",
    "            data = pd.read_csv(input_cvs_file_name.split(\".\")[0] + data_clean_type + \".csv\")\n",
    "            words = []\n",
    "            embeddings = []\n",
    "            summed_embeddings = []\n",
    "            problem_index = []\n",
    "            all_posts = data['cleanedContent'].tolist()\n",
    "            for i in range(0, len(all_posts)):\n",
    "                if (i % 10 == 0):\n",
    "                    print(str(i) + \" posts' processing has finished\")\n",
    "                word, embedding, summed_embedding = single_post_RoBerta_embedding(all_posts[i])\n",
    "                if (len(word) == 0 and len(embedding) == 0 and len(summed_embedding) == 0):\n",
    "                    problem_index.append(i)\n",
    "                words.append(word)\n",
    "                embeddings.append(embedding)\n",
    "                summed_embeddings.append(summed_embedding)\n",
    "\n",
    "            embedded_data = pd.DataFrame(summed_embeddings)\n",
    "            embedded_data.insert(0, \"New Category\", data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"].tolist())\n",
    "            embedded_data.insert(0, \"category\", data[\"category\"].tolist())\n",
    "            embedded_data.insert(0, \"totalFollowUpPosts\", data[\"totalFollowUpPosts\"].tolist())\n",
    "            embedded_data.insert(0, \"timeCreated\", data[\"timeCreated\"].tolist())\n",
    "            embedded_data.insert(0, \"tags\", data[\"tags\"].tolist())\n",
    "            embedded_data.insert(0, \"cleanedContent\", data[\"cleanedContent\"].tolist())\n",
    "            embedded_data.insert(0, \"title\", data[\"title\"].tolist())\n",
    "            embedded_data.insert(0, \"type\", data[\"type\"].tolist())\n",
    "            embedded_data.insert(0, \"id\", data[\"Unnamed: 0\"].tolist())\n",
    "            embedded_data = embedded_data.drop(embedded_data.index[problem_index])\n",
    "\n",
    "            output_cvs_file_name = input_cvs_file_name.split(\".\")[0] + data_clean_type + \"(RoBerta Embedded)\" \n",
    "            embedded_data.to_csv(output_cvs_file_name + '.csv', index=False)\n",
    "\n",
    "    elif embedding_method == \"Word2Vec\":\n",
    "        Embedding_Dim = 300 # Dimension of Word2Vec-embedding\n",
    "        Embedding_File = '/Users/gubow/COMP 691H/Find Duplicates Project/GoogleNews-vectors-negative300.bin'\n",
    "        print('Creating Word2Vec word embedding dictionary...')\n",
    "\n",
    "        word2vec = KeyedVectors.load_word2vec_format(datapath(Embedding_File), binary=True)\n",
    "        print('Found %s word vectors of word2vec' % len(word2vec))\n",
    "        for input_cvs_file_name in input_cvs_file_name_array:\n",
    "            print(embedding_method + \" embedding \" + input_cvs_file_name + \"...\")\n",
    "            \n",
    "            data = pd.read_csv(input_cvs_file_name.split(\".\")[0] + data_clean_type + \".csv\")\n",
    "            words = []\n",
    "            embeddings = []\n",
    "            summed_embeddings = []\n",
    "            problem_index = []\n",
    "            all_posts = data['cleanedContent'].tolist()\n",
    "            for i in range(0, len(all_posts)):\n",
    "                if (i % 10 == 0):\n",
    "                    print(str(i) + \" posts' processing has finished\")\n",
    "                word, embedding, summed_embedding = single_post_Word2Vec_embedding(all_posts[i], word2vec)\n",
    "                if (len(word) == 0 and len(embedding) == 0 and len(summed_embedding) == 0):\n",
    "                    problem_index.append(i)\n",
    "                words.append(word)\n",
    "                embeddings.append(embedding)\n",
    "                summed_embeddings.append(summed_embedding)\n",
    "\n",
    "            embedded_data = pd.DataFrame(summed_embeddings)\n",
    "            embedded_data.insert(0, \"New Category\", data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"].tolist())\n",
    "            embedded_data.insert(0, \"category\", data[\"category\"].tolist())\n",
    "            embedded_data.insert(0, \"totalFollowUpPosts\", data[\"totalFollowUpPosts\"].tolist())\n",
    "            embedded_data.insert(0, \"timeCreated\", data[\"timeCreated\"].tolist())\n",
    "            embedded_data.insert(0, \"tags\", data[\"tags\"].tolist())\n",
    "            embedded_data.insert(0, \"cleanedContent\", data[\"cleanedContent\"].tolist())\n",
    "            embedded_data.insert(0, \"title\", data[\"title\"].tolist())\n",
    "            embedded_data.insert(0, \"type\", data[\"type\"].tolist())\n",
    "            embedded_data.insert(0, \"id\", data[\"Unnamed: 0\"].tolist())\n",
    "            embedded_data = embedded_data.drop(embedded_data.index[problem_index])\n",
    "\n",
    "            output_cvs_file_name = input_cvs_file_name.split(\".\")[0] + data_clean_type + \"(Word2Vec Embedded)\" \n",
    "            embedded_data.to_csv(output_cvs_file_name + '.csv', index=False)\n",
    "\n",
    "    else:\n",
    "        print(\"Undefined embedding method\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b83bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_clean_type:\n",
    "# 1. (cleaned)\n",
    "# 2. (hyper_cleaned)\n",
    "# 3. (punctuation_removed)\n",
    "# 4. (stopwords_removed)\n",
    "# 5. (alternative_stopwords_used)\n",
    "# 6. (words_shortened)\n",
    "\n",
    "# embedding_method:\n",
    "# 1. Bert (will use Bert large)\n",
    "# 2. DeBerta (will use DeBerta-v2-xlarge)\n",
    "# 3. GloVe\n",
    "# 4. RoBerta (will use RoBerta-large)\n",
    "# 5. Word2Vec\n",
    "\n",
    "input_cvs_file_name_array = ['comp401-Fall-2012_labeled (new).csv', 'comp401-Fall-2013_labeled (new).csv', 'comp401-Fall-2015_labeled (new).csv', 'comp401-Fall-2016_labeled (new).csv', 'comp401-Fall-2017_labeled (new).csv', 'comp401-Fall-2018_labeled (new).csv', 'comp411-Fall-2019_labeled (new).csv', 'comp411-Fall-2020_labeled (new).csv', 'comp411-Spring-2019_labeled (new).csv', 'comp411-Spring-2020A_labeled (new).csv', 'comp411-Spring-2020B_labeled (new).csv', 'comp426-Fall-2019_labeled (new).csv', 'comp426-Fall-2020_labeled (new).csv']\n",
    "data_clean_type = \"(cleaned)(punctuation_removed)(stopwords_removed)(alternative_stopwords_used)\"\n",
    "embedding_method = \"Word2Vec\"\n",
    "\n",
    "word_embedding(input_cvs_file_name_array, data_clean_type, embedding_method)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
