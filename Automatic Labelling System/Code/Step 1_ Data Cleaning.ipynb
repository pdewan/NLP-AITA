{"cells":[{"cell_type":"code","execution_count":null,"id":"b687f7bd","metadata":{"id":"b687f7bd"},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import os\n","import gc\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","from nltk.corpus import stopwords\n","from string import punctuation\n","from nltk.stem import SnowballStemmer"]},{"cell_type":"code","execution_count":null,"id":"bb61264b","metadata":{"id":"bb61264b"},"outputs":[],"source":["stops = set(stopwords.words(\"english\"))\n","print(stops)\n","\n","alternative_stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n","              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n","              'Is','If','While','This']"]},{"cell_type":"code","execution_count":null,"id":"f87111f3","metadata":{"id":"f87111f3"},"outputs":[],"source":["# Parameters:\n","# hyper_cleaning: If true, do a second round, more through data cleaning\n","# remove_punctuation: If true, remove all punctuations in the text\n","# remove_stopwords: If true, remove all stopwords (the list of stop words is shown above)\n","# use_alternative_stopwords: If true, remove all stopwords in a smaller set of stopwords\n","# stem_words: If true, shorten words to their stems\n","\n","def data_cleaning(input_cvs_file_name_array, hyper_cleaning = False, remove_punctuation = True, remove_stopwords = True, use_alternative_stopwords = True, stem_words = False):\n","    for input_cvs_file_name in input_cvs_file_name_array:\n","        print(\"Cleaning \" + input_cvs_file_name + \"...\")\n","        data = pd.read_csv(input_cvs_file_name)\n","        contents = data['cleanedContent']\n","\n","        contents_cleaned = []\n","\n","        for content in contents:\n","            cleaned_content = text_to_wordlist(str(content), hyper_cleaning, remove_punctuation, remove_stopwords, use_alternative_stopwords, stem_words)\n","            contents_cleaned.append(cleaned_content)\n","\n","        data['cleanedContent'] = contents_cleaned\n","\n","        output_cvs_file_name = input_cvs_file_name.split(\".\")[0] + \"(cleaned)\"\n","\n","        if hyper_cleaning:\n","            output_cvs_file_name = output_cvs_file_name + \"(hyper_cleaned)\"\n","        if remove_punctuation:\n","            output_cvs_file_name = output_cvs_file_name + \"(punctuation_removed)\"\n","        if remove_stopwords:\n","            output_cvs_file_name = output_cvs_file_name + \"(stopwords_removed)\"\n","        if use_alternative_stopwords:\n","            output_cvs_file_name = output_cvs_file_name + \"(alternative_stopwords_used)\"\n","        if stem_words:\n","            output_cvs_file_name = output_cvs_file_name + \"(words_shortened)\"\n","\n","        data.to_csv(output_cvs_file_name + '.csv', index=False)"]},{"cell_type":"code","execution_count":null,"id":"63d3e7c1","metadata":{"id":"63d3e7c1"},"outputs":[],"source":["def text_to_wordlist(text, hyper_cleaning, remove_punctuation, remove_stopwords, use_alternative_stopwords, stem_words):\n","    \n","    # Clean the text, with the option to remove stopwords and to stem words.\n","    \n","    # Convert words to lower case and split them\n","    text = text.lower().split()\n","\n","    # Optionally, remove stop words\n","    if remove_stopwords:\n","        if use_alternative_stopwords:\n","            stops = ['the','a','an','and','but','if','or','because','as','what','which','this','that','these','those','then',\n","              'just','so','than','such','both','through','about','for','is','of','while','during','to','What','Which',\n","              'Is','If','While','This']\n","        else:\n","            stops = set(stopwords.words(\"english\"))\n","        text = [w for w in text if not w in stops]\n","\n","    text = \" \".join(text)\n","\n","    # Clean the text\n","    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n","    text = re.sub(r\"what's\", \"what is \", text)\n","    text = re.sub(r\"\\'s\", \" \", text)\n","    text = re.sub(r\"\\'ve\", \" have \", text)\n","    text = re.sub(r\"can't\", \"cannot \", text)\n","    text = re.sub(r\"n't\", \" not \", text)\n","    text = re.sub(r\"i'm\", \"i am \", text)\n","    text = re.sub(r\"\\'re\", \" are \", text)\n","    text = re.sub(r\"\\'d\", \" would \", text)\n","    text = re.sub(r\"\\'ll\", \" will \", text)\n","    text = re.sub(r\",\", \" \", text)\n","    text = re.sub(r\"\\.\", \" \", text)\n","    text = re.sub(r\"!\", \" ! \", text)\n","    text = re.sub(r\"\\/\", \" \", text)\n","    text = re.sub(r\"\\^\", \" ^ \", text)\n","    text = re.sub(r\"\\+\", \" + \", text)\n","    text = re.sub(r\"\\-\", \" - \", text)\n","    text = re.sub(r\"\\=\", \" = \", text)\n","    text = re.sub(r\"'\", \" \", text)\n","    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n","    text = re.sub(r\":\", \" : \", text)\n","    text = re.sub(r\" e g \", \" eg \", text)\n","    text = re.sub(r\" b g \", \" bg \", text)\n","    text = re.sub(r\" u s \", \" american \", text)\n","    text = re.sub(r\"\\0s\", \"0\", text)\n","    text = re.sub(r\" 9 11 \", \"911\", text)\n","    text = re.sub(r\"e - mail\", \"email\", text)\n","    text = re.sub(r\"j k\", \"jk\", text)\n","    text = re.sub(r\"\\s{2,}\", \" \", text)\n","\n","    if hyper_cleaning:\n","        # Clean the text (second round)\n","        text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n","        text = re.sub(r\"what's\", \"\", text)\n","        text = re.sub(r\"What's\", \"\", text)\n","        text = re.sub(r\"\\'s\", \" \", text)\n","        text = re.sub(r\"\\'ve\", \" have \", text)\n","        text = re.sub(r\"can't\", \"cannot \", text)\n","        text = re.sub(r\"n't\", \" not \", text)\n","        text = re.sub(r\"I'm\", \"I am\", text)\n","        text = re.sub(r\" m \", \" am \", text)\n","        text = re.sub(r\"\\'re\", \" are \", text)\n","        text = re.sub(r\"\\'d\", \" would \", text)\n","        text = re.sub(r\"\\'ll\", \" will \", text)\n","        text = re.sub(r\"60k\", \" 60000 \", text)\n","        text = re.sub(r\" e g \", \" eg \", text)\n","        text = re.sub(r\" b g \", \" bg \", text)\n","        text = re.sub(r\"\\0s\", \"0\", text)\n","        text = re.sub(r\" 9 11 \", \"911\", text)\n","        text = re.sub(r\"e-mail\", \"email\", text)\n","        text = re.sub(r\"\\s{2,}\", \" \", text)\n","        text = re.sub(r\"quikly\", \"quickly\", text)\n","        text = re.sub(r\" usa \", \" America \", text)\n","        text = re.sub(r\" USA \", \" America \", text)\n","        text = re.sub(r\" u s \", \" America \", text)\n","        text = re.sub(r\" uk \", \" England \", text)\n","        text = re.sub(r\" UK \", \" England \", text)\n","        text = re.sub(r\"india\", \"India\", text)\n","        text = re.sub(r\"switzerland\", \"Switzerland\", text)\n","        text = re.sub(r\"china\", \"China\", text)\n","        text = re.sub(r\"chinese\", \"Chinese\", text) \n","        text = re.sub(r\"imrovement\", \"improvement\", text)\n","        text = re.sub(r\"intially\", \"initially\", text)\n","        text = re.sub(r\"quora\", \"Quora\", text)\n","        text = re.sub(r\" dms \", \"direct messages \", text)  \n","        text = re.sub(r\"demonitization\", \"demonetization\", text) \n","        text = re.sub(r\"actived\", \"active\", text)\n","        text = re.sub(r\"kms\", \" kilometers \", text)\n","        text = re.sub(r\"KMs\", \" kilometers \", text)\n","        text = re.sub(r\" cs \", \" computer science \", text) \n","        text = re.sub(r\" upvotes \", \" up votes \", text)\n","        text = re.sub(r\" iPhone \", \" phone \", text)\n","        text = re.sub(r\"\\0rs \", \" rs \", text) \n","        text = re.sub(r\"calender\", \"calendar\", text)\n","        text = re.sub(r\"ios\", \"operating system\", text)\n","        text = re.sub(r\"gps\", \"GPS\", text)\n","        text = re.sub(r\"gst\", \"GST\", text)\n","        text = re.sub(r\"programing\", \"programming\", text)\n","        text = re.sub(r\"bestfriend\", \"best friend\", text)\n","        text = re.sub(r\"dna\", \"DNA\", text)\n","        text = re.sub(r\"III\", \"3\", text) \n","        text = re.sub(r\"the US\", \"America\", text)\n","        text = re.sub(r\"Astrology\", \"astrology\", text)\n","        text = re.sub(r\"Method\", \"method\", text)\n","        text = re.sub(r\"Find\", \"find\", text) \n","        text = re.sub(r\"banglore\", \"Banglore\", text)\n","        text = re.sub(r\" J K \", \" JK \", text)\n","\n","    if remove_punctuation:\n","        # Remove punctuation from text\n","        text = ''.join([c for c in text if c not in punctuation])\n","\n","    # Optionally, shorten words to their stems\n","    if stem_words:\n","        text = text.split()\n","        stemmer = SnowballStemmer('english')\n","        stemmed_words = [stemmer.stem(word) for word in text]\n","        text = \" \".join(stemmed_words)\n","\n","    # Return a list of words\n","    return(text)"]},{"cell_type":"code","execution_count":null,"id":"af641ee6","metadata":{"id":"af641ee6"},"outputs":[],"source":["# load data\n","input_cvs_file_name_array = ['comp401-Fall-2012_labeled (new).csv', 'comp401-Fall-2013_labeled (new).csv', 'comp401-Fall-2015_labeled (new).csv', 'comp401-Fall-2016_labeled (new).csv', 'comp401-Fall-2017_labeled (new).csv', 'comp401-Fall-2018_labeled (new).csv', 'comp411-Fall-2019_labeled (new).csv', 'comp411-Fall-2020_labeled (new).csv', 'comp411-Spring-2019_labeled (new).csv', 'comp411-Spring-2020A_labeled (new).csv', 'comp411-Spring-2020B_labeled (new).csv', 'comp426-Fall-2019_labeled (new).csv', 'comp426-Fall-2020_labeled (new).csv']\n","data_cleaning(input_cvs_file_name_array)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"Step 1_ Data Cleaning.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}