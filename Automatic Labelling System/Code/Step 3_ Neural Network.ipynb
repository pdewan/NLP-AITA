{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e8b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GlobalAveragePooling1D, Lambda\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization.batch_normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras.backend as K\n",
    "\n",
    "from BertEmbeddings import BertEmbeddings\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.test.utils import datapath\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "from transformers import DebertaV2Tokenizer, TFDebertaV2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97527a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_Sequence_Length = 60\n",
    "Max_Num_Words = 200000\n",
    "\n",
    "Validation_Split_Ratio = 0.2\n",
    "\n",
    "Num_Lstm = np.random.randint(175, 275)\n",
    "Num_Dense = np.random.randint(100, 150)\n",
    "Rate_Drop_Lstm = 0.15 + np.random.rand() * 0.25\n",
    "Rate_Drop_Dense = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "act_f = 'relu'\n",
    "\n",
    "# training_data_type\n",
    "# 1. all (use all data as training data)\n",
    "# 2. COMP401 (use all COMP 401 courses as training data)\n",
    "# 3. COMP411 (use all COMP 411 courses as training data)\n",
    "# 4. COMP426 (use all COMP 426 courses as training data)\n",
    "\n",
    "# test_data_type\n",
    "# 1. all (use all data as test data)\n",
    "# 2. COMP401 (use all COMP 401 courses as test data)\n",
    "# 3. COMP411 (use all COMP 411 courses as test data)\n",
    "# 4. COMP426 (use all COMP 426 courses as test data)\n",
    "\n",
    "# data_clean_type:\n",
    "# 1. (cleaned)\n",
    "# 2. (hyper_cleaned)\n",
    "# 3. (punctuation_removed)\n",
    "# 4. (stopwords_removed)\n",
    "# 5. (alternative_stopwords_used)\n",
    "# 6. (words_shortened)\n",
    "\n",
    "# embedding_method:\n",
    "# 1. Bert (will use Bert large)\n",
    "# 2. DeBerta (will use DeBerta-v2-xlarge)\n",
    "# 3. GloVe\n",
    "# 4. RoBerta (will use RoBerta-large)\n",
    "# 5. Word2Vec\n",
    "\n",
    "# Support neural network:\n",
    "# 1. LSTM\n",
    "# 2. CNN\n",
    "\n",
    "training_data_type = \"COMP401\"\n",
    "test_data_type = \"COMP411\"\n",
    "data_clean_type = \"(cleaned)(punctuation_removed)(stopwords_removed)(alternative_stopwords_used)\"\n",
    "embedding_method = \"DeBerta\"\n",
    "neural_network = \"LSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f13fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name below should be changed according to different courses\n",
    "data_array = []\n",
    "training_data_array = []\n",
    "test_data_array = []\n",
    "all_data = pd.DataFrame()\n",
    "train = pd.DataFrame()\n",
    "test = pd.DataFrame()\n",
    "\n",
    "if training_data_type == test_data_type:\n",
    "    if training_data_type == \"all\":\n",
    "        data_array = ['comp401-Fall-2012_labeled (new).csv', 'comp401-Fall-2013_labeled (new).csv', 'comp401-Fall-2015_labeled (new).csv', 'comp401-Fall-2016_labeled (new).csv', 'comp401-Fall-2017_labeled (new).csv', 'comp401-Fall-2018_labeled (new).csv', 'comp411-Fall-2019_labeled (new).csv', 'comp411-Fall-2020_labeled (new).csv', 'comp411-Spring-2019_labeled (new).csv', 'comp411-Spring-2020A_labeled (new).csv', 'comp411-Spring-2020B_labeled (new).csv', 'comp426-Fall-2019_labeled (new).csv', 'comp426-Fall-2020_labeled (new).csv']\n",
    "    elif training_data_type == \"COMP401\":\n",
    "        data_array = ['comp401-Fall-2012_labeled (new).csv', 'comp401-Fall-2013_labeled (new).csv', 'comp401-Fall-2015_labeled (new).csv', 'comp401-Fall-2016_labeled (new).csv', 'comp401-Fall-2017_labeled (new).csv', 'comp401-Fall-2018_labeled (new).csv']\n",
    "    elif training_data_type == \"COMP411\":\n",
    "        data_array = ['comp411-Fall-2019_labeled (new).csv', 'comp411-Fall-2020_labeled (new).csv', 'comp411-Spring-2019_labeled (new).csv', 'comp411-Spring-2020A_labeled (new).csv', 'comp411-Spring-2020B_labeled (new).csv']\n",
    "    elif training_data_type == \"COMP426\":\n",
    "        data_array = ['comp426-Fall-2019_labeled (new).csv', 'comp426-Fall-2020_labeled (new).csv']\n",
    "    else:\n",
    "        print(\"Unidefined data type\")\n",
    "    for file_name in data_array:\n",
    "        data = pd.read_csv(file_name.split(\".\")[0] + data_clean_type + \".csv\")\n",
    "        data = data[(data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] != 'None') & (data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] != 'Note') & (data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] != 'Poll')]\n",
    "        data.loc[data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] == \"Active\", \"Converted category\"] = 0\n",
    "        data.loc[data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] == \"Constructive\", \"Converted category\"] = 1\n",
    "        data.loc[data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] == \"Logistical\", \"Converted category\"] = 2\n",
    "        data.loc[data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] == \"Content-Clarification\", \"Converted category\"] = 3\n",
    "        all_data = all_data.append(data, ignore_index=True)\n",
    "\n",
    "    train, test  = train_test_split(all_data, test_size=0.20, random_state=4242)\n",
    "\n",
    "else:\n",
    "    if training_data_type == \"all\":\n",
    "        training_data_array = ['comp401-Fall-2012_labeled (new).csv', 'comp401-Fall-2013_labeled (new).csv', 'comp401-Fall-2015_labeled (new).csv', 'comp401-Fall-2016_labeled (new).csv', 'comp401-Fall-2017_labeled (new).csv', 'comp401-Fall-2018_labeled (new).csv', 'comp411-Fall-2019_labeled (new).csv', 'comp411-Fall-2020_labeled (new).csv', 'comp411-Spring-2019_labeled (new).csv', 'comp411-Spring-2020A_labeled (new).csv', 'comp411-Spring-2020B_labeled (new).csv', 'comp426-Fall-2019_labeled (new).csv', 'comp426-Fall-2020_labeled (new).csv']\n",
    "    elif training_data_type == \"COMP401\":\n",
    "        training_data_array = ['comp401-Fall-2012_labeled (new).csv', 'comp401-Fall-2013_labeled (new).csv', 'comp401-Fall-2015_labeled (new).csv', 'comp401-Fall-2016_labeled (new).csv', 'comp401-Fall-2017_labeled (new).csv', 'comp401-Fall-2018_labeled (new).csv']\n",
    "    elif training_data_type == \"COMP411\":\n",
    "        training_data_array = ['comp411-Fall-2019_labeled (new).csv', 'comp411-Fall-2020_labeled (new).csv', 'comp411-Spring-2019_labeled (new).csv', 'comp411-Spring-2020A_labeled (new).csv', 'comp411-Spring-2020B_labeled (new).csv']\n",
    "    elif training_data_type == \"COMP426\":\n",
    "        training_data_array = ['comp426-Fall-2019_labeled (new).csv', 'comp426-Fall-2020_labeled (new).csv']\n",
    "    else:\n",
    "        print(\"Unidefined data type\")\n",
    "    if test_data_type == \"all\":\n",
    "        test_data_array = ['comp401-Fall-2012_labeled (new).csv', 'comp401-Fall-2013_labeled (new).csv', 'comp401-Fall-2015_labeled (new).csv', 'comp401-Fall-2016_labeled (new).csv', 'comp401-Fall-2017_labeled (new).csv', 'comp401-Fall-2018_labeled (new).csv', 'comp411-Fall-2019_labeled (new).csv', 'comp411-Fall-2020_labeled (new).csv', 'comp411-Spring-2019_labeled (new).csv', 'comp411-Spring-2020A_labeled (new).csv', 'comp411-Spring-2020B_labeled (new).csv', 'comp426-Fall-2019_labeled (new).csv', 'comp426-Fall-2020_labeled (new).csv']\n",
    "    elif test_data_type == \"COMP401\":\n",
    "        test_data_array = ['comp401-Fall-2012_labeled (new).csv', 'comp401-Fall-2013_labeled (new).csv', 'comp401-Fall-2015_labeled (new).csv', 'comp401-Fall-2016_labeled (new).csv', 'comp401-Fall-2017_labeled (new).csv', 'comp401-Fall-2018_labeled (new).csv']\n",
    "    elif test_data_type == \"COMP411\":\n",
    "        test_data_array = ['comp411-Fall-2019_labeled (new).csv', 'comp411-Fall-2020_labeled (new).csv', 'comp411-Spring-2019_labeled (new).csv', 'comp411-Spring-2020A_labeled (new).csv', 'comp411-Spring-2020B_labeled (new).csv']\n",
    "    elif test_data_type == \"COMP426\":\n",
    "        test_data_array = ['comp426-Fall-2019_labeled (new).csv', 'comp426-Fall-2020_labeled (new).csv']\n",
    "    else:\n",
    "        print(\"Unidefined data type\")\n",
    "\n",
    "    for file_name in training_data_array:\n",
    "        train_data = pd.read_csv(file_name.split(\".\")[0] + data_clean_type + \".csv\")\n",
    "        train_data = train_data[(train_data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] != 'None') & (train_data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] != 'Note') & (train_data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] != 'Poll')]\n",
    "        train_data.loc[train_data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] == \"Active\", \"Converted category\"] = 0\n",
    "        train_data.loc[train_data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] == \"Constructive\", \"Converted category\"] = 1\n",
    "        train_data.loc[train_data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] == \"Logistical\", \"Converted category\"] = 2\n",
    "        train_data.loc[train_data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] == \"Content-Clarification\", \"Converted category\"] = 3\n",
    "        train = train.append(train_data, ignore_index=True)\n",
    "    for file_name in test_data_array:\n",
    "        test_data = pd.read_csv(file_name.split(\".\")[0] + data_clean_type + \".csv\")\n",
    "        test_data = test_data[(test_data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] != 'None') & (test_data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] != 'Note') & (test_data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] != 'Poll')]\n",
    "        test_data.loc[test_data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] == \"Active\", \"Converted category\"] = 0\n",
    "        test_data.loc[test_data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] == \"Constructive\", \"Converted category\"] = 1\n",
    "        test_data.loc[test_data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] == \"Logistical\", \"Converted category\"] = 2\n",
    "        test_data.loc[test_data[\"New category (Active, Constructive, Logistical, Content-Clarification, Note, Poll)\"] == \"Content-Clarification\", \"Converted category\"] = 3\n",
    "        test = test.append(test_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfc2de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process text in dataset\n",
    "print('Processing text dataset')\n",
    "\n",
    "# load data and process with text_to_wordlist\n",
    "df_train = train\n",
    "\n",
    "train_content = df_train['cleanedContent'].tolist()\n",
    "train_labels = df_train['Converted category'].tolist()\n",
    "\n",
    "df_test = test\n",
    "\n",
    "test_content = df_test['cleanedContent'].tolist()\n",
    "test_ids = df_test['Converted category'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2136de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for specifying the number of labels to use\n",
    "train_labels = to_categorical(train_labels, 4)\n",
    "test_ids = to_categorical(test_ids, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4841aacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words in all sentences\n",
    "tokenizer = Tokenizer(num_words=Max_Num_Words)\n",
    "tokenizer.fit_on_texts(train_content + test_content)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_content)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_content)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('{} unique tokens are found'.format(len(word_index)))\n",
    "\n",
    "# pad all train with Max_Sequence_Length\n",
    "train_data = pad_sequences(train_sequences, maxlen=Max_Sequence_Length)\n",
    "train_labels = np.array(train_labels)\n",
    "print('Shape of train data tensor:', train_data.shape)\n",
    "print('Shape of train labels tensor:', train_labels.shape)\n",
    "\n",
    "# pad all test with Max_Sequence_Length\n",
    "test_data = pad_sequences(test_sequences, maxlen=Max_Sequence_Length)\n",
    "test_ids = np.array(test_ids)\n",
    "print('Shape of test data tensor:', test_data.shape)\n",
    "print('Shape of test ids tensor:', test_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ea59b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding\n",
    "if embedding_method == \"GloVe\":\n",
    "    Embedding_Dim = 300 # Dimension of GloVe-embedding\n",
    "    Embedding_File = '/Users/gubow/COMP 691H/Find Duplicates Project/glove.840B.300d.txt' # This needs to be changed to your directory to glove.840B.300d.txt\n",
    "    # Create word embedding dictionary from 'glove.840B.300d.txt'\n",
    "    print('Creating GloVe word embedding dictionary...')\n",
    "\n",
    "    embeddings_index = {}\n",
    "    f = open(Embedding_File, encoding='utf-8')\n",
    "\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        # word = values[0]\n",
    "        word = ''.join(values[:-300])   \n",
    "        coefs = np.asarray(values[-300:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Found {} word vectors of glove.'.format(len(embeddings_index)))\n",
    "    \n",
    "if embedding_method == \"Word2Vec\":\n",
    "    Embedding_Dim = 300 # Dimension of Word2Vec-embedding\n",
    "    Embedding_File = '/Users/gubow/COMP 691H/Find Duplicates Project/GoogleNews-vectors-negative300.bin'# This needs to be changed to your directory to GoogleNews-vectors-negative300.bin\n",
    "    print('Creating Word2Vec word embedding dictionary...')\n",
    "\n",
    "    word2vec = KeyedVectors.load_word2vec_format(datapath(Embedding_File), binary=True)\n",
    "    print('Found %s word vectors of word2vec' % len(word2vec))\n",
    "    \n",
    "if embedding_method == \"Bert\":\n",
    "    Embedding_Dim = 1024 # Dimension of Bert-embedding\n",
    "    print('Creating Bert word embedding dictionary...')\n",
    "    bert_embeddings = BertEmbeddings(model_name = 'bert-large-uncased-whole-word-masking')\n",
    "    # This will create a tensor too large for a single computer, a more powerful one is needed\n",
    "    '''\n",
    "    embeddings_index = {}\n",
    "    for word in word_index:\n",
    "        output = bert_embeddings([word])\n",
    "        for value in output[0]['embeddings_map'].values():\n",
    "            embeddings_index[word] = np.array(value)\n",
    "    print('Found {} word vectors of bert.'.format(len(embeddings_index)))\n",
    "    '''\n",
    "    # Use this instead\n",
    "    embeddings_index = {}\n",
    "    for word in word_index:\n",
    "        output = bert_embeddings([word])\n",
    "        result = np.array(output[0]['hidden_states'])[0][-1] + np.array(output[0]['hidden_states'])[0][-2] + np.array(output[0]['hidden_states'])[0][-3] + np.array(output[0]['hidden_states'])[0][-4]\n",
    "        embeddings_index[word] = result\n",
    "    print('Found {} word vectors of bert.'.format(len(embeddings_index)))\n",
    "    \n",
    "if embedding_method == \"RoBerta\":\n",
    "    Embedding_Dim = 1024 # Dimension of Bert-embedding\n",
    "    print('Creating RoBerta word embedding dictionary...')\n",
    "    roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "    roberta_model = TFRobertaModel.from_pretrained('roberta-large')\n",
    "    embeddings_index = {}\n",
    "    for word in word_index:\n",
    "        inputs = roberta_tokenizer(word, return_tensors=\"tf\")\n",
    "        outputs = roberta_model(inputs)\n",
    "        result = np.array(outputs.last_hidden_state[0][1:-1])\n",
    "        embeddings_index[word] = result\n",
    "    print('Found {} word vectors of RoBerta.'.format(len(embeddings_index)))\n",
    "    \n",
    "if embedding_method == \"DeBerta\":\n",
    "    Embedding_Dim = 1536 # Dimension of Bert-embedding\n",
    "    print('Creating DeBerta word embedding dictionary...')\n",
    "    deberta_tokenizer = DebertaV2Tokenizer.from_pretrained('kamalkraj/deberta-v2-xlarge')\n",
    "    deberta_model = TFDebertaV2Model.from_pretrained('kamalkraj/deberta-v2-xlarge')\n",
    "    embeddings_index = {}\n",
    "    for word in word_index:\n",
    "        inputs = deberta_tokenizer(word, return_tensors=\"tf\")\n",
    "        outputs = deberta_model(inputs)\n",
    "        result = np.array(outputs.last_hidden_state[0][1:-1])\n",
    "        embeddings_index[word] = result\n",
    "    print('Found {} word vectors of DeBerta.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8072731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix for embedding layer\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "num_words = min(Max_Num_Words, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, Embedding_Dim))\n",
    "if embedding_method == \"GloVe\" or embedding_method == \"Bert\":\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "if embedding_method == \"Word2Vec\":\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_vector = word2vec.get_vector(word)\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except:\n",
    "            continue\n",
    "if embedding_method == \"RoBerta\" or embedding_method == \"DeBerta\":\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)[0]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: '.format(np.sum(np.sum(embedding_matrix, axis=1) == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d85689",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7dfa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Validation split\n",
    "perm = np.random.permutation(len(train_data))\n",
    "idx_train = perm[:int(len(train_data)*(1-Validation_Split_Ratio))]\n",
    "idx_val = perm[int(len(train_data)*(1-Validation_Split_Ratio)):]\n",
    "\n",
    "data_train = train_data[idx_train]\n",
    "\n",
    "labels_train = train_labels[idx_train]\n",
    "\n",
    "data_val = train_data[idx_val]\n",
    "\n",
    "labels_val = train_labels[idx_val]\n",
    "\n",
    "weight_val = np.ones(len(labels_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6fd6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if neural_network == \"LSTM\":\n",
    "    # The embedding layer containing the word vectors\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=num_words,\n",
    "        output_dim=Embedding_Dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=Max_Sequence_Length,\n",
    "        trainable=False\n",
    "    )    \n",
    "\n",
    "\n",
    "    # LSTM layer\n",
    "\n",
    "    lstm_layer = LSTM(Num_Lstm, dropout=Rate_Drop_Lstm, recurrent_dropout=Rate_Drop_Lstm)\n",
    "\n",
    "    # Define inputs\n",
    "    seq = Input(shape=(Max_Sequence_Length,), dtype='int32')\n",
    "\n",
    "    # Run inputs through embedding\n",
    "    emb = emb_layer(seq)\n",
    "\n",
    "    # Run through LSTM layers\n",
    "    lstm = lstm_layer(emb)\n",
    "    # glob1 = GlobalAveragePooling1D()(lstm)\n",
    "\n",
    "    merged = concatenate([lstm])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(Rate_Drop_Dense)(merged)\n",
    "\n",
    "    merged = Dense(Num_Dense, activation=act_f)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(Rate_Drop_Dense)(merged)\n",
    "\n",
    "    preds = Dense(4, activation='softmax')(merged)\n",
    "    class_weight = None\n",
    "    \n",
    "    # Train the model\n",
    "    model = Model(inputs=[seq], outputs=preds)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "\n",
    "    # Set early stopping (large patience should be useful)\n",
    "    early_stopping =EarlyStopping(monitor='val_acc', patience=20)\n",
    "    bst_model_path = training_data_type + \" predicts \" + test_data_type + data_clean_type + \" (\" + embedding_method + \" + \" + neural_network + \")\" + '.h5'\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "    if embedding_method == \"RoBerta\" or embedding_method == \"DeBerta\":\n",
    "        hist = model.fit([data_train], labels_train, \\\n",
    "        validation_data=([data_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=512, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "    else:\n",
    "        hist = model.fit([data_train], labels_train, \\\n",
    "        validation_data=([data_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "        \n",
    "if neural_network == \"CNN\":\n",
    "    # The embedding layer containing the word vectors\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=num_words,\n",
    "        output_dim=Embedding_Dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=Max_Sequence_Length,\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    # 1D convolutions that can iterate over the word vectors\n",
    "    conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "    conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\n",
    "    conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "    conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\n",
    "    conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n",
    "    conv6 = Conv1D(filters=32, kernel_size=6, padding='same', activation='relu')\n",
    "\n",
    "    # Define inputs\n",
    "    seq = Input(shape=(60,))\n",
    "\n",
    "    # Run inputs through embedding\n",
    "    emb = emb_layer(seq)\n",
    "\n",
    "    # Run through CONV + GAP layers\n",
    "    conv1 = conv1(emb)\n",
    "    glob1 = GlobalAveragePooling1D()(conv1)\n",
    "\n",
    "    conv2 = conv2(emb)\n",
    "    glob2 = GlobalAveragePooling1D()(conv2)\n",
    "\n",
    "    conv3 = conv3(emb)\n",
    "    glob3 = GlobalAveragePooling1D()(conv3)\n",
    "\n",
    "    conv4 = conv4(emb)\n",
    "    glob4 = GlobalAveragePooling1D()(conv4)\n",
    "\n",
    "    conv5 = conv5(emb)\n",
    "    glob5 = GlobalAveragePooling1D()(conv5)\n",
    "\n",
    "    conv6 = conv6(emb)\n",
    "    glob6 = GlobalAveragePooling1D()(conv6)\n",
    "\n",
    "    merge = concatenate([glob1, glob2, glob3, glob4, glob5, glob6])\n",
    "\n",
    "    diff = Lambda(lambda x: K.abs(x[0]), output_shape=(4 * 128 + 2*32,))([merge])\n",
    "    mul = Lambda(lambda x: x[0], output_shape=(4 * 128 + 2*32,))([merge])\n",
    "\n",
    "    merge = concatenate([diff, mul])\n",
    "    class_weight = None\n",
    "\n",
    "    # The MLP that determines the outcome\n",
    "    x = Dropout(0.2)(merge)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(300, activation='relu')(x)\n",
    "\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    pred = Dense(4, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=[seq], outputs=pred)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "\n",
    "    # Set early stopping (large patience should be useful)\n",
    "    early_stopping =EarlyStopping(monitor='val_acc', patience=20)\n",
    "    bst_model_path = training_data_type + \" predicts \" + test_data_type + data_clean_type + \" (\" + embedding_method + \" + \" + neural_network + \")\" + '.h5' \n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "    if embedding_method == \"RoBerta\" or embedding_method == \"DeBerta\":\n",
    "        hist = model.fit([data_train], labels_train, \\\n",
    "        validation_data=([data_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=512, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "    else:\n",
    "        hist = model.fit([data_train], labels_train, \\\n",
    "        validation_data=([data_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4ea5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the model training has finished. Save the model for future use\n",
    "model.save(bst_model_path) # store model parameters in .h5 file\n",
    "bst_val_score = min(hist.history['val_acc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
