{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e180651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GlobalAveragePooling1D, Lambda\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization.batch_normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras.backend as K\n",
    "\n",
    "from BertEmbeddings import BertEmbeddings\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "from transformers import DebertaV2Tokenizer, TFDebertaV2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a243d50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name below should be changed according to different courses\n",
    "file = \"COMP 301 Summer 2021.csv\"\n",
    "Train_Data_File = file.split(\".\")[0] + \" (train).csv\"\n",
    "Test_Data_File = file.split(\".\")[0] + \" (test).csv\"\n",
    "Max_Sequence_Length = 60\n",
    "Max_Num_Words = 200000\n",
    "\n",
    "Validation_Split_Ratio = 0.2\n",
    "\n",
    "Num_Lstm = np.random.randint(175, 275)\n",
    "Num_Dense = np.random.randint(100, 150)\n",
    "Rate_Drop_Lstm = 0.15 + np.random.rand() * 0.25\n",
    "Rate_Drop_Dense = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "act_f = 'relu'\n",
    "\n",
    "# embedding_method:\n",
    "# 1. Bert (will use Bert large)\n",
    "# 2. DeBerta (will use DeBerta-v2-xlarge)\n",
    "# 3. GloVe\n",
    "# 4. RoBerta (will use RoBerta-large)\n",
    "# 5. Word2Vec\n",
    "embedding_model = \"Bert\"\n",
    "\n",
    "# Support neural network:\n",
    "# LSTM\n",
    "# CNN\n",
    "neural_network = \"LSTM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4063a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process text in dataset\n",
    "print('Processing text dataset')\n",
    "\n",
    "# load data and process with text_to_wordlist\n",
    "df_train = pd.read_csv(Train_Data_File, encoding='utf-8')\n",
    "\n",
    "train_content = df_train['Post'].tolist()\n",
    "train_labels = df_train['Incomplete?'].tolist()\n",
    "\n",
    "df_test = pd.read_csv(Test_Data_File, encoding='utf-8')\n",
    "\n",
    "test_content = df_test['Post'].tolist()\n",
    "test_ids = df_test['ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a9dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words in all sentences\n",
    "tokenizer = Tokenizer(num_words=Max_Num_Words)\n",
    "tokenizer.fit_on_texts(train_content + test_content)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_content)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_content)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('{} unique tokens are found'.format(len(word_index)))\n",
    "\n",
    "# pad all train with Max_Sequence_Length\n",
    "train_data = pad_sequences(train_sequences, maxlen=Max_Sequence_Length)\n",
    "train_labels = np.array(train_labels)\n",
    "print('Shape of train data tensor:', train_data.shape)\n",
    "print('Shape of train labels tensor:', train_labels.shape)\n",
    "\n",
    "# pad all test with Max_Sequence_Length\n",
    "test_data = pad_sequences(test_sequences, maxlen=Max_Sequence_Length)\n",
    "test_ids = np.array(test_ids)\n",
    "print('Shape of test data tensor:', test_data.shape)\n",
    "print('Shape of test ids tensor:', test_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe57de38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word embedding\n",
    "if embedding_model == \"GloVe\":\n",
    "    Embedding_Dim = 300 # Dimension of GloVe-embedding\n",
    "    Embedding_File = '/Users/gubow/COMP 691H/Find Duplicates Project/glove.840B.300d.txt'\n",
    "    # Create word embedding dictionary from 'glove.840B.300d.txt'\n",
    "    print('Creating GloVe word embedding dictionary...')\n",
    "\n",
    "    embeddings_index = {}\n",
    "    f = open(Embedding_File, encoding='utf-8')\n",
    "\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        # word = values[0]\n",
    "        word = ''.join(values[:-300])   \n",
    "        coefs = np.asarray(values[-300:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Found {} word vectors of glove.'.format(len(embeddings_index)))\n",
    "    \n",
    "if embedding_model == \"Word2Vec\":\n",
    "    Embedding_Dim = 300 # Dimension of Word2Vec-embedding\n",
    "    Embedding_File = '/Users/gubow/COMP 691H/Find Duplicates Project/GoogleNews-vectors-negative300.bin'\n",
    "    print('Creating Word2Vec word embedding dictionary...')\n",
    "\n",
    "    word2vec = KeyedVectors.load_word2vec_format(datapath(Embedding_File), binary=True)\n",
    "    print('Found %s word vectors of word2vec' % len(word2vec))\n",
    "    \n",
    "if embedding_model == \"Bert\":\n",
    "    Embedding_Dim = 1024 # Dimension of Bert-embedding\n",
    "    print('Creating Bert word embedding dictionary...')\n",
    "    bert_embeddings = BertEmbeddings(model_name = 'bert-large-uncased-whole-word-masking')\n",
    "    # This will create a tensor too large for a single computer, a more powerful one is needed\n",
    "    '''\n",
    "    embeddings_index = {}\n",
    "    for word in word_index:\n",
    "        output = bert_embeddings([word])\n",
    "        for value in output[0]['embeddings_map'].values():\n",
    "            embeddings_index[word] = np.array(value)\n",
    "    print('Found {} word vectors of bert.'.format(len(embeddings_index)))\n",
    "    '''\n",
    "    # Use this instead\n",
    "    embeddings_index = {}\n",
    "    for word in word_index:\n",
    "        output = bert_embeddings([word])\n",
    "        result = np.array(output[0]['hidden_states'])[0][-1] + np.array(output[0]['hidden_states'])[0][-2] + np.array(output[0]['hidden_states'])[0][-3] + np.array(output[0]['hidden_states'])[0][-4]\n",
    "        embeddings_index[word] = result\n",
    "    print('Found {} word vectors of bert.'.format(len(embeddings_index)))\n",
    "    \n",
    "if embedding_model == \"RoBerta\":\n",
    "    Embedding_Dim = 1024 # Dimension of Bert-embedding\n",
    "    print('Creating RoBerta word embedding dictionary...')\n",
    "    roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "    roberta_model = TFRobertaModel.from_pretrained('roberta-large')\n",
    "    embeddings_index = {}\n",
    "    for word in word_index:\n",
    "        inputs = roberta_tokenizer(word, return_tensors=\"tf\")\n",
    "        outputs = roberta_model(inputs)\n",
    "        result = np.array(outputs.last_hidden_state[0][1:-1])\n",
    "        embeddings_index[word] = result\n",
    "    print('Found {} word vectors of RoBerta.'.format(len(embeddings_index)))\n",
    "    \n",
    "if embedding_model == \"DeBerta\":\n",
    "    Embedding_Dim = 1536 # Dimension of Bert-embedding\n",
    "    print('Creating DeBerta word embedding dictionary...')\n",
    "    deberta_tokenizer = DebertaV2Tokenizer.from_pretrained('kamalkraj/deberta-v2-xlarge')\n",
    "    deberta_model = TFDebertaV2Model.from_pretrained('kamalkraj/deberta-v2-xlarge')\n",
    "    embeddings_index = {}\n",
    "    for word in word_index:\n",
    "        inputs = deberta_tokenizer(word, return_tensors=\"tf\")\n",
    "        outputs = deberta_model(inputs)\n",
    "        result = np.array(outputs.last_hidden_state[0][1:-1])\n",
    "        embeddings_index[word] = result\n",
    "    print('Found {} word vectors of DeBerta.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e97d003",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks = df_train[['Time Feature 1', 'Time Feature 2', 'Reason Feature 1']]\n",
    "test_leaks = df_test[['Time Feature 1', 'Time Feature 2', 'Reason Feature 1']]\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((leaks, test_leaks)))\n",
    "leaks = ss.transform(leaks)\n",
    "test_leaks = ss.transform(test_leaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22931c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding matrix for embedding layer\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "num_words = min(Max_Num_Words, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, Embedding_Dim))\n",
    "if embedding_model == \"GloVe\" or embedding_model == \"Bert\":\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "if embedding_model == \"Word2Vec\":\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_vector = word2vec.get_vector(word)\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except:\n",
    "            continue\n",
    "if embedding_model == \"RoBerta\" or embedding_model == \"DeBerta\":\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)[0]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: '.format(np.sum(np.sum(embedding_matrix, axis=1) == 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Validation split\n",
    "perm = np.random.permutation(len(train_data))\n",
    "idx_train = perm[:int(len(train_data)*(1-Validation_Split_Ratio))]\n",
    "idx_val = perm[int(len(train_data)*(1-Validation_Split_Ratio)):]\n",
    "\n",
    "data_train = train_data[idx_train]\n",
    "leaks_train = leaks[idx_train]\n",
    "\n",
    "labels_train = train_labels[idx_train]\n",
    "\n",
    "data_val = train_data[idx_val]\n",
    "leaks_val = leaks[idx_val]\n",
    "\n",
    "labels_val = train_labels[idx_val]\n",
    "\n",
    "weight_val = np.ones(len(labels_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if neural_network == \"LSTM\":\n",
    "    # The embedding layer containing the word vectors\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=num_words,\n",
    "        output_dim=Embedding_Dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=Max_Sequence_Length,\n",
    "        trainable=False\n",
    "    )    \n",
    "\n",
    "\n",
    "    # LSTM layer\n",
    "\n",
    "    lstm_layer = LSTM(Num_Lstm, dropout=Rate_Drop_Lstm, recurrent_dropout=Rate_Drop_Lstm)\n",
    "\n",
    "    # Define inputs\n",
    "    seq = Input(shape=(Max_Sequence_Length,), dtype='int32')\n",
    "\n",
    "    # Run inputs through embedding\n",
    "    emb = emb_layer(seq)\n",
    "\n",
    "    # Run through LSTM layers\n",
    "    lstm = lstm_layer(emb)\n",
    "    # glob1 = GlobalAveragePooling1D()(lstm)\n",
    "\n",
    "    magic_input = Input(shape=(leaks.shape[1],))\n",
    "    magic_dense = BatchNormalization()(magic_input)\n",
    "    magic_dense = Dense(int(Num_Dense/2), activation=act_f)(magic_input)\n",
    "\n",
    "    merged = concatenate([lstm, magic_dense])\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(Rate_Drop_Dense)(merged)\n",
    "\n",
    "    merged = Dense(Num_Dense, activation=act_f)(merged)\n",
    "    merged = BatchNormalization()(merged)\n",
    "    merged = Dropout(Rate_Drop_Dense)(merged)\n",
    "\n",
    "    preds = Dense(1, activation='sigmoid')(merged)\n",
    "    class_weight = None\n",
    "    \n",
    "    # Train the model\n",
    "    model = Model(inputs=[seq, magic_input], outputs=preds)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n",
    "\n",
    "    # Set early stopping (large patience should be useful)\n",
    "    early_stopping =EarlyStopping(monitor='val_acc', patience=6)\n",
    "    bst_model_path = embedding_model + \" + \" + neural_network + '.h5'\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "    hist = model.fit([data_train, leaks_train], labels_train, \\\n",
    "        validation_data=([data_val, leaks_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "        \n",
    "if neural_network == \"CNN\":\n",
    "    # The embedding layer containing the word vectors\n",
    "    emb_layer = Embedding(\n",
    "        input_dim=num_words,\n",
    "        output_dim=Embedding_Dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=Max_Sequence_Length,\n",
    "        trainable=False\n",
    "    )\n",
    "\n",
    "    # 1D convolutions that can iterate over the word vectors\n",
    "    conv1 = Conv1D(filters=128, kernel_size=1, padding='same', activation='relu')\n",
    "    conv2 = Conv1D(filters=128, kernel_size=2, padding='same', activation='relu')\n",
    "    conv3 = Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')\n",
    "    conv4 = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')\n",
    "    conv5 = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')\n",
    "    conv6 = Conv1D(filters=32, kernel_size=6, padding='same', activation='relu')\n",
    "\n",
    "    # Define inputs\n",
    "    seq = Input(shape=(60,))\n",
    "\n",
    "    # Run inputs through embedding\n",
    "    emb = emb_layer(seq)\n",
    "\n",
    "    # Run through CONV + GAP layers\n",
    "    conv1 = conv1(emb)\n",
    "    glob1 = GlobalAveragePooling1D()(conv1)\n",
    "\n",
    "    conv2 = conv2(emb)\n",
    "    glob2 = GlobalAveragePooling1D()(conv2)\n",
    "\n",
    "    conv3 = conv3(emb)\n",
    "    glob3 = GlobalAveragePooling1D()(conv3)\n",
    "\n",
    "    conv4 = conv4(emb)\n",
    "    glob4 = GlobalAveragePooling1D()(conv4)\n",
    "\n",
    "    conv5 = conv5(emb)\n",
    "    glob5 = GlobalAveragePooling1D()(conv5)\n",
    "\n",
    "    conv6 = conv6(emb)\n",
    "    glob6 = GlobalAveragePooling1D()(conv6)\n",
    "\n",
    "    merge = concatenate([glob1, glob2, glob3, glob4, glob5, glob6])\n",
    "\n",
    "    diff = Lambda(lambda x: K.abs(x[0]), output_shape=(4 * 128 + 2*32,))([merge])\n",
    "    mul = Lambda(lambda x: x[0], output_shape=(4 * 128 + 2*32,))([merge])\n",
    "\n",
    "    magic_input = Input(shape=(leaks.shape[1],))\n",
    "    magic_dense = BatchNormalization()(magic_input)\n",
    "    magic_dense = Dense(64, activation='relu')(magic_dense)\n",
    "\n",
    "    merge = concatenate([diff, mul, magic_dense])\n",
    "    class_weight = None\n",
    "\n",
    "    # The MLP that determines the outcome\n",
    "    x = Dropout(0.2)(merge)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(300, activation='relu')(x)\n",
    "\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    pred = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[seq, magic_input], outputs=pred)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "    # Set early stopping (large patience should be useful)\n",
    "    early_stopping =EarlyStopping(monitor='val_acc', patience=6)\n",
    "    bst_model_path = embedding_model + \" + \" + neural_network + '.h5' \n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "    hist = model.fit([data_train, leaks_train], labels_train, \\\n",
    "        validation_data=([data_val, leaks_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879cb77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the model training has finished. Save the model for future use\n",
    "model.save(bst_model_path) # store model parameters in .h5 file\n",
    "bst_val_score = min(hist.history['val_acc'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
