{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6539dd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from BertEmbeddings import BertEmbeddings\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "from math import log, e\n",
    "import pandas as pd\n",
    "import timeit\n",
    "\n",
    "import json\n",
    "from ibm_watson import NaturalLanguageUnderstandingV1\n",
    "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\n",
    "from ibm_watson.natural_language_understanding_v1 import Features, KeywordsOptions, EntitiesOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6b8de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "file = \"COMP All courses.csv\" # This is the Google Sheet in the \"Data\" folder. Combine all 4 sub-sheets into one sheet and rename it to get a single csv file\n",
    "df_train = pd.read_csv(file, encoding='utf-8')\n",
    "# Remove rows containing \"N\" (unrelated to OH requests)\n",
    "\n",
    "# && df retrieve column error\n",
    "#df_train = df_train.loc[df_train[\"Type (Good(G), Lack of Time element(T), Lack of Reason element(R), No asking for OH(N))\"] != \"N\"]\n",
    "df_train = df_train.loc[df_train.iloc[:,4] != \"N\"]\n",
    "df_train = df_train.dropna()\n",
    "\n",
    "content = df_train.iloc[:,2]\n",
    "labels = df_train.iloc[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d192e5",
   "metadata": {},
   "source": [
    "#### Add rows for fine labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a96f6ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "time = []\n",
    "reason = []\n",
    "for label in labels:\n",
    "    if \"T\" in label:\n",
    "        time.append(1)\n",
    "    else:\n",
    "        time.append(0)\n",
    "    if \"R\" in label:\n",
    "        reason.append(1)\n",
    "    else:\n",
    "        reason.append(0)\n",
    "time = np.array(time)\n",
    "reason = np.array(reason)\n",
    "df_train[\"Lack of time compartment?\"] = time\n",
    "df_train[\"Lack of reason compartment?\"] = reason"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dd3d13",
   "metadata": {},
   "source": [
    "#### Time feature 1: Does the request contains substring \"at|before|after|around X/Xam/AM|pm/PM/X am/AM|pm/PM\" (such as \"at 10\" or \"10am\") or HH:MM (such as 10:00) or HH-HH (such as 10-11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8517084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_feature_1 = []\n",
    "for post in content:\n",
    "    result = re.findall(r'at [0-9]+|before [0-9]+|after [0-9]+|around [0-9]+|[0-9]+am|[0-9]+pm|[0-9]+ am|[0-9]+ pm|[0-9]+AM|[0-9]+PM|[0-9]+ AM|[0-9]+ PM', post)\n",
    "    result2 = re.findall(r'[0-9]+:[0-9]+', post)\n",
    "    result3 = re.findall(r'[0-9]+-[0-9]+', post)\n",
    "    if len(result) != 0 or len(result2) != 0 or len(result3) != 0:\n",
    "        time_feature_1.append(1)\n",
    "    else:\n",
    "        time_feature_1.append(0)\n",
    "df_train['Time Feature 1'] = time_feature_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9bd0b5",
   "metadata": {},
   "source": [
    "#### Time feature 2: Does the request contains substring \"after someone\" or \"with someone\" (such as \" I would like to come after/with someone...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de7e2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_feature_2 = []\n",
    "for post in content:\n",
    "    result = re.findall(r'after [A-Z][a-z]+', post)\n",
    "    result2 = re.findall(r'with [A-Z][a-z]+', post)\n",
    "    if len(result) != 0 or len(result2) != 0:\n",
    "        time_feature_2.append(1)\n",
    "    else:\n",
    "        time_feature_2.append(0)\n",
    "df_train['Time Feature 2'] = time_feature_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7fb29c",
   "metadata": {},
   "source": [
    "#### Time feature 3: Does the request contains substring \"MM/DD HH\" (such as \"11/17 11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff77acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_feature_3 = []\n",
    "for post in content:\n",
    "    result = re.findall(r'[0-9]+/[0-9]+ [0-9]+', post)\n",
    "    if len(result) != 0:\n",
    "        time_feature_3.append(1)\n",
    "    else:\n",
    "        time_feature_3.append(0)\n",
    "df_train['Time Feature 3'] = time_feature_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6dc348",
   "metadata": {},
   "source": [
    "#### Reason feature 1: How many words does the request have? (Usually short requests are incomplete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc6a3a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_feature_1 = []\n",
    "for post in content:\n",
    "    # Remove punctuation\n",
    "    post = ''.join([c for c in post if c not in punctuation])\n",
    "    reason_feature_1.append(len(post.split(\" \")))\n",
    "df_train['Reason Feature 1'] = reason_feature_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0a95bc",
   "metadata": {},
   "source": [
    "#### Reason feature 2: What is the entropy of the request? (Higher entroy means more unique words, which leads to more specific reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a80bb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy1(labels, base=None):\n",
    "    value,counts = np.unique(labels, return_counts=True)\n",
    "    return entropy(counts, base=base)\n",
    "\n",
    "def entropy2(labels, base=None):\n",
    "    \"\"\" Computes entropy of label distribution. \"\"\"\n",
    "\n",
    "    n_labels = len(labels)\n",
    "\n",
    "    if n_labels <= 1:\n",
    "        return 0\n",
    "\n",
    "    value,counts = np.unique(labels, return_counts=True)\n",
    "    probs = counts / n_labels\n",
    "    n_classes = np.count_nonzero(probs)\n",
    "\n",
    "    if n_classes <= 1:\n",
    "        return 0\n",
    "\n",
    "    ent = 0.\n",
    "\n",
    "    # Compute entropy\n",
    "    base = e if base is None else base\n",
    "    for i in probs:\n",
    "        ent -= i * log(i, base)\n",
    "\n",
    "    return ent\n",
    "\n",
    "def entropy3(labels, base=None):\n",
    "    vc = pd.Series(labels).value_counts(normalize=True, sort=False)\n",
    "    base = e if base is None else base\n",
    "    return -(vc * np.log(vc)/np.log(base)).sum()\n",
    "\n",
    "def entropy4(labels, base=None):\n",
    "    value,counts = np.unique(labels, return_counts=True)\n",
    "    norm_counts = counts / counts.sum()\n",
    "    base = e if base is None else base\n",
    "    return -(norm_counts * np.log(norm_counts)/np.log(base)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f683584",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_feature_2 = []\n",
    "for post in content:\n",
    "    # Remove punctuation\n",
    "    post = ''.join([c for c in post if c not in punctuation])\n",
    "    word_array = post.split(\" \")\n",
    "    reason_feature_2.append(entropy2(word_array))\n",
    "df_train['Reason Feature 2'] = reason_feature_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf2aa51",
   "metadata": {},
   "source": [
    "#### Reason feature 3: What is the average TF-IDF of each request (average of each words' TF-IDF in the request)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e458f52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(bowCount)\n",
    "    return tfDict\n",
    "\n",
    "def computeIDF(docList):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "        \n",
    "    return idfDict\n",
    "\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f4eb980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build word set\n",
    "word_set = set()\n",
    "for post in content:\n",
    "    # Remove punctuation\n",
    "    post = ''.join([c for c in post if c not in punctuation])\n",
    "    word_array = post.split(\" \")\n",
    "    word_set = set(word_set).union(set(word_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10c08701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build word dictionary list\n",
    "dictionary_list = []\n",
    "for post in content:\n",
    "    # Remove punctuation\n",
    "    post = ''.join([c for c in post if c not in punctuation])\n",
    "    word_array = post.split(\" \")\n",
    "    word_dict = dict.fromkeys(word_set, 0)\n",
    "    for word in word_array:\n",
    "        word_dict[word] = word_dict[word] + 1\n",
    "    dictionary_list.append(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4137289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate each post's average TF-IDF\n",
    "reason_feature_3 = []\n",
    "for post in content:\n",
    "    # Remove punctuation\n",
    "    post = ''.join([c for c in post if c not in punctuation])\n",
    "    word_array = post.split(\" \")\n",
    "    word_dict = dict.fromkeys(word_set, 0)\n",
    "    for word in word_array:\n",
    "        word_dict[word] = word_dict[word] + 1\n",
    "    post_tf = computeTF(word_dict, word_array)\n",
    "    post_idf = computeIDF(dictionary_list)\n",
    "    post_tfidf = computeTFIDF(post_tf, post_idf)\n",
    "    average_tfidf = []\n",
    "    for word in word_array:\n",
    "        average_tfidf.append(post_tfidf[word])\n",
    "    reason_feature_3.append(np.average(average_tfidf))\n",
    "df_train['Reason Feature 3'] = reason_feature_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7799e0d4",
   "metadata": {},
   "source": [
    "#### Reason feature 4: How many keywords are in the post? Usually less keywords lead to incompliteness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a5857",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_feature_4 = []\n",
    "authenticator = IAMAuthenticator('Your IBM Watson Natural Language Understanding API password here')\n",
    "natural_language_understanding = NaturalLanguageUnderstandingV1(\n",
    "    version='2021-08-01',\n",
    "    authenticator=authenticator\n",
    ")\n",
    "\n",
    "natural_language_understanding.set_service_url('Your IBM Watson Natural Language Understanding API service URL here')\n",
    "\n",
    "progress_count = 0\n",
    "for post in content:\n",
    "    try:\n",
    "        response = natural_language_understanding.analyze(text = post, features=Features(keywords=KeywordsOptions(sentiment=True,emotion=True,limit=999))).get_result()\n",
    "        reason_feature_4.append(len(response[\"keywords\"]))\n",
    "    except:\n",
    "        reason_feature_4.append(0)\n",
    "    progress_count = progress_count + 1\n",
    "    if (progress_count % 50 == 0):\n",
    "        print(str(progress_count) + \" posts processed\")\n",
    "    \n",
    "df_train['Reason Feature 4'] = reason_feature_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec045a02",
   "metadata": {},
   "source": [
    "#### Reason feature 5: Does the request contains a link to a Piazza post using \"@\"? This also indicates completeness of reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e0a0072",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_feature_5 = []\n",
    "for post in content:\n",
    "    result = re.findall(r'@[0-9]+', post)\n",
    "    if len(result) != 0:\n",
    "        reason_feature_5.append(1)\n",
    "    else:\n",
    "        reason_feature_5.append(0)\n",
    "df_train['Reason Feature 5'] = reason_feature_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c9afce",
   "metadata": {},
   "source": [
    "#### Reason feature 6: How many times does the request contain keyword \"to\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1325176",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_feature_6 = []\n",
    "for post in content:\n",
    "    reason_feature_6.append(post.split(\" \").count(\"to\"))\n",
    "df_train['Reason Feature 6'] = reason_feature_6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9e4f59",
   "metadata": {},
   "source": [
    "#### Reason feature 7: How many times does the request contain keyword \"I\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab80121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_feature_7 = []\n",
    "for post in content:\n",
    "    reason_feature_7.append(post.split(\" \").count(\"I\"))\n",
    "df_train['Reason Feature 7'] = reason_feature_7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed4b868",
   "metadata": {},
   "source": [
    "#### Reason feature 8: How many times does the request contain keyword \"the\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "415b5c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_feature_8 = []\n",
    "for post in content:\n",
    "    reason_feature_8.append(post.split(\" \").count(\"the\"))\n",
    "df_train['Reason Feature 8'] = reason_feature_8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f08238",
   "metadata": {},
   "source": [
    "#### Reason feature 9: Does the request contain keyword \"think\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21a64949",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_feature_9 = []\n",
    "for post in content:\n",
    "    occurrence = post.split(\" \").count(\"think\")\n",
    "    if occurrence > 0:\n",
    "        reason_feature_9.append(1)\n",
    "    else:\n",
    "        reason_feature_9.append(0)\n",
    "df_train['Reason Feature 9'] = reason_feature_9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74af2e7",
   "metadata": {},
   "source": [
    "#### Reason feature 10: Does the request contain keyword \"Hello!\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a1d78f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_feature_10 = []\n",
    "for post in content:\n",
    "    occurrence = post.split(\" \").count(\"Hello!\")\n",
    "    if occurrence > 0:\n",
    "        reason_feature_10.append(1)\n",
    "    else:\n",
    "        reason_feature_10.append(0)\n",
    "df_train['Reason Feature 10'] = reason_feature_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74397a5f",
   "metadata": {},
   "source": [
    "#### Reason feature 11: Does the request contain keyword \"OHs\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bc1cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_feature_11 = []\n",
    "for post in content:\n",
    "    occurrence = post.split(\" \").count(\"OHs\")\n",
    "    if occurrence > 0:\n",
    "        reason_feature_11.append(1)\n",
    "    else:\n",
    "        reason_feature_11.append(0)\n",
    "df_train['Reason Feature 11'] = reason_feature_11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6291f52",
   "metadata": {},
   "source": [
    "#### Reason feature 12: Does the request contain keyword \"use\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d775e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_feature_12 = []\n",
    "for post in content:\n",
    "    occurrence = post.split(\" \").count(\"use\")\n",
    "    if occurrence > 0:\n",
    "        reason_feature_12.append(1)\n",
    "    else:\n",
    "        reason_feature_12.append(0)\n",
    "df_train['Reason Feature 12'] = reason_feature_12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10785882",
   "metadata": {},
   "source": [
    "#### Reason feature 13: Does the request contain keyword \"some\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff9e15f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_feature_13 = []\n",
    "for post in content:\n",
    "    occurrence = post.split(\" \").count(\"some\")\n",
    "    if occurrence > 0:\n",
    "        reason_feature_13.append(1)\n",
    "    else:\n",
    "        reason_feature_13.append(0)\n",
    "df_train['Reason Feature 13'] = reason_feature_13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2c5d9e",
   "metadata": {},
   "source": [
    "#### Reason feature 14: Does the request contain keyword \"question(s)\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40acdda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_feature_14 = []\n",
    "for post in content:\n",
    "    occurrence = post.split(\" \").count(\"question\") + post.split(\" \").count(\"questions\")\n",
    "    if occurrence > 0:\n",
    "        reason_feature_14.append(1)\n",
    "    else:\n",
    "        reason_feature_14.append(0)\n",
    "df_train['Reason Feature 14'] = reason_feature_14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ba8a83",
   "metadata": {},
   "source": [
    "#### Reason feature 15: Does the request contain keyword \"error(s)\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bff16dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_feature_15 = []\n",
    "for post in content:\n",
    "    occurrence = post.split(\" \").count(\"error\") + post.split(\" \").count(\"errors\")\n",
    "    if occurrence > 0:\n",
    "        reason_feature_15.append(1)\n",
    "    else:\n",
    "        reason_feature_15.append(0)\n",
    "df_train['Reason Feature 15'] = reason_feature_15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9222c42c",
   "metadata": {},
   "source": [
    "#### Split labelled data with features into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "068cd88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test  = train_test_split(df_train, test_size=0.20, random_state=4242)\n",
    "train.to_csv(file.split(\".\")[0] + \" (train).csv\", index=False)\n",
    "test.to_csv(file.split(\".\")[0] + \" (test).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b189909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMP All courses.csv\n"
     ]
    }
   ],
   "source": [
    "print(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
